[{"authors":["admin"],"categories":null,"content":"专注于基础架构，Cloud Native 拥护者，希望成为坚守开发一线打磨匠艺的架构师。\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"专注于基础架构，Cloud Native 拥护者，希望成为坚守开发一线打磨匠艺的架构师。","tags":null,"title":"L CJ","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":null,"categories":null,"content":" Kubernetes 本身不提供容器网络, 但是实现了一套支持多种网络插件的框架代码, 通过调用网络插件来为容器设置网络环境。\n而约束网络插件的是 CNI（Container Network Interface），一种标准的容器网络接口，定义了如何将容器加入网络和将容器从网络中删除。\nCNI 接口由 runtime 在创建容器和删除容器时调用。具体的接口定义如下：\n// vendor/github.com/containernetworking/cni/libcni/api.go type CNI interface { AddNetworkList(net *NetworkConfigList, rt *RuntimeConf) (types.Result, error) DelNetworkList(net *NetworkConfigList, rt *RuntimeConf) error AddNetwork(net *NetworkConfig, rt *RuntimeConf) (types.Result, error) DelNetwork(net *NetworkConfig, rt *RuntimeConf) error }  Kubernetes plugin 接口 kubelet 是通过 NetworkPlugin interface 来调用底层的网络插件为容器设置网络环境.\n// kubelet/dockershim/network/plugins.go // Plugin is an interface to network plugins for the kubelet type NetworkPlugin interface { // Init initializes the plugin. This will be called exactly once // before any other methods are called. Init(host Host, hairpinMode kubeletconfig.HairpinMode, nonMasqueradeCIDR string, mtu int) error // Called on various events like: // NET_PLUGIN_EVENT_POD_CIDR_CHANGE Event(name string, details map[string]interface{}) // Name returns the plugin's name. This will be used when searching // for a plugin by name, e.g. Name() string // Returns a set of NET_PLUGIN_CAPABILITY_* Capabilities() utilsets.Int // SetUpPod is the method called after the infra container of // the pod has been created but before the other containers of the // pod are launched. SetUpPod(namespace string, name string, podSandboxID kubecontainer.ContainerID, annotations, options map[string]string) error // TearDownPod is the method called before a pod's infra container will be deleted TearDownPod(namespace string, name string, podSandboxID kubecontainer.ContainerID) error // GetPodNetworkStatus is the method called to obtain the ipv4 or ipv6 addresses of the container GetPodNetworkStatus(namespace string, name string, podSandboxID kubecontainer.ContainerID) (*PodNetworkStatus, error) // Status returns error if the network plugin is in error state Status() error }  实现了 NetworkPlugin interface 就可以新增一种 Kubernetes 的 Network plugin。这个 interface 也并没有具体容器网络的实现，而是做了一层封装，具体的容器网络由独立的二进制实现，比如官方提供的 bridge、host-local 或者第三方的 calico、flannel 等，也可以是自己定制的实现。\nK8S 支持两种 plugin：\n cniNetworkPlugin kubenetNetworkPlugin  下面讲述 plugin 是如何初始化和工作的\nkubelet 启动 kubelet 启动后会调用 run() 进入处理流程，在进入主处理流程之前的初始化阶段会根据用户配置的网络插件名选择对应的网络插件。\n// cmd/kubelet/app/server.go func run(s *options.KubeletServer, kubeDeps *kubelet.KubeletDeps) (err error) { ... // 创建 kubelete // 根据 kubelet 的运行参数运行 kubelet // 这里会根据用户配置的网络插件名选择网络插件 if err := RunKubelet(\u0026amp;s.KubeletConfiguration, kubeDeps, s.RunOnce, standaloneMode); err != nil { return err } ... }  // cmd/kubelet/app/server.go func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error { ... k, err := CreateAndInitKubelet(\u0026amp;kubeServer.KubeletConfiguration, kubeDeps, ...) ... }  func CreateAndInitKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, ...) { k, err = kubelet.NewMainKubelet(kubeCfg, kubeDeps, crOptions ... ) k.BirthCry() k.StartGarbageCollection() }  // NewMainKubelet instantiates a new Kubelet object along with all the required internal modules. // No initialization of Kubelet and its modules should happen here. func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies ...) { klet := \u0026amp;Kubelet{ hostname: hostname, hostnameOverridden: len(hostnameOverride) \u0026gt; 0, ... } switch containerRuntime { case kubetypes.DockerContainerRuntime: // Create and start the CRI shim running as a grpc server. streamingConfig := getStreamingConfig(kubeCfg, kubeDeps, crOptions) ds, err := dockershim.NewDockerService(kubeDeps.DockerClientConfig, crOptions.PodSandboxImage, streamingConfig, \u0026amp;pluginSettings, runtimeCgroups, kubeCfg.CgroupDriver, crOptions.DockershimRootDirectory, !crOptions.RedirectContainerStreaming) ... server := dockerremote.NewDockerServer(remoteRuntimeEndpoint, ds) if err := server.Start(); err != nil { return nil, err } case kubetypes.RemoteContainerRuntime: // No-op. break default: return nil, fmt.Errorf(\u0026quot;unsupported CRI runtime: %q\u0026quot;, containerRuntime) } // 向 k8s network plugin 通报 NET_PLUGIN_EVENT_POD_CIDR_CHANGE 事件 // 该事件将会被 NetworkPlugin interface 的 Event 方法捕获 if _, err := klet.updatePodCIDR(kubeCfg.PodCIDR); err != nil { klog.Errorf(\u0026quot;Pod CIDR update failed %v\u0026quot;, err) } ... }  目前只支持 CRI 为 docker。\n根据用户配置选择 CNI // pkg/kubelet/dockershim/docker_service.go // NOTE: Anything passed to DockerService should be eventually handled in another way when we switch to running the shim as a different process. func NewDockerService(config *ClientConfig, podSandboxImage string, ...) (DockerService, error) { ds := \u0026amp;dockerService{ client: c, os: kubecontainer.RealOS{}, podSandboxImage: podSandboxImage, streamingRuntime: \u0026amp;streamingRuntime{ client: client, execHandler: \u0026amp;NativeExecHandler{}, }, containerManager: cm.NewContainerManager(cgroupsName, client), checkpointManager: checkpointManager, startLocalStreamingServer: startLocalStreamingServer, networkReady: make(map[string]bool), } // Determine the hairpin mode. if err := effectiveHairpinMode(pluginSettings); err != nil { // This is a non-recoverable error. Returning it up the callstack will just // lead to retries of the same failure, so just fail hard. return nil, err } // 根据配置配置 CNI // dockershim currently only supports CNI plugins. pluginSettings.PluginBinDirs = cni.SplitDirs(pluginSettings.PluginBinDirString) cniPlugins := cni.ProbeNetworkPlugins(pluginSettings.PluginConfDir, pluginSettings.PluginBinDirs) // 加了一个默认的 CNI 插件 kubenet cniPlugins = append(cniPlugins, kubenet.NewPlugin(pluginSettings.PluginBinDirs)) netHost := \u0026amp;dockerNetworkHost{ \u0026amp;namespaceGetter{ds}, \u0026amp;portMappingGetter{ds}, } // 根据用户配置选择对应的网络插件对象，做 init() 初始化 plug, err := network.InitNetworkPlugin(cniPlugins, pluginSettings.PluginName, netHost, pluginSettings.HairpinMode, pluginSettings.NonMasqueradeCIDR, pluginSettings.MTU) ds.network = network.NewPluginManager(plug) klog.Infof(\u0026quot;Docker cri networking managed by %v\u0026quot;, plug.Name()) return ds, nil }  Hairpin 模式\n发夹式转发模式 (Hairpin mode)又称反射式转发模式 (Reflective Relay) ，指交换机可以将报文的接受端口同时作为发送端口, 即报文可以从它的入端口转发出去, 如下图所示:\nNewDockerService() 函数首先通过 effectiveHairpinMode() 计算出有效的 Hairpin 模式, 然后根据 NetworkPluginName 从插件列表中选择对应的网络插件对象.\nProbeNetworkPlugins() 根据配置的 CNI 插件的路径生成 network.NetworkPlugin interface 的实现 cniNetworkPlugin。\nInitNetworkPlugin() 负责从网络插件对象列表中根据用户配置的网络插件名选择对应的网络插件对象，调用插件的 init() 执行初始化。\n// pkg/kubelet/dockershim/network/plugins.go // InitNetworkPlugin inits the plugin that matches networkPluginName. Plugins must have unique names. func InitNetworkPlugin(plugins []NetworkPlugin, networkPluginName string, host Host, hairpinMode kubeletconfig.HairpinMode, nonMasqueradeCIDR string, mtu int) (NetworkPlugin, error) { // 如果用户没有配置网络插件名, 默认就是NoopNetworkPlugin, 不会提供任何容器网络 // NoopNetworkPlugin 是 NetworkPlugin interface 的实现 if networkPluginName == \u0026quot;\u0026quot; { // default to the no_op plugin plug := \u0026amp;NoopNetworkPlugin{} plug.Sysctl = utilsysctl.New() if err := plug.Init(host, hairpinMode, nonMasqueradeCIDR, mtu); err != nil { return nil, err } return plug, nil } ... chosenPlugin := pluginMap[networkPluginName] if chosenPlugin != nil { // 执行插件的初始化操作 err := chosenPlugin.Init(host, hairpinMode, nonMasqueradeCIDR, mtu) } ... }  通告 Pod CIDR 的更新 k8s 对 Pod 的管理是通过 runtime 来操作的，因此对 CIDR 的更新也是通过 runtime 实现。当 Pod 的 CIDR 更新时调用 runtime 的 UpdatePodCIDR()。\n// pkg/kubelet/kubelet_network.go // updatePodCIDR updates the pod CIDR in the runtime state if it is different // from the current CIDR. Return true if pod CIDR is actually changed. func (kl *Kubelet) updatePodCIDR(cidr string) (bool, error) { // 配置与当前状态比较，没有变化直接返回 podCIDR := kl.runtimeState.podCIDR() if podCIDR == cidr { return false, nil } // kubelet -\u0026gt; generic runtime -\u0026gt; runtime shim -\u0026gt; network plugin // docker/non-cri implementations have a passthrough UpdatePodCIDR if err := kl.getRuntime().UpdatePodCIDR(cidr); err != nil { // If updatePodCIDR would fail, theoretically pod CIDR could not change. // But it is better to be on the safe side to still return true here. return true, fmt.Errorf(\u0026quot;failed to update pod CIDR: %v\u0026quot;, err) } // 更新当前状态，以便以后比较 kl.runtimeState.setPodCIDR(cidr) return true, nil }  runtime 要先讲下 k8s runtime 的管理。\nk8s 通过 kubeGenericRuntimeManager 来做统一的 RC 管理，该类会调用对应的 RC shim 来做下发操作。\nkubelet 的 containerRuntime 是在 NewMainKubelet() 函数中如下代码片段配置的。\nruntime, err := kuberuntime.NewKubeGenericRuntimeManager( kubecontainer.FilterEventRecorder(kubeDeps.Recorder), ...) klet.containerRuntime = runtime klet.streamingRuntime = runtime klet.runner = runtime  // kuberuntime/kuberuntime_manager.go // UpdatePodCIDR is just a passthrough method to update the runtimeConfig of the shim // with the podCIDR supplied by the kubelet. func (m *kubeGenericRuntimeManager) UpdatePodCIDR(podCIDR string) error { // TODO(#35531): do we really want to write a method on this manager for each // field of the config? klog.Infof(\u0026quot;updating runtime config through cri with podcidr %v\u0026quot;, podCIDR) return m.runtimeService.UpdateRuntimeConfig( \u0026amp;runtimeapi.RuntimeConfig{ NetworkConfig: \u0026amp;runtimeapi.NetworkConfig{ PodCidr: podCIDR, }, }) }  kubeGenericRuntimeManager 的 runtimeService 在初始化时设置的是 instrumentedRuntimeService，这个结构是对 RuntimeService interface 的一个封装和实现，用来记录操作和错误的 metrics。\n// instrumentedRuntimeService wraps the RuntimeService and records the operations // and errors metrics. type instrumentedRuntimeService struct { service internalapi.RuntimeService }  而真正的 RuntimeService interface 的实现是在 NewMainKubelet() 的如下片段中赋值的。\nruntimeService, imageService, err := getRuntimeAndImageServices(remoteRuntimeEndpoint, remoteImageEndpoint, kubeCfg.RuntimeRequestTimeout) klet.runtimeService = runtimeService  remoteRuntimeEndpoint 是在 kubelet 启动命令中指定的值为 unix:///var/run/dockershim.sock ，kubelet 就是通过这个 socket 与 runtime 进行gRPC 通信的。保存在 KubeletFlags 中，该参数在\ntype KubeletFlags struct { KubeConfig string ... RemoteRuntimeEndpoint string RemoteImageEndpoint string }  getRuntimeAndImageServices() 调用 NewRemoteRuntimeService() 根据 RC 的 endpoint 创建一个 gRPC 的 client 封装到 RemoteRuntimeService 中，这是一个 internalapi.RuntimeService interface 的具体实现。\n// NewRemoteRuntimeService creates a new internalapi.RuntimeService. func NewRemoteRuntimeService(endpoint string, connectionTimeout time.Duration) (internalapi.RuntimeService, error) { addr, dailer, err := util.GetAddressAndDialer(endpoint) ctx, cancel := context.WithTimeout(context.Background(), connectionTimeout) defer cancel() conn, err := grpc.DialContext(ctx, addr, grpc.WithInsecure(), grpc.WithDialer(dailer), grpc.WithDefaultCallOptions(grpc.MaxCallRecvMsgSize(maxMsgSize))) return \u0026amp;RemoteRuntimeService{ timeout: connectionTimeout, runtimeClient: runtimeapi.NewRuntimeServiceClient(conn), lastError: make(map[string]string), errorPrinted: make(map[string]time.Time), }, nil }  runtime 主要提供两种服务：RuntimeService 和 ImageService 用来管理容器的镜像。 k8s 与 runtime 通过 RPC 通信，在配置 podCIDR 时调用的是 RuntimeService 的 UpdateRuntimeConfig rpc：\n// pkg/kubelet/apis/cri/runtime/v1alpha2/api.proto service RuntimeService { ... // UpdateRuntimeConfig updates the runtime configuration based on the given request. rpc UpdateRuntimeConfig(UpdateRuntimeConfigRequest) returns (UpdateRuntimeConfigResponse) {} }  这样网络配置就下发给了 runtime，runtime 调用 CNI 插件来做网络配置变更。\n// pkg/kubelet/dockershim/docker_service.go // UpdateRuntimeConfig updates the runtime config. Currently only handles podCIDR updates. func (ds *dockerService) UpdateRuntimeConfig(_ context.Context, r *runtimeapi.UpdateRuntimeConfigRequest) (*runtimeapi.UpdateRuntimeConfigResponse, error) { runtimeConfig := r.GetRuntimeConfig() if runtimeConfig == nil { return \u0026amp;runtimeapi.UpdateRuntimeConfigResponse{}, nil } klog.Infof(\u0026quot;docker cri received runtime config %+v\u0026quot;, runtimeConfig) if ds.network != nil \u0026amp;\u0026amp; runtimeConfig.NetworkConfig.PodCidr != \u0026quot;\u0026quot; { event := make(map[string]interface{}) event[network.NET_PLUGIN_EVENT_POD_CIDR_CHANGE_DETAIL_CIDR] = runtimeConfig.NetworkConfig.PodCidr ds.network.Event(network.NET_PLUGIN_EVENT_POD_CIDR_CHANGE, event) } return \u0026amp;runtimeapi.UpdateRuntimeConfigResponse{}, nil }  下图是 kubelet runtime UML\nkubenet plugin 实现 前面知道网络插件的接口是 NetworkPlugin interface，k8s kubenet 网络框架用 embed network.NoopNetworkPlugin 的 kubenetNetworkPlugin 实现了接口。\nkubenet 利用的是官方提供的三个 cni 类型插件: bridge, host-local, loopback (参考 cni plugins, cni ipam), 这个插件一般位于每个 Node 的 /opt/cni/bin 目录。\ntype kubenetNetworkPlugin struct { network.NoopNetworkPlugin host network.Host netConfig *libcni.NetworkConfig loConfig *libcni.NetworkConfig cniConfig libcni.CNI bandwidthShaper bandwidth.BandwidthShaper mu sync.Mutex //Mutex for protecting podIPs map, netConfig, and shaper initialization podIPs map[kubecontainer.ContainerID]string mtu int execer utilexec.Interface nsenterPath string hairpinMode kubeletconfig.HairpinMode // kubenet can use either hostportSyncer and hostportManager to implement hostports // Currently, if network host supports legacy features, hostportSyncer will be used, // otherwise, hostportManager will be used. hostportSyncer hostport.HostportSyncer hostportManager hostport.HostPortManager iptables utiliptables.Interface sysctl utilsysctl.Interface ebtables utilebtables.Interface // binDirs is passed by kubelet cni-bin-dir parameter. // kubenet will search for CNI binaries in DefaultCNIDir first, then continue to binDirs. binDirs []string nonMasqueradeCIDR string podCidr string gateway net.IP }  kubenet 直接利用了官方提供的三个 cni plugin:\n// pkg/kubelet/network/kubenet/kubenet_linux.go // CNI plugins required by kubenet in /opt/cni/bin or vendor directory var requiredCNIPlugins = [...]string{\u0026quot;bridge\u0026quot;, \u0026quot;host-local\u0026quot;, \u0026quot;loopback\u0026quot;}  kubenet 网络框架原理非常的简单, 主要利用 \u0026ldquo;bridge\u0026rdquo;, \u0026ldquo;host-local\u0026rdquo;, \u0026ldquo;loopback\u0026rdquo; (位于 /opt/cni/bin 目录下) 这三个 cni plugin主要的功能：\n 在每个 Node 上创建一个 cbr0 网桥 根据 PodCIDR 为每个 Pod 的 interface 分配一个 ip, 将该 interface 连接到 cbr0 网桥上.  当然, 对于 kubernetes 集群来说, 还需要解决两个问题:\n Node 的 PodCIDR 设置\nk8s kubenet 网络框架中，必须给每个 node 配置一个 podCIDR.\n那么, 每个 Node 的 PodCIDR 如何设置呢? 这个需要参考 kubenet 网络的配置文档了:\n The node must be assigned an IP subnet through either the \u0026ndash;pod-cidr kubelet command-line option or the \u0026ndash;allocate-node-cidrs=true \u0026ndash;cluster-cidr= controller-manager command-line options.\n 其实就是两种方式:\n 通过 \u0026ndash;pod-cidr 为每个 Node 上的 kubelet 配置好 PodCIDR 通过 \u0026ndash;allocate-node-cidrs=true \u0026ndash;cluster-cidr= 让 controller-manager 来为每个 Node 分配 PodCIDR.  Node 之间的路由设置\n虽然现在每个 Node 都配置好了 PodCIDR, 比如:\nNode1: 192.168.0.0/24 Node2: 192.168.1.0/24  但是 Node1 和 Node2 上的容器如何通信呢?\n It is typically used together with a cloud provider that sets up routing rules for communication between nodes, or in single-node environments.\n 通常情况下, kubenet 网络插件会跟 cloud provider 一起使用, 从而利用 cloud provider 来设置节点间的路由. kubenet 网络插件也可以用在单节点环境, 这样就不需要考虑 Node 间的路由了. 另外, 我们还可以通过实现一个 network controller 来保证 Node 间的路由.\n  kubenet Init // pkg/kubelet/dockershim/network/kubenet/kubenet_linux.go func NewPlugin(networkPluginDirs []string) network.NetworkPlugin { protocol := utiliptables.ProtocolIpv4 execer := utilexec.New() dbus := utildbus.New() sysctl := utilsysctl.New() iptInterface := utiliptables.New(execer, dbus, protocol) return \u0026amp;kubenetNetworkPlugin{ podIPs: make(map[kubecontainer.ContainerID]string), execer: utilexec.New(), iptables: iptInterface, sysctl: sysctl, binDirs: append([]string{DefaultCNIDir}, networkPluginDirs...), hostportSyncer: hostport.NewHostportSyncer(iptInterface), hostportManager: hostport.NewHostportManager(iptInterface), nonMasqueradeCIDR: \u0026quot;10.0.0.0/8\u0026quot;, } }  在前面的 InitNetworkPlugin() 流程中会调用各个插件的 Init() 来初始化插件。\n// pkg/kubelet/dockershim/network/kubenet/kubenet_linux.go func (plugin *kubenetNetworkPlugin) Init(host network.Host, hairpinMode kubeletconfig.HairpinMode, nonMasqueradeCIDR string, mtu int) error { // 配置 MTU todo 为什么这里要设置 MTU？ ... // 确认加载了 br-netfilter，设置 bridge-nf-call-iptables=1 plugin.execer.Command(\u0026quot;modprobe\u0026quot;, \u0026quot;br-netfilter\u0026quot;).CombinedOutput() err := plugin.sysctl.SetSysctl(sysctlBridgeCallIPTables, 1) // 配置 loopback cni 插件 plugin.loConfig, err = libcni.ConfFromBytes([]byte(`{ \u0026quot;cniVersion\u0026quot;: \u0026quot;0.1.0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;kubenet-loopback\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;loopback\u0026quot; }`)) plugin.nsenterPath, err = plugin.execer.LookPath(\u0026quot;nsenter\u0026quot;) // 下发 SNAT 的 ipatable rule // Need to SNAT outbound traffic from cluster if err = plugin.ensureMasqRule(); err != nil { return err } return nil }  kubenet Event kubelet 启动到 NewMainKubelet 时, 根据用户配置通过 klet.updatePodCIDR(kubeCfg.PodCIDR) 向 k8s network plugin 通报 NET_PLUGIN_EVENT_POD_CIDR_CHANGE 事件, 该事件将会被 Event 方法捕获.\n// pkg/kubelet/network/kubenet/kubenet_linux.go const NET_CONFIG_TEMPLATE = `{ \u0026quot;cniVersion\u0026quot;: \u0026quot;0.1.0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;kubenet\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;bridge\u0026quot;, \u0026quot;bridge\u0026quot;: \u0026quot;%s\u0026quot;, \u0026quot;mtu\u0026quot;: %d, \u0026quot;addIf\u0026quot;: \u0026quot;%s\u0026quot;, \u0026quot;isGateway\u0026quot;: true, \u0026quot;ipMasq\u0026quot;: false, \u0026quot;hairpinMode\u0026quot;: %t, \u0026quot;ipam\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;host-local\u0026quot;, \u0026quot;subnet\u0026quot;: \u0026quot;%s\u0026quot;, \u0026quot;gateway\u0026quot;: \u0026quot;%s\u0026quot;, \u0026quot;routes\u0026quot;: [ { \u0026quot;dst\u0026quot;: \u0026quot;0.0.0.0/0\u0026quot; } ] } }` func (plugin *kubenetNetworkPlugin) Event(name string, details map[string]interface{}) { podCIDR, ok := details[network.NET_PLUGIN_EVENT_POD_CIDR_CHANGE_DETAIL_CIDR].(string) _, cidr, err := net.ParseCIDR(podCIDR) if err == nil { setHairpin := plugin.hairpinMode == kubeletconfig.HairpinVeth // Set bridge address to first address in IPNet cidr.IP[len(cidr.IP)-1] += 1 // 更新 cni 网络配置 // 从 NET_CONFIG_TEMPLATE 中看出, host-local ipam 的 subnet 就是 podCIDR // 这其实也就是为什么 k8s kubenet 网络插件需要为每个 node 分配 podCIDR 的原因 json := fmt.Sprintf(NET_CONFIG_TEMPLATE, BridgeName, plugin.mtu, network.DefaultInterfaceName, setHairpin, podCIDR, cidr.IP.String()) // 网络配置都保存在 netConfig 中 plugin.netConfig, err = libcni.ConfFromBytes([]byte(json)) if err == nil { klog.V(5).Infof(\u0026quot;CNI network config:\\n%s\u0026quot;, json) // Ensure cbr0 has no conflicting addresses; CNI's 'bridge' // plugin will bail out if the bridge has an unexpected one plugin.clearBridgeAddressesExcept(cidr) } plugin.podCidr = podCIDR plugin.gateway = cidr.IP } }   todo: Event() 也只是更新了 podCidr 和 netConfig，哪里下发了更新？\n 根据配置的改变设置 kubenetNetworkPlugin 对应的变量。\nkubenet SetUpPod 创建 Pod 的时候会调用该方法，该方法调用 setup() 来完成配置，这个接口最重要的功能是将容器的 eth0 接口加入到了 namespace 中\n// setup sets up networking through CNI using the given ns/name and sandbox ID. func (plugin *kubenetNetworkPlugin) setup(namespace string, name string, id kubecontainer.ContainerID, annotations map[string]string) error { // 添加 loopback interface 到 pod 的 network namespace // Bring up container loopback interface if _, err := plugin.addContainerToNetwork(plugin.loConfig, \u0026quot;lo\u0026quot;, namespace, name, id); err != nil { return err } // 添加 DefaultInterfaceName eth0 到 pod 的 network namespace // Hook container up with our bridge resT, err := plugin.addContainerToNetwork(plugin.netConfig, network.DefaultInterfaceName, namespace, name, id) if err != nil { return err } // Coerce the CNI result version res, err := cnitypes020.GetResult(resT) ip4 := res.IP4.IP.IP.To4() // 为了配置 hairpin 设置网卡混杂模式 ... plugin.podIPs[id] = ip4.String() // TODO: replace with CNI port-forwarding plugin // TODO: portMappings 的用途是什么？ portMappings, err := plugin.host.GetPodPortMappings(id.ID) if err != nil { return err } if portMappings != nil \u0026amp;\u0026amp; len(portMappings) \u0026gt; 0 { if err := plugin.hostportManager.Add(id.ID, \u0026amp;hostport.PodPortMapping{ Namespace: namespace, Name: name, PortMappings: portMappings, IP: ip4, HostNetwork: false, }, BridgeName); err != nil { return err } } return nil }  接着看看 addContainerToNetwork() 方法:\n// pkg/kubelet/dockershim/network/kubenet/kubenet_linux.go func (plugin *kubenetNetworkPlugin) addContainerToNetwork(config *libcni.NetworkConfig, ifName, namespace, name string, id kubecontainer.ContainerID) (cnitypes.Result, error) { rt, err := plugin.buildCNIRuntimeConf(ifName, id, true) if err != nil { return nil, fmt.Errorf(\u0026quot;Error building CNI config: %v\u0026quot;, err) } // The network plugin can take up to 3 seconds to execute, // so yield the lock while it runs. plugin.mu.Unlock() res, err := plugin.cniConfig.AddNetwork(config, rt) plugin.mu.Lock() return res, nil }  由前面 CNI 库接口可知, plugin.cniConfig.AddNetwork() 实际上调用的是 cni plugin 去实现容器网络配置. kubenet plugin 主要通过 loopback 和 bridge cni 插件将容器的 lo 和 eth0 添加到容器网络中. bridge 插件负责 Node 上 cbr0 的创建, 然后创建 veth 接口对, 通过 veth 接口对, 将容器添加到容器网络中. 另外, host-local IPAM plugin 负责为 eth0 分配 ip 地址.\nkubenet TearDownPod 删除 Pod 的时候会被调用。主要是通过函数 teardown() 实现。主要的流程是调用 CNI 删除网络配置。\n// Tears down as much of a pod's network as it can even if errors occur. Returns // an aggregate error composed of all errors encountered during the teardown. func (plugin *kubenetNetworkPlugin) teardown(namespace string, name string, id kubecontainer.ContainerID, podIP string) error { errList := []error{} if err := plugin.delContainerFromNetwork(plugin.netConfig, network.DefaultInterfaceName, namespace, name, id); err != nil { // This is to prevent returning error when TearDownPod is called twice on the same pod. This helps to reduce event pollution. if podIP != \u0026quot;\u0026quot; { klog.Warningf(\u0026quot;Failed to delete container from kubenet: %v\u0026quot;, err) } else { errList = append(errList, err) } } portMappings, err := plugin.host.GetPodPortMappings(id.ID) if err != nil { errList = append(errList, err) } else if portMappings != nil \u0026amp;\u0026amp; len(portMappings) \u0026gt; 0 { if err = plugin.hostportManager.Remove(id.ID, \u0026amp;hostport.PodPortMapping{ Namespace: namespace, Name: name, PortMappings: portMappings, HostNetwork: false, }); err != nil { errList = append(errList, err) } } return utilerrors.NewAggregate(errList) }  由前面 CNI 库接口可知, plugin.cniConfig.DelNetwork() 实际上调用的是 cni plugin 去删除容器网络配置. bridge 插件负责调用 host-local IPAM plugin 释放该容器的 ip, 然后删除容器的网络接口等.\nCNI plugin 实现 CNI plugin 是一种更通用的实现，允许用户自定义插件。cniNetworkPlugin 是 NetworkPlugin interface 的一个实现，具体的代码如下。\n// pkg/kubelet/dockershim/network/cni/cni.go type cniNetworkPlugin struct { network.NoopNetworkPlugin loNetwork *cniNetwork sync.RWMutex defaultNetwork *cniNetwork host network.Host execer utilexec.Interface nsenterPath string confDir string binDirs []string podCidr string }  通过 cniNetwork 类型的 loNetwork 和 defaultNetwork 来调用 CNI 插件，cniNetwork 定义如下。\n// pkg/kubelet/dockershim/network/cni/cni.go type cniNetwork struct { name string NetworkConfig *libcni.NetworkConfigList CNIConfig libcni.CNI }  CNI Init 在 NewDockerService() 函数中调用 ProbeNetworkPlugins() 根据配置的 CNI 插件的路径生成 network.NetworkPlugin interface 的实现 cniNetworkPlugin。\n// pkg/kubelet/dockershim/network/cni/cni.go func ProbeNetworkPlugins(confDir string, binDirs []string) []network.NetworkPlugin { old := binDirs binDirs = make([]string, 0, len(binDirs)) for _, dir := range old { if dir != \u0026quot;\u0026quot; { binDirs = append(binDirs, dir) } } plugin := \u0026amp;cniNetworkPlugin{ defaultNetwork: nil, loNetwork: getLoNetwork(binDirs), execer: utilexec.New(), confDir: confDir, binDirs: binDirs, } // sync NetworkConfig in best effort during probing. plugin.syncNetworkConfig() return []network.NetworkPlugin{plugin} }  主要是对 loNetwork 和 defaultNetwork 变量的配置。\n// pkg/kubelet/dockershim/network/cni/cni_others.go func getLoNetwork(binDirs []string) *cniNetwork { loConfig, err := libcni.ConfListFromBytes([]byte(`{ \u0026quot;cniVersion\u0026quot;: \u0026quot;0.2.0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;cni-loopback\u0026quot;, \u0026quot;plugins\u0026quot;:[{ \u0026quot;type\u0026quot;: \u0026quot;loopback\u0026quot; }] }`)) loNetwork := \u0026amp;cniNetwork{ name: \u0026quot;lo\u0026quot;, NetworkConfig: loConfig, CNIConfig: \u0026amp;libcni.CNIConfig{Path: binDirs}, } return loNetwork }  Init() 做的就是配置 defaultNetwork\nfunc getDefaultCNINetwork(confDir string, binDirs []string) (*cniNetwork, error) { // 从配置文件获取 confList network := \u0026amp;cniNetwork{ name: confList.Name, NetworkConfig: confList, CNIConfig: \u0026amp;libcni.CNIConfig{Path: binDirs}, } return network, nil } }  CNI Event 收到 NET_PLUGIN_EVENT_POD_CIDR_CHANGE 事件时只是更新了 podCidr 的值\nfunc (plugin *cniNetworkPlugin) Event(name string, details map[string]interface{}) { podCIDR, ok := details[network.NET_PLUGIN_EVENT_POD_CIDR_CHANGE_DETAIL_CIDR].(string) plugin.podCidr = podCIDR }  可见 k8s cni 网络方式并没有规定使用 podCidr 来配置 node 上容器的网络 ip 段, 而把 pod 的 ip 分配完全交给 IPAM, 这样使得 IPAM 更加灵活, 多样化和定制化\nCNI SetUpPod // pkg/kubelet/dockershim/network/cni/cni.go func (plugin *cniNetworkPlugin) SetUpPod(namespace string, name string, id kubecontainer.ContainerID, annotations, options map[string]string) error { ... // Windows doesn't have loNetwork. It comes only with Linux if plugin.loNetwork != nil { if _, err = plugin.addToNetwork(plugin.loNetwork, name, namespace, id, netnsPath, annotations, options); err != nil { return err } } _, err = plugin.addToNetwork(plugin.getDefaultNetwork(), name, namespace, id, netnsPath, annotations, options) return err }  func (plugin *cniNetworkPlugin) addToNetwork(network *cniNetwork, podName string, podNamespace string, podSandboxID kubecontainer.ContainerID, podNetnsPath string, annotations, options map[string]string) (cnitypes.Result, error) { netConf, cniNet := network.NetworkConfig, network.CNIConfig res, err := cniNet.AddNetworkList(netConf, rt) }  由前面 CNI 库接口可知, cninet.AddNetwork() 实际上调用的是底层用户配置的 cni plugin 去实现容器网络配置.\nCNI TearDownPod 与前面的流程类似，最终调用底层的 cni plugin 删掉配置。代码略。\nCNI 演进 多 interface 支持 目前 Kubernetes CNI 网络模型中还不支持多个 interface，社区关于这方面的讨论：\n Multiple interfaces in a POD Support multiple pod IP addresses README: List multus as 3rd party plugin Multiple network support  多 CNI plugin 支持 目前只支持单个 CNI plugin，关于 CNI plugin chain的讨论：\n Is there any reference yaml file applying multus-cni for pod network? MULTUS CNI plugin Update CNI plugin to newest version; support ConfigLists  参考 k8s network\n问题  dockerService 用途是什么？  ","date":1559001600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559001600,"objectID":"631156bfce891192f2b6a9b8eef15b66","permalink":"/post/cloud/k8s/201905-k8s-network-arch/","publishdate":"2019-05-28T00:00:00Z","relpermalink":"/post/cloud/k8s/201905-k8s-network-arch/","section":"post","summary":"Kubernetes本身不提供容器网络，但具有可扩展的网络框架","tags":["Kubernetes","CNI"],"title":"Kubernetes网络框架","type":"post"},{"authors":null,"categories":null,"content":" 人们对云计算提出了更高的要求，为大量项目构建运营环境的效率问题，缩短新业务的上线部署时间，大规模的计算机房快速迁移需求；提高服务器资源的利用率，同时确保相同的性能和可用性，又有降低成本的需求。相较于传统的虚拟化解决方案，容器云可以较好的实现上述目标。\n容器云的核心功能：\n 快速扩容 智能调度和编排 弹性伸缩  快速扩容  扩容速度：  VM - 扩容20个实例需要4分钟（扩容完成后需要再执行服务发布） docker - 扩容20个实例仅需30s，秒级别扩容（扩容完成即服务启动）  扩容速度提高 8~12倍 节约了用户手动操作申请/发布的成本  智能调度 调度系统是云集群的中央处理器，要解决的核心问题是为容器选择合适的宿主机。有如下的指标：\n 资源利用率，提高整体物理集群的资源利用率 业务可用性保障：业务容器容灾能力、保障运行业务的稳定高可用 并发调度能力：调度系统请求处理能力的体现  资源最大化利用  按CPU/Mem/IO等类型对服务进行调度，最大化资源利用 业务按需使用资源，提升资源利用率  混布与独占  在线服务与离线任务混布 重要业务资源池独占  容器编排  有调用关系的多个服务实例，优先部署到相同/相近的宿主机上 同服务实例打散，分布到不同宿主机上，提高服务可用性 高负载容器，自动迁移到低负载宿主机 自动化容器实例健康检查，异常实例自动迁移  调度计算 通过先过滤filter之后排序打分rank的方式找到最优的部署位置。\n在一批宿主机中先过滤掉超售的，然后考虑到打散、混部、减少碎片和负载均衡之后找到合适的宿主机\n调度SLA（Service Level Agreement）  高可用：99.999 调度成功率：99.99 并发调度：单机并发处理200+，并发调度机器1000+ 低延迟：TCP90 63ms HA：分布式调度，横向扩展，多IDC部署容灾 监控报警：Falcon  弹性收缩 周期收缩\n根据设定时间段伸缩（适合秒杀/直播等业务）\n监控伸缩\n 根据QPS/CPU等触发条件伸缩 线性可扩展的无状态服务  服务画像\n针对数据建模，描绘服务特征：\n 服务画像：仿照用户画像，根据服务数据，抽取服务Tag  QPS特征（高峰时段、QPS max/min等） 资源利用率 CPU密集型 or IO密集型   基于历史数据建模的服务画像可以做服务特征值的预测，比如QPS的预测：\n QPS预测：RNN LSTM 即使监控数据源完全不可用，无数据，也能较准确的扩缩容  异常处理  监控数据异常，怎么办？会不会因监控值偏低而一直缩容？ 监控数据有延迟，怎么办？ 监控数据没了，怎么办？  通过数据无关的缩容退避+熔断机制来保证异常情况下的正常运行：\n 针对监控数据偏低（异常）而触发持续缩容 数据无关，不关心数据是否异常 如果连续缩容，那么缩容速度会越来越慢 —\u0026gt; 退避 如果连续缩容次数超过阈值，一段时间内禁止缩容 —\u0026gt; 熔断  ","date":1557158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557158400,"objectID":"318d50b7d7643d72671620a688ce7837","permalink":"/post/cloud/container/201905-why-container-cloud/why-container/","publishdate":"2019-05-07T00:00:00+08:00","relpermalink":"/post/cloud/container/201905-why-container-cloud/why-container/","section":"post","summary":"容器云解决大量项目构建运营环境的效率问题","tags":["Docker","Cloud"],"title":"Why 容器云","type":"post"},{"authors":null,"categories":null,"content":" API对象 在Kubernetes中API对象是以树形结构表示的，一个API对象在Etcd里完整资源路径，是由Group（API组）、Version（API版本）和Resource（API资源类型）三个部分组成的。\n如果现在要声明一个CronJob对象，那么YAML的开始部分会这么写，CronJob就是这个API对象的资源类型，Batch就是它们的组，v2alpha1就是它的版本\napiVersion: batch/v2alpha1 kind: CronJob ...  API解析 Kubernetes通过对API解析找到对应的对象，分为如下3步：\n 解析API的组 Kubernetes的对象分两种：  核心API对象（如Pod、Node），是不需要Group的，直接在 /api这个下面进行解析 非核心API对象，在 /apis 下先解析出Group，根据batch这个Group找到 /apis/batch，API Group的分类是以对象功能为依据的。  解析API对象的版本号 匹配API对象的资源类型  创建对象 在前面匹配到正确的版本之后，Kubernetes就知道要创建的是一个/apis/batch/v2alpha1下的CronJob对象，APIServer会继续创建这个Cronjob对象。创建过程如下图\n 当发起创建CronJob的POST请求之后，YAML的信息就被提交给了APIServer，APIServer的第一个功能就是过滤这个请求，并完成一些前置性的工作，比如授权、超时处理、审计等 请求进入MUX和Routes流程，MUX和Routes是APIServer完成URL和Handler绑定的场所。APIServer的Handler要做的事情，就是按照上面介绍的匹配过程，找到对应的CronJob类型定义。 根据这个CronJob类型定义，使用用户提交的YAML文件里的字段，创建一个CronJob对象。这个过程中，APIServer会把用户提交的YAML文件，转换成一个叫做Super Version的对象，它正是该API资源类型所有版本的字段全集，这样用户提交的不同版本的YAML文件，就都可以用这个SuperVersion对象来进行处理了。 APIServer会先后进行Admission（如Admission Controller 和 Initializer）和Validation操作（负责验证这个对象里的各个字段是否何方，被验证过得API对象都保存在APIServer里一个叫做Registry的数据结构中）。 APIServer会把验证过得API对象转换成用户最初提交的版本，进行系列化操作，并调用Etcd的API把它保存起来。\n  CRD API插件CRD（Custom Resource Definition） 允许用户在Kubernetes中添加一个跟Pod、Node类似的、新的API资源类型，即：自定义API资源\n举个栗子，添加一个叫Network的API资源类型，它的作用是一旦用户创建一个Network对象，那么Kubernetes就可以使用这个对象定义的网络参数，调用真实的网络插件，为用户创建一个真正的网络，这个过程分为两步\n 首先定义CRD  定义一个group为samplecrd.k8s.io， version为v1的API信息，指定了这个CR的资源类型叫做Network，定义的这个Network是属于一个Namespace的对象，类似于Pod。\napiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: networks.samplecrd.k8s.io spec: group: samplecrd.k8s.io version: v1 names: kind: Network plural: networks scope: Namespaced   对象实例化  实例化名为example-network的Network对象，API组是samplecrd.k8s.io，版本是v1。\napiVersion: samplecrd.k8s.io/v1 kind: Network metadata: name: example-network spec: cidr: \u0026quot;192.168.0.0/16\u0026quot; gateway: \u0026quot;192.168.0.1\u0026quot;  Network对象YAML文件，名叫example-network.yaml,API资源类型是Network，API组是samplecrd.k8s.io，版本是v1\nKubernetes的声明式API能够对API对象进行增量的更新操作：\n 定义好期望的API对象后，Kubernetes来尽力让对象的状态符合预期 允许多个YAML表达，以PATCH的方式对API对象进行修改，而不用关心原始YAML的内容  基于上面两种特性，Kubernetes可以实现基于API对象的更删改查，完成预期和定义的协调过程。\n因此Kubernetes项目编排能力的核心是声明式API。\nKubernetes编程范式即：如何使用控制器模式，同Kubernetes里的API对象的“增、删、改、查”进行协作，进而完成用户业务逻辑的编写过程。\nkubectl apply kubectl apply是声明式的请求，下面一个Deployment的YAML的例子\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80  然后用kubectl apply创建这个Deployment\n$ kubectl apply -f nginx.yaml  修改一下nginx里定义的镜像\napiVersion: apps/v1 kind: Deployment ... spec: containers: - name: nginx image: nginx:1.7.9  执行kubectl apply命令，触发滚动更新\n$ kubectl apply -f nginx.yaml  后面一次的 kubectl apply命令执行了一个对原有API对象的PATCH操作，这是声明式命令同时可以进行多个写操作，具有Merge的能力；而像 kubectl replace命令是用新的YAML替换旧的，这种响应式命令每次只能处理一次写操作。\n声明式API的应用 Istio通过声明式API实现对应用容器所在POD注入Sidecar，然后通过iptables劫持POD的进站和出站流量到Sidecar，Istio通过对Sidecar下发策略来实现对应用流量的管控，继而实现微服务治理。\n在微服务治理中，对Envoy容器的部署和对Envoy代理的配置，应用容器都是不感知的。Istio是使用Kubernetes的Dynamic Admission Control来实现的。\n在APIServer收到API对象的提交请求后，在正常处理这些操作之前会做一些初始化的操作，比如为某些pod或容器加上一些label。这些初始化操作是通过Kubernetes的Admission Controller实现的，在APIServer对象创建之后调用，但这种方式的缺陷是需要将Admission Controller的代码编译到APIServer中，这不是很方便。Kubernetes 1.7引入了热插拔的Admission机制，它就是Dynamic Admission Control，也叫做Initializer。\n如下定义的应用的Pod，包含一个myapp-container的容器。\napiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026amp;\u0026amp; sleep 3600']  Istio要做的就是在这个Pod YAML被提交给Kubernetes之后，在它对应的API对象里自动加上Envoy容器的配置，使对象变成如下的样子：\napiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026amp;\u0026amp; sleep 3600'] - name: envoy image: lyft/envoy:845747b88f102c0fd262ab234308e9e22f693a1 command: [\u0026quot;/usr/local/bin/envoy\u0026quot;] ...  这个pod多了一个envoy的容器，Istio具体的做法是\n 定义Envoy容器的Initializer，并以ConfigMap的方式保存到Kubernetes中 Istio将编写好的Initializer作为一个Pod部署在Kubernetes中  Envoy容器的ConfigMap定义，\napiVersion: v1 kind: ConfigMap metadata: name: envoy-initializer data: config: | containers: - name: envoy image: lyft/envoy:845747db88f102c0fd262ab234308e9e22f693a1 command: [\u0026quot;/usr/local/bin/envoy\u0026quot;] args: - \u0026quot;--concurrency 4\u0026quot; - \u0026quot;--config-path /etc/envoy/envoy.json\u0026quot; - \u0026quot;--mode serve\u0026quot; ports: - containerPort: 80 protocol: TCP resources: limits: cpu: \u0026quot;1000m\u0026quot; memory: \u0026quot;512Mi\u0026quot; requests: cpu: \u0026quot;100m\u0026quot; memory: \u0026quot;64Mi\u0026quot; volumeMounts: - name: envoy-conf mountPath: /etc/envoy volumes: - name: envoy-conf configMap: name: envoy  这个ConfigMap的data部分，正是一个Pod对象的一部分定义，其中可以看到Envoy容器对应的Container字段，以及一个用来声明Envoy配置文件的volumes字段。Initializer要做的就是把这部分Envoy相关的字段，自动添加到用户提交的Pod的API对象里。但是用户提交的Pod里本来就有containers和volumes字段，所以Kubernetes在处理这样的更新请求时，就必须使用类似于git merge这样的操作，才能将这两部分内容合并在一起。即Initializer更新用户的Pod对象时，必须使用PATCH API来完成。\nEnvoy Initializer的pod定义\napiVersion: v1 kind: Pod metadata: labels: app: envoy-initializer name: envoy-initializer spec: containers: - name: envoy-initializer image: envoy-initializer:0.0.1 imagePullPolicy: Always ```\t`envoy-initializer:0.0.1` 镜像是一个自定义控制器（Custom Controller）。Kubernetes的控制器实际上是一个死循环：它不断地获取实际状态，然后与期望状态作对比，并以此为依据决定下一步的操作。 对Initializer控制器，不断获取的实际状态，就是用户新创建的Pod，它期望的状态就是这个Pod里被添加了Envoy容器的定义。它的控制逻辑如下： ```go for { // 获取新创建的 Pod pod := client.GetLatestPod() // Diff 一下，检查是否已经初始化过 if !isInitialized(pod) { // 没有？那就来初始化一下 //istio要往这个Pod里合并的字段，就是ConfigMap里data字段的值 doSomething(pod) } } func doSomething(pod) { //调用APIServer拿到ConfigMap cm := client.Get(ConfigMap, \u0026quot;envoy-initializer\u0026quot;) //把ConfigMap里存在的containers和volumes字段，直接添加进一个空的Pod对象 newPod := Pod{} newPod.Spec.Containers = cm.Containers newPod.Spec.Volumes = cm.Volumes // Kubernetes的API库，提供一个方法使我们可以直接使用新旧两个Pod对象，生成 patch 数据 patchBytes := strategicpatch.CreateTwoWayMergePatch(pod, newPod) // 发起 PATCH 请求，修改这个 pod 对象 client.Patch(pod.Name, patchBytes) }  Envoy机制正是利用了Kubernetes能够对API对象做增量更新，这是Kubernetes声明式API的独特之处。\n参考 Dynamic Admission Control\n【Kubernetes】深入解析声明式API\n","date":1545753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545753600,"objectID":"a932d713bc6cbd242ccbb247be2576d4","permalink":"/post/cloud/k8s/201904-k8s-declarative-api/","publishdate":"2018-12-26T00:00:00+08:00","relpermalink":"/post/cloud/k8s/201904-k8s-declarative-api/","section":"post","summary":"声明式API是Kubernetes成为容器编排事实标准的利器","tags":["Kubernetes"],"title":"Kubernetes声明式API","type":"post"}]