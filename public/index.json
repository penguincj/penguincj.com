[{"authors":["admin"],"categories":null,"content":"专注于基础架构，喜欢 Cloud Native，希望成为坚守开发一线打磨匠艺的架构师。\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"专注于基础架构，喜欢 Cloud Native，希望成为坚守开发一线打磨匠艺的架构师。","tags":null,"title":"L CJ","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"d9975920f3fb24c07bdc68e5a0f70299","permalink":"/istio/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/istio/example/","section":"istio","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"2d3419a1f0960318ce200b192e83a046","permalink":"/istio/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/istio/example/example1/","section":"istio","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"46c1c50a9cb1a6cc9bdb431c8244fc44","permalink":"/istio/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/istio/example/example2/","section":"istio","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":null,"categories":null,"content":" 根据前面的文章 容器组件 我们知道组件之间的关系如下，本文通过 docker update 命令来追寻各组件之间的调用关系。\n图，docker 组件之间的调用关系\n比如新增一个自定义的属性 cacheProperty，并且支持该属性的 update。\ndocker update --cacheProperty test abebf7571666  docker client 在调用上面的 docker update 命令时首先调用的是 docker client，会进入到如下的流程。\n// cli/command/container/update.go func runUpdate(dockerCli *command.DockerCli, opts *updateOptions) error { resources := containertypes.Resources{ ... CPURealtimePeriod: opts.cpuRealtimePeriod, CacheProperty: opts.cacheProperty, } updateConfig := containertypes.UpdateConfig{ Resources: resources, RestartPolicy: restartPolicy, } for _, container := range opts.containers { r, err := dockerCli.Client().ContainerUpdate(ctx, container, updateConfig) ...... } }  // client/container_update.go func (cli *Client) ContainerUpdate(ctx context.Context, containerID string, updateConfig container.UpdateConfig) (container.ContainerUpdateOKBody, error) { serverResp, err := cli.post(ctx, \u0026quot;/containers/\u0026quot;+containerID+\u0026quot;/update\u0026quot;, nil, updateConfig, nil) }  上面的流程组装了一个 /containers/\u0026quot;+containerID+\u0026quot;/update 的API 请求。\nfunc (cli *Client) post(ctx context.Context, path string, query url.Values, obj interface{}, headers map[string][]string) (serverResponse, error) { body, headers, err := encodeBody(obj, headers) if err != nil { return serverResponse{}, err } return cli.sendRequest(ctx, \u0026quot;POST\u0026quot;, path, query, body, headers) }  func (cli *Client) doRequest(ctx context.Context, req *http.Request) (serverResponse, error) { serverResp := serverResponse{statusCode: -1} resp, err := ctxhttp.Do(ctx, cli.client, req) ... }  server //api/server/router/container/container.go func (r *containerRouter) initRoutes() { router.NewPostRoute(\u0026quot;/containers/{name:.*}/update\u0026quot;, r.postContainerUpdate), }  // container_routes.go func (s *containerRouter) postContainerUpdate(ctx context.Context, w http.ResponseWriter, r *http.Request, vars map[string]string) error { resp, err := s.backend.ContainerUpdate(name, hostConfig) }  // daemon/update.go func (daemon *Daemon) ContainerUpdate(name string, hostConfig *container.HostConfig) (container.ContainerUpdateOKBody, error) { var warnings []string if err := daemon.update(name, hostConfig); err != nil { return container.ContainerUpdateOKBody{Warnings: warnings}, err } }  func (daemon *Daemon) update(name string, hostConfig *container.HostConfig) error { // 更新container的配置 if err := container.UpdateContainer(hostConfig); err != nil { restoreConfig = true return errCannotUpdate(container.ID, err) } // 实时更新continer的状态 if container.IsRunning() \u0026amp;\u0026amp; !container.IsRestarting() { if err := daemon.containerd.UpdateResources(container.ID, toContainerdResources(hostConfig.Resources)); err != nil { restoreConfig = true return errCannotUpdate(container.ID, err) } } }  // container/container_unix.go func (container *Container) UpdateContainer(hostConfig *containertypes.HostConfig) error { container.Lock() defer container.Unlock() // update resources of container resources := hostConfig.Resources cResources := \u0026amp;container.HostConfig.Resources if resources.BlkioWeight != 0 { cResources.BlkioWeight = resources.BlkioWeight } if resources.cacheProperty != \u0026quot;\u0026quot; { cResources.CacheProperty = resources.CacheProperty } }  // libcontainerd/client_linux.go func (clnt *client) UpdateResources(containerID string, resources Resources) error { ... _, err = clnt.remote.apiClient.UpdateContainer(context.Background(), \u0026amp;containerd.UpdateContainerRequest{ Id: containerID, Pid: InitFriendlyName, Resources: (*containerd.UpdateResource)(\u0026amp;resources), }) }  通过 RPC 向 containerd 调用 UpdateContainer，proto 定义如下\n// containerd/api/grpc/types/api.proto service API { ... rpc UpdateContainer(UpdateContainerRequest) returns (UpdateContainerResponse) {} }  proto 生成的代码如下\n// containerd/api/grpc/types/api.pb.go func (c *aPIClient) UpdateContainer(ctx context.Context, in *UpdateContainerRequest, opts ...grpc.CallOption) (*UpdateContainerResponse, error) { out := new(UpdateContainerResponse) err := grpc.Invoke(ctx, \u0026quot;/types.API/UpdateContainer\u0026quot;, in, out, c.cc, opts...) if err != nil { return nil, err } return out, nil }  // daemon/update_linux.go func toContainerdResources(resources container.Resources) libcontainerd.Resources { var r libcontainerd.Resources ... r.MemoryReservation = uint64(resources.MemoryReservation) r.KernelMemoryLimit = uint64(resources.KernelMemory) r.CacheProperty = uint64(resources.CacheProperty) return r }  需要在 containerd.UpdateResource 中增加 CacheProperty 字段属性。\n// containerd/api/grpc/types/api.pb.go type UpdateResource struct { CpuPeriod uint64 `protobuf:\u0026quot;varint,3,opt,name=cpuPeriod\u0026quot; json:\u0026quot;cpuPeriod,omitempty\u0026quot;` CpuQuota uint64 `protobuf:\u0026quot;varint,4,opt,name=cpuQuota\u0026quot; json:\u0026quot;cpuQuota,omitempty\u0026quot;` ... CacheProperty uint64 `protobuf:\u0026quot;varint,19,opt,name=...\u0026quot; json:\u0026quot;...,omitempty\u0026quot;` }  containerd containerd 作为 gRPC server，接收 dockerd 的请求，具体的流程如下。\n// api/grpc/server/server.go func (s *apiServer) UpdateContainer(ctx context.Context, r *types.UpdateContainerRequest) (*types.UpdateContainerResponse, error) { e := \u0026amp;supervisor.UpdateTask{} e.WithContext(ctx) e.ID = r.Id e.State = runtime.State(r.Status) if r.Resources != nil { rs := r.Resources e.Resources = \u0026amp;runtime.Resource{} if rs.CpuShares != 0 { e.Resources.CPUShares = int64(rs.CpuShares) } } s.sv.SendTask(e) }  // supervisor/update.go func (s *Supervisor) updateContainer(t *UpdateTask) error { i, ok := s.containers[t.ID] if !ok { return ErrContainerNotFound } container := i.container if t.State != \u0026quot;\u0026quot; { switch t.State { case runtime.Running: if err := container.Resume(); err != nil { return err } s.notifySubscribers(Event{ ID: t.ID, Type: StateResume, Timestamp: time.Now(), }) case runtime.Paused: if err := container.Pause(); err != nil { return err } s.notifySubscribers(Event{ ID: t.ID, Type: StatePause, Timestamp: time.Now(), }) default: return ErrUnknownContainerStatus } return nil } if t.Resources != nil { return container.UpdateResources(t.Resources) } return nil }  // runtime/container_linux.go func (c *container) UpdateResources(r *Resource) error { sr := ocs.Resources{ Memory: \u0026amp;ocs.Memory{ Limit: u64Ptr(uint64(r.Memory)), Reservation: u64Ptr(uint64(r.MemoryReservation)), Swap: u64Ptr(uint64(r.MemorySwap)), Kernel: u64Ptr(uint64(r.KernelMemory)), KernelTCP: u64Ptr(uint64(r.KernelTCPMemory)), }, CPU: \u0026amp;ocs.CPU{ Shares: u64Ptr(uint64(r.CPUShares)), Quota: u64Ptr(uint64(r.CPUQuota)), Period: u64Ptr(uint64(r.CPUPeriod)), Cpus: \u0026amp;r.CpusetCpus, Mems: \u0026amp;r.CpusetMems, RealtimePeriod: u64Ptr(uint64(r.CPURealtimePeriod)), RealtimeRuntime: u64Ptr(uint64(r.CPURealtimeRuntime)), }, BlockIO: \u0026amp;ocs.BlockIO{ Weight: \u0026amp;r.BlkioWeight, }, } srStr := bytes.NewBuffer(nil) if err := json.NewEncoder(srStr).Encode(\u0026amp;sr); err != nil { return err } args := c.runtimeArgs args = append(args, \u0026quot;update\u0026quot;, \u0026quot;-r\u0026quot;, \u0026quot;-\u0026quot;, c.id) cmd := exec.Command(c.runtime, args...) cmd.Stdin = srStr b, err := cmd.CombinedOutput() if err != nil { return fmt.Errorf(string(b)) } return nil }  通过 exec 调用 runc 的命令行。\nrunc // update.go var updateCommand = cli.Command{ Name: \u0026quot;update\u0026quot;, Usage: \u0026quot;update container resource constraints\u0026quot;, ArgsUsage: `\u0026lt;container-id\u0026gt;`, Action: func(context *cli.Context) error { container, err := getContainer(context) if err != nil { return err } r := specs.Resources{ Memory: \u0026amp;specs.Memory{ Limit: u64Ptr(0), Reservation: u64Ptr(0), Swap: u64Ptr(0), Kernel: u64Ptr(0), KernelTCP: u64Ptr(0), }, } } } config.Cgroups.Resources.BlkioWeight = *r.BlockIO.Weight config.Cgroups.Resources.CpuPeriod = int64(*r.CPU.Period) }  自此 runc 接收到配置的更新并做相应的处理。后续的流程就不分析了。\n","date":1560556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560556800,"objectID":"d20b0e1194cbf3fb27a5deaa9ad39d24","permalink":"/post/cloud/container/201906-docker-update/","publishdate":"2019-06-15T00:00:00Z","relpermalink":"/post/cloud/container/201906-docker-update/","section":"post","summary":"以 docker update 流程分析各组件之间的调用关系","tags":["Docker"],"title":"Docker update 流程分析","type":"post"},{"authors":null,"categories":null,"content":" 功能简介 kube-proxy 运行在 kubernetes 集群中每个 worker 节点上，负责实现 service 这个概念提供的功能。kube-proxy 会把访问 service VIP 的请求转发到运行的 pods 上，实现负载均衡。\n当用户创建 service 的时候，endpointController 会根据 service 的 selector 找到对应的 pod，然后生成 endpoints 对象保存到 etcd 中。kube-proxy 的主要工作就是监听 etcd（通过 apiserver 的接口，而不是直接读取 etcd），来实时更新节点上的 iptables。\nservice 有关的信息保存在 etcd 的 /registry/services 目录，比如在我的集群中，这个目录的内容是这样的：\n~]$ etcdctl ls --recursive /registry/services /registry/services/endpoints /registry/services/endpoints/default /registry/services/endpoints/default/whoami /registry/services/endpoints/default/kubernetes /registry/services/endpoints/kube-system /registry/services/endpoints/kube-system/kube-controller-manager /registry/services/endpoints/kube-system/container-log /registry/services/endpoints/kube-system/container-terminal /registry/services/endpoints/kube-system/kube-scheduler /registry/services/endpoints/kube-system/kube-dns /registry/services/specs /registry/services/specs/default /registry/services/specs/default/kubernetes /registry/services/specs/default/whoami /registry/services/specs/kube-system /registry/services/specs/kube-system/kube-dns /registry/services/specs/kube-system/container-log /registry/services/specs/kube-system/container-terminal  架构 kube proxy是部署在 node 上的为应用容器提供代理转发的功能，\n在Kubernetes中，kube proxy负责为Pod创建代理服务；引到访问至服务；并实现服务到Pod的路由和转发，以及通过应用的负载均衡。\nservice 和 kube-proxy 原理 在 kubernetes 集群中，网络是非常基础也非常重要的一部分。对于大规模的节点和容器来说，要保证网络的连通性、网络转发的高效，同时能做的 ip 和 port 自动化分配和管理，并让用户用直观简单的方式来访问需要的应用，这是需要复杂且细致设计的。\nkubernetes 在这方面下了很大的功夫，它通过 service、dns、ingress 等概念，解决了服务发现、负载均衡的问题，也大大简化了用户的使用和配置。\n跨主机网络配置：flannel 一直以来，kubernetes 并没有专门的网络模块负责网络配置，它需要用户在主机上已经配置好网络。kubernetes 对网络的要求是：容器之间（包括同一台主机上的容器，和不同主机的容器）可以互相通信，容器和集群中所有的节点也能直接通信。\n至于具体的网络方案，用户可以自己选择，目前使用比较多的是 flannel，因为它比较简单，而且刚好满足 kubernetes 对网络的要求。我们会使用 flannel vxlan 模式，具体的配置我在博客之前有文章介绍过，这里不再赘述。\n以后 kubernetes 网络的发展方向是希望通过插件的方式来集成不同的网络方案， CNI 就是这一努力的结果，flannel 也能够通过 CNI 插件的形式使用。\nkube-proxy 和 service 配置好网络之后，集群是什么情况呢？我们可以创建 pod，也能通过 ReplicationController 来创建特定副本的 pod（这是更推荐也是生产上要使用的方法，即使某个 rc 中只有一个 pod 实例）。可以从集群中获取每个 pod ip 地址，然后也能在集群内部直接通过 podIP:Port 来获取对应的服务。\n但是还有一个问题：pod 是经常变化的，每次更新 ip 地址都可能会发生变化，如果直接访问容器 ip 的话，会有很大的问题。而且进行扩展的时候，rc 中会有新的 pod 创建出来，出现新的 ip 地址，我们需要一种更灵活的方式来访问 pod 的服务。\nService 和 cluster IP 针对这个问题，kubernetes 的解决方案是“服务”（service），每个服务都一个固定的虚拟 ip（这个 ip 也被称为 cluster IP），自动并且动态地绑定后面的 pod，所有的网络请求直接访问服务 ip，服务会自动向后端做转发。Service 除了提供稳定的对外访问方式之外，还能起到负载均衡（Load Balance）的功能，自动把请求流量分布到后端所有的服务上，服务可以做到对客户透明地进行水平扩展（scale）。\n而实现 service 这一功能的关键，就是 kube-proxy。kube-proxy 运行在每个节点上，监听 API Server 中服务对象的变化，通过管理 iptables 来实现网络的转发。\n实例启动和测试 我们可以在终端上启动 kube-proxy，也可以使用诸如 systemd 这样的工具来管理它，比如下面就是一个简单的 kube-proxy.service 配置文件\n[root@localhost]# cat /usr/lib/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Proxy Service Documentation=http://kubernetes.com After=network.target Wants=network.target [Service] Type=simple EnvironmentFile=-/etc/sysconfig/kube-proxy ExecStart=/usr/bin/kube-proxy \\ --master=http://172.17.8.100:8080 \\ --v=4 \\ --proxy-mode=iptables TimeoutStartSec=0 Restart=on-abnormal [Install] WantedBy=multi-user.target  为了方便测试，我们创建一个 rc，里面有三个 pod。这个 pod 运行的是 cizixs/whoami 容器，它是一个简单的 HTTP 服务器，监听在 3000 端口，访问它会返回容器的 hostname。\n[root@localhost ~]# cat whoami-rc.yml apiVersion: v1 kind: ReplicationController metadata: name: whoami spec: replicas: 3 selector: app: whoami template: metadata: name: whoami labels: app: whoami env: dev spec: containers: - name: whoami image: cizixs/whoami:v0.5 ports: - containerPort: 3000 env: - name: MESSAGE value: viola  我们为每个 pod 设置了两个 label：app=whoami 和 env=dev，这两个标签很重要，也是后面服务进行绑定 pod 的关键。\n为了使用 service，我们还要定义另外一个文件，并通过 kubectl create -f ./whoami-svc.yml 来创建出来对象：\napiVersion: v1 kind: Service metadata: labels: name: whoami name: whoami spec: ports: - port: 3000 targetPort: 3000 protocol: TCP selector: app: whoami env: dev  其中 selector 告诉 kubernetes 这个 service 和后端哪些 pod 绑定在一起，这里包含的键值对会对所有 pod 的 labels 进行匹配，只要完全匹配，service 就会把 pod 作为后端。也就是说，service 和 rc 并不是对应的关系，一个 service 可能会使用多个 rc 管理的 pod 作为后端应用。\nports 字段指定服务的端口信息：\n port：虚拟 ip 要绑定的 port，每个 service 会创建出来一个虚拟 ip，通过访问 vip:port 就能获取服务的内容。这个 port 可以用户随机选取，因为每个服务都有自己的 vip，也不用担心冲突的情况 targetPort：pod 中暴露出来的 port，这是运行的容器中具体暴露出来的端口，一定不能写错 protocol：提供服务的协议类型，可以是 TCP 或者 UDP  创建之后可以列出 service ，发现我们创建的 service 已经分配了一个虚拟 ip (10.10.10.28)，这个虚拟 ip 地址是不会变化的（除非 service 被删除）。查看 service 的详情可以看到它的 endpoints 列出，对应了具体提供服务的 pod 地址和端口。\n[root@localhost ~]# kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.10.10.1 \u0026lt;none\u0026gt; 443/TCP 19d whoami 10.10.10.28 \u0026lt;none\u0026gt; 3000/TCP 1d [root@localhost ~]# kubectl describe svc whoami Name: whoami Namespace: default Labels: name=whoami Selector: app=whoami Type: ClusterIP IP: 10.10.10.28 Port: \u0026lt;unset\u0026gt; 3000/TCP Endpoints: 10.11.32.6:3000,10.13.192.4:3000,10.16.192.3:3000 Session Affinity: None No events.  默认的 service 类型是 ClusterIP，这个也可以从上面输出看出来。在这种情况下，只能从集群内部访问这个 IP，不能直接从集群外部访问服务。如果想对外提供服务，我们后面会讲解决方案。\n测试一下，访问 service 服务的时候可以看到它会随机地访问后端的 pod，给出不同的返回：\n[root@localhost ~]# curl http://10.10.10.28:3000 viola from whoami-8fpqp [root@localhost ~]# curl http://10.10.10.28:3000 viola from whoami-c0x6h [root@localhost ~]# curl http://10.10.10.28:3000 viola from whoami-8fpqp [root@localhost ~]# curl http://10.10.10.28:3000 viola from whoami-dc9ds  默认情况下，服务会随机转发到可用的后端。如果希望保持会话（同一个 client 永远都转发到相同的 pod），可以把 service.spec.sessionAffinity 设置为 ClientIP。\nNOTE: 需要注意的是，服务分配的 cluster IP 是一个虚拟 ip，如果你尝试 ping 这个 IP 会发现它没有任何响应，这也是刚接触 kubernetes service 的人经常会犯的错误。实际上，这个虚拟 IP 只有和它的 port 一起的时候才有作用，直接访问它，或者想访问该 IP 的其他端口都是徒劳。\n外部能够访问的服务 上面创建的服务只能在集群内部访问，这在生产环境中还不能直接使用。如果希望有一个能直接对外使用的服务，可以使用 NodePort 或者 LoadBalancer 类型的 Service。我们先说说 NodePort ，它的意思是在所有 worker 节点上暴露一个端口，这样外部可以直接通过访问 nodeIP:Port 来访问应用。\n我们先把刚才创建的服务删除：\n[root@localhost ~]# kubectl delete rc whoami replicationcontroller \u0026quot;whoami\u0026quot; deleted [root@localhost ~]# kubectl delete svc whoami service \u0026quot;whoami\u0026quot; deleted [root@localhost ~]# kubectl get pods,svc,rc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.10.10.1 \u0026lt;none\u0026gt; 443/TCP 14h  对我们原来的 Service 配置文件进行修改，把 spec.type 写成 NodePort 类型：\n[root@localhost ~]# cat whoami-svc.yml apiVersion: v1 kind: Service metadata: labels: name: whoami name: whoami spec: ports: - port: 3000 protocol: TCP # nodePort: 31000 selector: app: whoami type: NodePort  因为我们的应用比较简单，只有一个端口。如果 pod 有多个端口，也可以在 spec.ports中继续添加，只有保证多个 port 之间不冲突就行。\n重新创建 rc 和 svc：\n[root@localhost ~]# kubectl create -f ./whoami-svc.yml service \u0026quot;whoami\u0026quot; created [root@localhost ~]# kubectl get rc,pods,svc NAME DESIRED CURRENT READY AGE rc/whoami 3 3 3 10s NAME READY STATUS RESTARTS AGE po/whoami-8zc3d 1/1 Running 0 10s po/whoami-mc2fg 1/1 Running 0 10s po/whoami-z6skj 1/1 Running 0 10s NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes 10.10.10.1 \u0026lt;none\u0026gt; 443/TCP 14h svc/whoami 10.10.10.163 \u0026lt;nodes\u0026gt; 3000:31647/TCP 7s  需要注意的是，因为我们没有指定 nodePort 的值，kubernetes 会自动给我们分配一个，比如这里的 31647（默认的取值范围是 30000-32767）。当然我们也可以删除配置中 # nodePort: 31000 的注释，这样会使用 31000 端口。\nnodePort 类型的服务会在所有的 worker 节点（运行了 kube-proxy）上统一暴露出端口对外提供服务，也就是说外部可以任意选择一个节点进行访问。比如我本地集群有三个节点：172.17.8.100、172.17.8.101 和 172.17.8.102：\n[root@localhost ~]# curl http://172.17.8.100:31647 viola from whoami-mc2fg [root@localhost ~]# curl http://172.17.8.101:31647 viola from whoami-8zc3d [root@localhost ~]# curl http://172.17.8.102:31647 viola from whoami-z6skj  有了 nodePort，用户可以通过外部的 Load Balance 或者路由器把流量转发到任意的节点，对外提供服务的同时，也可以做到负载均衡的效果。\nnodePort 类型的服务并不影响原来虚拟 IP 的访问方式，内部节点依然可以通过 vip:port 的方式进行访问。\nLoadBalancer 类型的服务需要公有云支持，如果你的集群部署在公有云（GCE、AWS等）可以考虑这种方式。\nservice 原理解析 (iptables) 目前 kube-proxy 默认使用 iptables 模式，上述展现的 service 功能都是通过修改 iptables 实现的。\n我们来看一下从主机上访问 service:port 的时候发生了什么（通过 iptables-save 命令打印出来当前机器上的 iptables 规则）。\n所有发送出去的报文会进入 KUBE-SERVICES 进行处理\n*nat -A PREROUTING -m comment --comment \u0026quot;kubernetes service portals\u0026quot; -j KUBE-SERVICES -A OUTPUT -m comment --comment \u0026quot;kubernetes service portals\u0026quot; -j KUBE-SERVICES -A POSTROUTING -m comment --comment \u0026quot;kubernetes postrouting rules\u0026quot; -j KUBE-POSTROUTING -A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000  KUBE-SERVICES 每条规则对应了一个 service，它告诉继续进入到某个具体的 service chain 进行处理，比如这里的 KUBE-SVC-OQCLJJ5GLLNFY3XB\n-A KUBE-SERVICES -d 10.10.10.28/32 -p tcp -m comment --comment \u0026quot;default/whoami: cluster IP\u0026quot; -m tcp --dport 3000 -j KUBE-SVC-OQCLJJ5GLLNFY3XB  更具体的 chain 中定义了怎么转发到对应 endpoint 的规则，比如我们的 rc 有三个 pods，这里也就会生成三个规则。这里利用了 iptables 随机和概率转发的功能\n-A KUBE-SVC-OQCLJJ5GLLNFY3XB -m comment --comment \u0026quot;default/whoami:\u0026quot; -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-VN72UHNM6XOXLRPW -A KUBE-SVC-OQCLJJ5GLLNFY3XB -m comment --comment \u0026quot;default/whoami:\u0026quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-YXCSPWPTUFI5WI5Y -A KUBE-SVC-OQCLJJ5GLLNFY3XB -m comment --comment \u0026quot;default/whoami:\u0026quot; -j KUBE-SEP-FN74S3YUBFMWHBLF  我们来看第一个 chain，这个 chain 有两个规则，第一个表示给报文打上 mark；第二个是进行 DNAT（修改报文的目的地址），转发到某个 pod 地址和端口。\n-A KUBE-SEP-VN72UHNM6XOXLRPW -s 10.11.32.6/32 -m comment --comment \u0026quot;default/whoami:\u0026quot; -j KUBE-MARK-MASQ -A KUBE-SEP-VN72UHNM6XOXLRPW -p tcp -m comment --comment \u0026quot;default/whoami:\u0026quot; -m tcp -j DNAT --to-destination 10.11.32.6:3000  会匹配 chain 的第二条规则，因为地址是发送出去的，报文会根据路由规则进行处理，后续的报文就是通过 flannel 的网络路径发送出去的。\nnodePort 类型的 service 原理也是类似的，在 KUBE-SERVICES chain 的最后，如果目标地址不是 VIP 则会通过 KUBE-NODEPORTS ：\nChain KUBE-SERVICES (2 references) pkts bytes target prot opt in out source destination 0 0 KUBE-NODEPORTS all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL  而 KUBE-NODEPORTS chain 和 KUBE-SERVICES chain 其他规则一样，都是转发到更具体的 service chain，然后转发到某个 pod 上面。\n-A KUBE-NODEPORTS -p tcp -m comment --comment \u0026quot;default/whoami:\u0026quot; -m tcp --dport 31647 -j KUBE-MARK-MASQ -A KUBE-NODEPORTS -p tcp -m comment --comment \u0026quot;default/whoami:\u0026quot; -m tcp --dport 31647 -j KUBE-SVC-OQCLJJ5GLLNFY3XB  注意：在流量转发出去的时候，就已经选择好了目的 Pod:TargetPod，而源IP是 node 的ip。\n不足之处 看起来 service 是个完美的方案，可以解决服务访问的所有问题，但是 service 这个方案（iptables 模式）也有自己的缺点。\n首先，如果转发的 pod 不能正常提供服务，它不会自动尝试另一个 pod，当然这个可以通过 readiness probes 来解决。每个 pod 都有一个健康检查的机制，当有 pod 健康状况有问题时，kube-proxy 会删除对应的转发规则。\n另外，nodePort 类型的服务也无法添加 TLS 或者更复杂的报文路由机制。\nproxy 的 iptables mode因为使用iptable NAT来完成转发，也存在不可忽视的性能损耗。另外，如果集群中存在上万的Service/Endpoint，那么Node上的iptables rules将会非常庞大，性能还会再打折扣。\n这也导致，目前大部分企业用k8s上生产时，都不会直接用kube-proxy作为服务代理，而是通过自己开发或者通过Ingress Controller来集成HAProxy, Nginx来代替kube-proxy。\n代码阅读 重要结构体说明 本文以iptables 代理模式为例,对proxy 的功能实现进行分析。基于iptables的kube-proxy的主要职责包括两大块：一块是侦听service更新事件，并更新service相关的iptables规则，一块是侦听endpoint更新事件，更新endpoint相关的iptables规则。也就是说kube-proxy只是作为controller 负责更新更新规则，实现转发服务的是内核的netfilter，体现在用户态则是iptables。\nProxyServer type ProxyServer struct { // k8s client Client clientset.Interface EventClient v1core.EventsGetter // 接口中定义了更新iptables 的方法集合，如DeleteChian,DeleteRule, EnsureChain,EnsureRule IptInterface utiliptables.Interface IpvsInterface utilipvs.Interface IpsetInterface utilipset.Interface // 定义包装os/exec库中Command, Commandcontext, LookPath方法的接口 execer exec.Interface // 处理同步时的处理器，有三种模式 Proxier proxy.ProxyProvider //接受Event，交于各个处理函数进行处理 Broadcaster record.EventBroadcaster // 代理模式，ipvs iptables userspace kernelspace(windows)四种 ProxyMode string // 配置同步周期 ConfigSyncPeriod time.Duration // service 与 endpoint 事件处理器 ServiceEventHandler config.ServiceHandler EndpointsEventHandler config.EndpointsHandler }  Proxier 在每一种代理模式下，都定义了自己的Proxier 结构体，该结构体及方法实现了该模式下的代理规则的更新方法。在Iptables 模式下，Proxier 结构体定义如下：\ntype Proxier struct { //EndpointChangeTracker中items属性为一个两级map,用来保存所有namespace 下endpoints 的变化信息。 //第一级map以namespece 为key，value 值为该namespace下所有endpoints 更新前（previous)、后(current)的信息。 //前、后信息分别为一个map ,即第二级map: ServiceMap。 //第二级map的key为ServicePortName 结构，标记endpoints 对应的service，value为endpoint信息。 // EndpointChangeTracker 中实现了更新endpoint 的方法 endpointsChanges *proxy.EndpointChangeTracker // 同理，ServiceChangeTracker 中使用一个两级map保存所有namespace 下的service的变化信息，并定义了更新service的方法 serviceChanges *proxy.ServiceChangeTracker mu sync.Mutex // protects the following fields serviceMap proxy.ServiceMap // 同serviceChanges 的第二及map 结构，记录了所有namespace下需要更新iptables规则的service endpointsMap proxy.EndpointsMap //同endpointsChanges 的第二及map 结构，记录了所有namespace 需要更新iptables规则的endpoints portsMap map[utilproxy.LocalPort]utilproxy.Closeable endpointsSynced bool // Proxier 初始化时为False servicesSynced bool // Proxier 初始化时为False initialized int32 syncRunner *async.BoundedFrequencyRunner //async.BoundedFrequencyRunner 具有QPS功能，控制被托管方法的发生速率 // These are effectively const and do not need the mutex to be held. iptables utiliptables.Interface //iptables的执行器，定义了Iptables 的操作方法 masqueradeAll bool masqueradeMark string exec utilexec.Interface // 抽象了 os/exec 中的方法 clusterCIDR string hostname string nodeIP net.IP portMapper utilproxy.PortOpener //以打开的UDP或TCP端口 recorder record.EventRecorder healthChecker healthcheck.Server healthzServer healthcheck.HealthzUpdater precomputedProbabilities []string iptablesData *bytes.Buffer existingFilterChainsData *bytes.Buffer filterChains *bytes.Buffer filterRules *bytes.Buffer natChains *bytes.Buffer natRules *bytes.Buffer endpointChainsNumber int // Values are as a parameter to select the interfaces where nodeport works. nodePortAddresses []string // networkInterfacer defines an interface for several net library functions. // Inject for test purpose. networkInterfacer utilproxy.NetworkInterfacer }  Proxier 自定义的链 在iptables 原有的5个链上，k8s 又增加了以下自定义链，在自定义链上添加规则，以控制iptables 对k8s 数据包的转发。\nconst ( iptablesMinVersion = utiliptables.MinCheckVersion // 支持-C/--flag 参数的iptable 最低版本 //对于Service type=ClusterIP的每个端口都会在KUBE-SERVICES中有一条对应的规则 kubeServicesChain utiliptables.Chain = \u0026quot;KUBE-SERVICES\u0026quot; // kubeExternalServicesChain utiliptables.Chain=\u0026quot;KUBE-EXTERNAL-SERVICES\u0026quot; //对于Service type=NodePort的每个端口都会在KUBE-NODEPORTS中有一条对应的规则 kubeNodePortsChain utiliptables.Chain = \u0026quot;KUBE-NODEPORTS\u0026quot; //在KUBE-POSTROUTING链上，对(0x400)包做SNAT kubePostroutingChain utiliptables.Chain = \u0026quot;KUBE-POSTROUTING\u0026quot; //打标签链，对于进入此链的报文打标签(0x400)，预示被标签包要做NAT KubeMarkMasqChain utiliptables.Chain = \u0026quot;KUBE-MARK-MASQ\u0026quot; //打标签链，对于进入此链的报文打标签(0x800)，预示此包将要被放弃 KubeMarkDropChain utiliptables.Chain = \u0026quot;KUBE-MARK-DROP\u0026quot; //跳转 kubeForwardChain utiliptables.Chain = \u0026quot;KUBE-FORWARD\u0026quot; )  Proxy Server 启动 穿过cobra.Command 包装的一个启动命令，走到跟kube-proxy 服务相关的一个代码入口 Run()。在Run()中，主要就是两件事：\n 生成一个ProxyServer 实例；\n 运行ProxyServer 实例的Run 方法，运行服务。\n  kubernetes/cmd/kube-proxy/app/server.go\nfunc (o *Options) Run() error { if len(o.WriteConfigTo) \u0026gt; 0 { return o.writeConfigFile() } proxyServer, err := NewProxyServer(o) //初始化结构体ProxyServer if err != nil { return err } return proxyServer.Run() // 运行ProxyServer }  ProxyServer 初始化 进入NewProxyServer(o) 方法，开始ProxyServer 的初始化过程初始化过程中，重要的一个环节就是根据不同的代理模式生成不通的Proxier。初始化过程中，主要变量的初始化及作用已在代码中说明。\ncmd/kube-proxy/app/server_others.go func newProxyServer( config *proxyconfigapi.KubeProxyConfiguration, cleanupAndExit bool, cleanupIPVS bool, scheme *runtime.Scheme, master string) (*ProxyServer, error) { ... protocol := utiliptables.ProtocolIpv4 // 获取机器使用的IP协议版本，默认使用IPV4 ... // Create a iptables utils. execer := exec.New() // 包装了os/exec中的command,LookPath,CommandContext 方法，组装一个系统调用的命令和参数 dbus = utildbus.New() //iptInterface 赋值为runner结构体，该结构体实现了接口utiliptables.Interface中定义的方法， //各方法中通过runContext()方法调用execer的命令包装方法返回一个被包装的iptables 命令 iptInterface = utiliptables.New(execer, dbus, protocol) ... //EventBroadcaster会将收到的Event交于各个处理函数进行处理。接收Event的缓冲队列长为1000，不停地取走Event并广播给各个watcher; //watcher通过recordEvent()方法将Event写入对应的EventSink里，最大重试次数为12次，重试间隔随机生成(见staging/src/k8s.io/client-go/tools/record/event.go); // EnventSink 将在ProxyServer.Run() 中调用s.Broadcaster.StartRecordingToSink（） 传进来; // NewBroadcaster() 最后会启动一个goroutine 运行Loop 方法（staging/src/k8s.io/apimachinery/pkg/watch/mux.go), eventBroadcaster := record.NewBroadcaster() //EventRecorder通过generateEvent()实际生成各种Event，并将其添加到监视队列。 recorder := eventBroadcaster.NewRecorder(scheme, v1.EventSource{Component: \u0026quot;kube-proxy\u0026quot;, Host: hostname}) ... if len(config.HealthzBindAddress) \u0026gt; 0 {//服务健康检查的 IP 地址和端口（IPv4默认为0.0.0.0:10256，对于所有 IPv6 接口设置为 ::） healthzServer = healthcheck.NewDefaultHealthzServer(config.HealthzBindAddress, 2*config.IPTables.SyncPeriod.Duration, recorder, nodeRef) healthzUpdater = healthzServer } ... proxyMode := getProxyMode(string(config.Mode), iptInterface, kernelHandler, ipsetInterface, iptables.LinuxKernelCompatTester{}) ... if proxyMode == proxyModeIPTables { klog.V(0).Info(\u0026quot;Using iptables Proxier.\u0026quot;) if config.IPTables.MasqueradeBit == nil { // MasqueradeBit must be specified or defaulted. return nil, fmt.Errorf(\u0026quot;unable to read IPTables MasqueradeBit from config\u0026quot;) } // 返回一个Proxier 结构体实例 proxierIPTables, err := iptables.NewProxier(...) //参数略 if err != nil { return nil, fmt.Errorf(\u0026quot;unable to create proxier: %v\u0026quot;, err) } metrics.RegisterMetrics() proxier = proxierIPTables // Iptables Proxier 实现了 ServiceHandler 和 EndpointsHandler 的接口。 serviceEventHandler = proxierIPTables endpointsEventHandler = proxierIPTables userspace.CleanupLeftovers(iptInterface)// 无条件强制清除之前userspace 模式的规则 // 因为无法区分iptables 规则是否由IPVS 代理生成，因此由用户根据实际情况决定是否调用ipvs.CleanupLeftovers() if canUseIPVS { ipvs.CleanupLeftovers(ipvsInterface, iptInterface, ipsetInterface, cleanupIPVS) } } else if proxyMode == proxyModeIPVS {// 初始化IPVS Proxier } else { // 初始化 userspace Proxier } iptInterface.AddReloadFunc(proxier.Sync) return \u0026amp;ProxyServer{ // 赋值过程略 ... }, nil }  Proxier 初始化 // kubernetes/pkg/proxy/iptables/proxier.go func NewProxier(...) (*Proxier, error) { //参数略 ... //kube-proxy要求NODE节点操作系统中有/sys/module/br_netfilter模块，还要设置bridge-nf-call-iptables=1； //如果不满足要求，kube-proxy在运行过程中设置的某些iptables规则就不会工作。 if val, err := sysctl.GetSysctl(sysctlBridgeCallIPTables); err == nil \u0026amp;\u0026amp; val != 1 { klog.Warning(\u0026quot;missing br-netfilter module or unset sysctl br-nf-call-iptables; proxy may not work as intended\u0026quot;) } // Generate the masquerade mark to use for SNAT rules. masqueradeValue := 1 \u0026lt;\u0026lt; uint(masqueradeBit) //masqueradeBit: Default 14 // 输出一个8位16进制数值 ，默认即0x00004000/0x00004000,用来标记k8s管理的报文。 //标记 0x4000的报文（即POD发出的报文)，在离开Node（物理机）的时候需要进行SNAT转换。 masqueradeMark := fmt.Sprintf(\u0026quot;%#08x/%#08x\u0026quot;, masqueradeValue, masqueradeValue) healthChecker := healthcheck.NewServer(hostname, recorder, nil, nil) // use default implementations of deps isIPv6 := ipt.IsIpv6() proxier := \u0026amp;Proxier{ portsMap: make(map[utilproxy.LocalPort]utilproxy.Closeable), serviceMap: make(proxy.ServiceMap), ... networkInterfacer: utilproxy.RealNetwork{}, } burstSyncs := 2 ... //Default: syncPeriod=30s (--iptables-sync-period duration)，将proxier.syncProxyRules 托管至BoundedFrequencyRunner 结构体， //BoundedFrequencyRunner 中含有一个Limiter ，该Limiter 采用\u0026quot;桶令牌\u0026quot; 限流算法控制proxier.syncProxyRules 方法运行的频率。 //minSyncPeriod=0 时，无速率限制。限流时，桶类初始令牌数量为burstSyncs。 proxier.syncRunner = async.NewBoundedFrequencyRunner(\u0026quot;sync-runner\u0026quot;, proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs) return proxier, nil }  注册ResourceHandler ProxyServer 及Proxier 这两个重要的结构体初始化完成以后，就进入了proxyServer.Run() 方法。在Run() 方法中，大致做了如下工作：\n 准备工作，如设置OOMScoreAdj, 注册service 和endpoints 的处理方法\n 使用list-watch 机制对service，endpoints资源监听。\n 最后进入一个无限循环，对service与endpoints的变化进行iptables规则的同步。\n  在Run方法中，主要关注一下对service 和endpoints资源变化的处理方法的注册过程。\n// cmd/kube-proxy/app/server.go func (s *ProxyServer) Run() error { ... //在用户空间通过写oomScoreAdj参数到/proc/self/oom_score_adj文件来改变进程的 oom_adj 内核参数； //oom_adj的值的大小决定了进程被 OOM killer，取值范围[-1000,1000] 选中杀掉的概率,值越低越不容易被杀死.此处默认值是-999。 if s.OOMScoreAdj != nil { oomAdjuster = oom.NewOOMAdjuster() if err := oomAdjuster.ApplyOOMScoreAdj(0, int(*s.OOMScoreAdj)); err != nil { klog.V(2).Info(err) } } if len(s.ResourceContainer) != 0 { ... // resourcecontainer.RunInResourceContainer(s.ResourceContainer); ... } if s.Broadcaster != nil \u0026amp;\u0026amp; s.EventClient != nil { // EventSinkImpl 包装了处理event 的方法create ,update, patchs //s.Broadcaster 已经在ProxyServer 初始化中作为一个goroutine 在运行。 s.Broadcaster.StartRecordingToSink(\u0026amp;v1core.EventSinkImpl{Interface: s.EventClient.Events(\u0026quot;\u0026quot;)}) } // Start up a healthz server if requested if s.HealthzServer != nil { s.HealthzServer.Run() } // Start up a metrics server if requested if len(s.MetricsBindAddress) \u0026gt; 0 { ... } // Tune conntrack, if requested // Conntracker is always nil for windows if s.Conntracker != nil { max, err := getConntrackMax(s.ConntrackConfiguration) ... } // Default: s.ConfigSyncPeriod =15m (--config-sync-period) //返回一个sharedInformerFactory结构体实例(staing/src/k8s.io/client-go/informers/factory.go) informerFactory := informers.NewSharedInformerFactory(s.Client, s.ConfigSyncPeriod) //ServiceConfig结构体跟踪记录Service配置信息的变化 //informerFactory.Core().V1().Services() 返回一个 serviceInformer 结构体引用(staing/src/k8s.io/client-go/informers/core/v1/service.go serviceConfig := config.NewServiceConfig(informerFactory.Core().V1().Services(), s.ConfigSyncPeriod) //RegisterEventHandler 是将Service的处理方法追加到serviceConfig的eventHandlers 中，eventHandlers为一个列表，元素类型ServiceHandler接口 // ServiceHandler接口定义了每个hanlder 处理service的api方法:OnServiceAdd,OnServiceUpdate,OnServiceDelete,OnServiceSynced // 此处s.ServiceEventHandler 为proxier，proxier实现了ServiceHandler接口定义的方法 //serviceConfig 中的handleAddService,handleUpdateService,handleDeleteService 将会调用每个eventHandler的OnServiceAdd等方法 serviceConfig.RegisterEventHandler(s.ServiceEventHandler) go serviceConfig.Run(wait.NeverStop) //初始化同步service,调用了一次proxier.syncProxyRules() endpointsConfig := config.NewEndpointsConfig(informerFactory.Core().V1().Endpoints(), s.ConfigSyncPeriod) endpointsConfig.RegisterEventHandler(s.EndpointsEventHandler) go endpointsConfig.Run(wait.NeverStop) // This has to start after the calls to NewServiceConfig and NewEndpointsConfig because those // functions must configure their shared informer event handlers first. go informerFactory.Start(wait.NeverStop) // Birth Cry after the birth is successful s.birthCry() // Just loop forever for now... s.Proxier.SyncLoop() return nil }  ServiceConfig 来跟踪 service 配置变化，通过 chan 接收到 set、add、remove的操作，然后调用相应的注册的 ServiceHandler。\ntype ServiceConfig struct { lister listers.ServiceLister listerSynced cache.InformerSynced eventHandlers []ServiceHandler }  上面以注释的方式描述了proxier中service处理方法的被调用流程：通过serviceConfig.RegisterEventHandler()方法实现了在serviceConfig中的handleAddService()等方法中调用proxier中的OnServiceAdd()等对应的方法。那么serviceConfig.handleAddService()等方法是在哪里以及何时被调用的呢？再次回看serviceConfig的实例化方法 NewServiceConfig() 挖掘handleAddService()的被调用处。\n// pkg/proxy/config/config.go func NewServiceConfig(serviceInformer coreinformers.ServiceInformer, resyncPeriod time.Duration) *ServiceConfig { result := \u0026amp;ServiceConfig{ lister: serviceInformer.Lister(), listerSynced: serviceInformer.Informer().HasSynced, } //结构体cache.ResourceEventHandlerFuncs 是一个ResourceEventHandler接口类型(staing/src/k8s.io/client-go/tools/cache/controller.go) //将ServicConfig 结构体的handleAddService 等方法赋予了cache.ResourceEventHandlerFuncs,实现一个ResourceEventHandler实例 //serviceInformer.Informer() 返回一个sharedIndexInformer 实例(staing/src/k8s.io/client-go/tools/cache/shared_informer.go) //通过AddEventHandlerWithResyncPeriod() 方法，将ResourceEventHandler实例赋值给processorListener结构体的handler属性 serviceInformer.Informer().AddEventHandlerWithResyncPeriod( cache.ResourceEventHandlerFuncs{ AddFunc: result.handleAddService, UpdateFunc: result.handleUpdateService, DeleteFunc: result.handleDeleteService, }, resyncPeriod, ) return result }  看完上面的注释，大概就明白了proxier 中的OnServiceAdd() 等法法的调用流程 在上边代码serviceInformer.Informer()返回之前，还将调用InformerFor()方法给informerFactory的informers属性赋值f.informers[informerType] = informer, 此行代码的意义可理解为：从api server 监听到 informerType类型资源变化的处理者记录(映射)为informer。此处的资源类型即为service, informer 便为sharedIndexInformer。\nfunc (c *ServiceConfig) handleAddService(obj interface{}) { service, ok := obj.(*v1.Service) ... for i := range c.eventHandlers { klog.V(4).Info(\u0026quot;Calling handler.OnServiceAdd\u0026quot;) c.eventHandlers[i].OnServiceAdd(service) } }  在调用 handleAddService() 中遍历调用 eventHandlers 的 OnServiceAdd() 方法\n具体的调用时机和最上层方法入口还要从informerFactory这个东西说起，这又是k8s 中另一个比较系统的公共组件的实现原理了，即client-go的SharedInformer。\npkg/proxy/apis/config/register.go const GroupName = \u0026quot;kubeproxy.config.k8s.io\u0026quot;   这个用途是什么？\n 记录资源变化 上面介绍了ResourceHandler 的注册及被调用过程。 Proxier 实现了 services 和 endpoints 事件各种最终的观察者，最终的事件触发都会在 proxier 中进行处理。对于通过监听 API Server 变化的信息，通过调用ResourceHandler将变化的信息保存到 endpointsChanges 和 serviceChanges。那么一个ResourceHandler是如何实现的呢？service 和endpoints 的变化如何记录为servriceChanges 和endpointsChanges？回看上边源码中被注册的对象s.ServiceEventHandler，s.EndpointsEventHandler的具体实现便可明白。\nservice 和endpoints 的处理原则相似，以对servcie 的处理为例，看一下对service 的处理方法。\nservice 和endpoints 的处理原则相似，以对servcie 的处理为例，看一下对service 的处理方法。\n//pkg/proxy/iptables/proxier.go func (proxier *Proxier) OnServiceAdd(service *v1.Service) { proxier.OnServiceUpdate(nil, service) } func (proxier *Proxier) OnServiceUpdate(oldService, service *v1.Service) { if proxier.serviceChanges.Update(oldService, service) \u0026amp;\u0026amp; proxier.isInitialized() { proxier.syncRunner.Run() // 通过channel 发送一个信号，调用tryRun() } } func (proxier *Proxier) OnServiceDelete(service *v1.Service) { proxier.OnServiceUpdate(service, nil) }  从上边代码中，可以看到，对service的处理方法大致分为三种：\n增加一个service 删除一个service 处理一个已存在的service的变化。\n其中，增加、删除service 都是给OnServiceUpdate() 传入参数后，由OnServiceUpdate() 方法处理。因此，重点看一下OnServiceUpdate()调用的update() 方法的实现。\nproxy 用 ServiceChangeTracker 来记录资源的变化情况，ServiceChangeTracker 是在 NewProxier() 函数中创建的，它用了一个双重的 map，外面一层 map 如下：\ntype ServiceChangeTracker struct { items map[types.NamespacedName]*serviceChange }  其中每个服务用 \u0026lt;namespace\u0026gt;/\u0026lt;name\u0026gt; 来标识，服务的变化存储在 serviceChange 中。\ntype serviceChange struct { previous ServiceMap current ServiceMap }  previous 是变化之前的状态，current 是变化之后的状态。\ntype ServiceMap map[ServicePortName]ServicePort // ServicePortName carries a namespace + name + portname. This is the unique // identifier for a load-balanced service. type ServicePortName struct { types.NamespacedName Port string } // ServicePort is an interface which abstracts information about a service. type ServicePort interface { // String returns service string. An example format can be: `IP:Port/Protocol`. String() string // ClusterIPString returns service cluster IP in string format. ClusterIPString() string // GetProtocol returns service protocol. GetProtocol() v1.Protocol // GetHealthCheckNodePort returns service health check node port if present. If return 0, it means not present. GetHealthCheckNodePort() int // GetNodePort returns a service Node port if present. If return 0, it means not present. GetNodePort() int }  通过 ServicePort 来表示一个服务。\n通过 ServiceChangeTracker.Update() 来实现服务的更新。\n//pkg/proxy/service.go func (sct *ServiceChangeTracker) Update(previous, current *v1.Service) bool { svc := current if svc == nil { svc = previous } // previous == nil \u0026amp;\u0026amp; current == nil is unexpected, we should return false directly. if svc == nil { return false } namespacedName := types.NamespacedName{Namespace: svc.Namespace, Name: svc.Name} sct.lock.Lock() defer sct.lock.Unlock() change, exists := sct.items[namespacedName] if !exists { // 在serviceChanges 中不存在一个以namespacedName 为key 的资源 change = \u0026amp;serviceChange{} // 初始化一个serviceChange change.previous = sct.serviceToServiceMap(previous) sct.items[namespacedName] = change } change.current = sct.serviceToServiceMap(current) // if change.previous equal to change.current, it means no change if reflect.DeepEqual(change.previous, change.current) { // 从update传递进来的资源没有变化，则从serviceChanges中删除。 delete(sct.items, namespacedName) } return len(sct.items) \u0026gt; 0 }  update 方法就是根据previous ,current 参数新生成一个change 或者修改一个存在的change。并且把无变化的资源从serviceChanges 中删除。serviceChanges.items 会在将变化信息更新到proxier.serviceMap 后清空。\n限流同步机制 在对proxy server 关心的资源变化进行了监听记录之后，最后从s.Proxier.SyncLoop()进入proxier.syncRunner.Loop()方法，由proxier.syncRunner 对托管syncProxyRules() ，syncProxyRules() 实现了修改iptables规则的具体流程。此处值得留意的是proxier.syncRunner采用“令牌桶”算法实现了限流的同步控制。\nfunc (proxier *Proxier) SyncLoop() { proxier.syncRunner.Loop(wait.NeverStop) }  //pkg/utils/async/bounded_frequency_runner.go func (bfr *BoundedFrequencyRunner) Loop(stop \u0026lt;-chan struct{}) { klog.V(3).Infof(\u0026quot;%s Loop running\u0026quot;, bfr.name) bfr.timer.Reset(bfr.maxInterval) for { select { case \u0026lt;-stop: bfr.stop() klog.V(3).Infof(\u0026quot;%s Loop stopping\u0026quot;, bfr.name) return //先确认是否到了运行时机，如果可以运行，就调用syncProxyRules()，之后重新计时。 //具体参考Timer 的实现机制 case \u0026lt;-bfr.timer.C(): bfr.tryRun() case \u0026lt;-bfr.run: //收到一个channel信号 bfr.tryRun() } } }  修改 Iptables 规则 介绍了资源监听、记录和同步机制，再来看一下kube-proxy是如何将资源的变化反馈到iptables规则中的。在iptables的代理模式中，syncProxyRule()方法实现了修改iptables规则的细节流程。走读分析该方法，能将明白在node节点观察到的新链及规则产生的方式及目的。\nsyncProxyRules()这一单个方法的代码较长（约700+ 行），具体的细节功能也多，本节将对syncProxyRules()里的代码执行流程分开介绍。\n 更新proxier.endpointsMap，proxier.servieMap。\nproxier.serviceMap：把sercvieChanges.current 写入proxier.serviceMap，再把存在于sercvieChanges.previous 但不存在于sercvieChanges.current 的service 从 proxier.serviceMap中删除，并且删除的时候，把使用UDP协议的cluster_ip 记录于UDPStaleClusterIP 。\nproxier.endpointsMap：把endpointsChanges.previous 从proxier.endpointsMap 删除，再把endpointsChanges.current 加入proxier.endpointsMap。把存在于endpointsChanges.previous 但不存在于endpointsChanges.current 的endpoint 组装为ServiceEndpoint 结构，把该结构记录于staleEndpoints。\n​具体相关代码流程如下：\n//kubernetes/pkg/proxy/iptables/proxier.go func (proxier *Proxier) syncProxyRules() { ... serviceUpdateResult := proxy.UpdateServiceMap(proxier.serviceMap, proxier.serviceChanges) endpointUpdateResult := proxy.UpdateEndpointsMap(proxier.endpointsMap, proxier.endpointsChanges) staleServices := serviceUpdateResult.UDPStaleClusterIP // 利用endpointUpdateResult.StaleServiceNames，再次更新 staleServices for _, svcPortName := range endpointUpdateResult.StaleServiceNames { if svcInfo, ok := proxier.serviceMap[svcPortName]; ok \u0026amp;\u0026amp; svcInfo != nil \u0026amp;\u0026amp; svcInfo.GetProtocol() == v1.ProtocolUDP { klog.V(2).Infof(\u0026quot;Stale udp service %v -\u0026gt; %s\u0026quot;, svcPortName, svcInfo.ClusterIPString()) staleServices.Insert(svcInfo.ClusterIPString()) } } ... } //kubernetes/pkg/proxy/servcie.go func UpdateServiceMap(serviceMap ServiceMap, changes *ServiceChangeTracker) (result UpdateServiceMapResult) { result.UDPStaleClusterIP = sets.NewString() // apply 方法中，继续调用了merge，filter, umerge // merge:将change.current的servicemap 信息合入proxier.servicemap中。 // filter:将change.previous和change.current共同存在的servicemap从将change.previous删除 // unmerge: 将change.previous 中使用UDP 的servicemap 从 proxier.serviceMap 中删除，并记录删除的服务IP 到UDPStaleClusterIP //apply中最后重置了proxy.serviceChanges.items serviceMap.apply(changes, result.UDPStaleClusterIP) //HCServiceNodePorts 保存proxier.serviceMap 中所有服务的健康检查端口 result.HCServiceNodePorts = make(map[types.NamespacedName]uint16) for svcPortName, info := range serviceMap { if info.GetHealthCheckNodePort() != 0 { result.HCServiceNodePorts[svcPortName.NamespacedName] = uint16(info.GetHealthCheckNodePort()) } } return result } //kubernetes/pkg/proxy/endpoints.go func UpdateEndpointsMap(endpointsMap EndpointsMap, changes *EndpointChangeTracker) (result UpdateEndpointMapResult) { result.StaleEndpoints = make([]ServiceEndpoint, 0) result.StaleServiceNames = make([]ServicePortName, 0) //从proixer.endpointsMap 中删除和change.previous 相同的elelment. // 将change.current 添加至proixer.endpointsMap // StaleEndpoints 保存了存在于previous 但不存在current的endpoints // StaleServicenames保存了一种ServicePortName,这样的ServicePortName在change.previous不存在对应的endpoints，在change.current存在endpoints。 // 最后重置了了proxy.endpointsChanges.items endpointsMap.apply(changes, \u0026amp;result.StaleEndpoints, \u0026amp;result.StaleServiceNames) // computing this incrementally similarly to endpointsMap. result.HCEndpointsLocalIPSize = make(map[types.NamespacedName]int) localIPs := GetLocalEndpointIPs(endpointsMap) for nsn, ips := range localIPs { result.HCEndpointsLocalIPSize[nsn] = len(ips) } return result }   在准好了更新iptables需要的资源变量后，接下来就是调用iptables 命令建立了自定义链，并在对应的内核链上引用这些自定义链。这些自定义链在k8s 服务中是必须的，不会跟随资源变化而变化，所以在更新规则之前，提前无条件生成这些链，做好准备工作，随后会在这些自定义链上创建相应的规则。  相关代码如下：\nfor _, chain := range iptablesJumpChains { if _, err := proxier.iptables.EnsureChain(chain.table, chain.chain); err != nil { //创建链 klog.Errorf(\u0026quot;Failed to ensure that %s chain %s exists: %v\u0026quot;, chain.table, kubeServicesChain, err) return } args := append(chain.extraArgs, \u0026quot;-m\u0026quot;, \u0026quot;comment\u0026quot;, \u0026quot;--comment\u0026quot;, chain.comment, \u0026quot;-j\u0026quot;, string(chain.chain), ) if _, err := proxier.iptables.EnsureRule(utiliptables.Prepend, chain.table, chain.sourceChain, args...); err != nil { // 引用链 klog.Errorf(\u0026quot;Failed to ensure that %s chain %s jumps to %s: %v\u0026quot;, chain.table, chain.sourceChain, chain.chain, err) return } }  上边代码完成的iptables命令如下：\niptables -w -N KUBE-EXTERNAL-SERVICES -t filter iptables -w -I INPUT -t filter -m conntrack --ctstate NEW -m comment --comment -j KUBE-EXTERNAL-SERVICES kubernetes externally-visible service portals iptables -w -N KUBE-SERVICES -t filter iptables -w -I OUTPUT -t filter -m conntrack --ctstate NEW -m comment --comment -j KUBE-SERVICES kubernetes service portals iptables -w -N KUBE-SERVICES -t nat iptables -w -I OUTPUT -t nat -m conntrack --ctstate NEW -m comment --comment -j KUBE-SERVICES kubernetes service portals iptables -w -N KUBE-SERVICES -t nat iptables -w -I PREROUTING -t nat -m conntrack --ctstate NEW -m comment --comment -j KUBE-SERVICES kubernetes service portals iptables -w -N KUBE-POSTROUTING -t nat iptables -w -I POSTROUTING -t nat -m conntrack --ctstate NEW -m comment --comment -j KUBE-POSTROUTING kubernetes postrouting rules iptables -w -N KUBE-FORWARD -t filter iptables -w -I FORWARD -t filter -m conntrack --ctstate NEW -m comment --comment -j KUBE-FORWARD kubernetes forwarding rules  将当前内核中filter表和nat 表中的全部规则临时导出到数个buffer，具体的：\n 使用 proxier.existingFilterChainsData 保存filter表的信息 使用 existingFilterChains保存 proxier.existingFilterChainsData 的chain 信息 使用 proxier.iptablesData 保存nat 表的信息 使用 existingNATChains 保存 proxier.iptablesData 的chain 信息 重置 proxier.filterChains，proxier.filterRules，proxier.natChains，proxier.natRules 四个buffer , 这四个buffer 用来缓存最新的关于k8s 服务于endpoints 的 iptables 信息。  在上面准备工作做好之后，开始向上述四个buffer中根据条件不断追加内容，缓存内容在同步规则的最后环节刷入内核。\n  问题  kube proxy 读取到的数据结构是怎样的？ （输入）\n读取到的数据是监听 API server 的 Service 和 Endpoint 的数据。\n kube proxy 写出的数据机构是怎样的？ （输出）\n通过 Service 和 Endpoint 组装 iptables rules。\n pod 如何发现和路由到远端的 pod？ （egress）\n参考问题 1 和 2 的答案\n node 接收到流量后怎么路由到相应的 pod 上的？ （ingress）\n收到的报文的目的 IP 是 Pod 的 IP，Node 上有到 Pod 的路由直接转发给 Pod。\n kube-proxy 有多种实现方式：userspace、iptables、ipvs，如何分别实现的？\n可以在命令行中指定模式，用 interface ProxyProvider，每种模式都会有一个 Proxier 来实现这个 interface。\n Proxy 如何解决了同一主宿机相同服务端口冲突的问题？\nPod 的 IP 不一样，即使端口冲突也没关系。\n ServiceInformer 是如何工作的？\nlist-watch 机制如何实现的？\n ServicePortName 为什么是负载均衡的唯一标识？\n// ServicePortName carries a namespace + name + portname. This is the unique // identifier for a load-balanced service. type ServicePortName struct { types.NamespacedName Port string }\n 既然 kube-proxy 能够自动监听 apiserver 的变化，并更新 iptables，为什么这里还要再每隔一段时间强制同步一次呢？\n我的看法是这只是安全防护措施，来规避有些情况（比如代码 bug，或者网络、环境问题等原因）下数据可能没有及时同步。\n 更新 iptable rule 的时候用什么策略？如果直接更新规则会影响正在转发的流量吗？\n只会影响有变化的配置，已经存在的规则不会变，先写入更新后的规则，然后删除旧的规则。\n  总结 node节点上的iptables中有到达所有service的规则，service 的cluster IP并不是一个实际的IP，它的存在只是为了找出实际的endpoint地址，对达到cluster IP的报文都要进行DNAT为Pod IP(+port)，不同node上的报文实际上是通过POD IP传输的，cluster IP只是本node节点的一个概念，用于查找并DNAT，即目的地址为clutter IP的报文只是本node发送的，其他节点不会发送(也没有路由支持)，即默认下cluster ip仅支持本node节点的service访问，如果需要跨node节点访问，可以使用插件实现，如flannel，它将pod ip进行了封装\n最后用两张图总结一下 kube-proxy 更新iptables 的流程\n1) 资源更新信息来源\n2) 链建立及规则导向\n另外：对于数据包的出入口，有这么一句心得：只要你站在内核的角度理解，无论从虚拟网卡还是物理网卡收到一个包，对内核来说都是收包，都是prerouting链开始。无论一个包去往物理网卡还是虚拟网卡，对内核来说都是发出，都是从postrouting结束。本机进程收到就是input链，本机进程发出就是output链。\n参考 我是怎么阅读kubernetes源代码的？\nkube-proxy 源码解析\n理解kubernetes环境的iptables\nkube-proxy 源码分析\n","date":1559692800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559692800,"objectID":"1801322119fd2ed28aed808b26e7e68e","permalink":"/post/cloud/k8s/201901-how-read-kubeproxy/","publishdate":"2019-06-05T00:00:00Z","relpermalink":"/post/cloud/k8s/201901-how-read-kubeproxy/","section":"post","summary":"Kube-proxy 把访问 Service VIP 的请求转发到运行的 Pods 上，并实现负载均衡","tags":["Kubernetes"],"title":"Kube proxy 源码解读","type":"post"},{"authors":null,"categories":null,"content":" Kubernetes 本身不提供容器网络, 但是实现了一套支持多种网络插件的框架代码, 通过调用网络插件来为容器设置网络环境。\n而约束网络插件的是 CNI（Container Network Interface），一种标准的容器网络接口，定义了如何将容器加入网络和将容器从网络中删除。\nCNI 接口由 runtime 在创建容器和删除容器时调用。具体的接口定义如下：\n// vendor/github.com/containernetworking/cni/libcni/api.go type CNI interface { AddNetworkList(net *NetworkConfigList, rt *RuntimeConf) (types.Result, error) DelNetworkList(net *NetworkConfigList, rt *RuntimeConf) error AddNetwork(net *NetworkConfig, rt *RuntimeConf) (types.Result, error) DelNetwork(net *NetworkConfig, rt *RuntimeConf) error }  Kubernetes plugin 接口 kubelet 是通过 NetworkPlugin interface 来调用底层的网络插件为容器设置网络环境.\n// kubelet/dockershim/network/plugins.go // Plugin is an interface to network plugins for the kubelet type NetworkPlugin interface { // Init initializes the plugin. This will be called exactly once // before any other methods are called. Init(host Host, hairpinMode kubeletconfig.HairpinMode, nonMasqueradeCIDR string, mtu int) error // Called on various events like: // NET_PLUGIN_EVENT_POD_CIDR_CHANGE Event(name string, details map[string]interface{}) // Name returns the plugin's name. This will be used when searching // for a plugin by name, e.g. Name() string // Returns a set of NET_PLUGIN_CAPABILITY_* Capabilities() utilsets.Int // SetUpPod is the method called after the infra container of // the pod has been created but before the other containers of the // pod are launched. SetUpPod(namespace string, name string, podSandboxID kubecontainer.ContainerID, annotations, options map[string]string) error // TearDownPod is the method called before a pod's infra container will be deleted TearDownPod(namespace string, name string, podSandboxID kubecontainer.ContainerID) error // GetPodNetworkStatus is the method called to obtain the ipv4 or ipv6 addresses of the container GetPodNetworkStatus(namespace string, name string, podSandboxID kubecontainer.ContainerID) (*PodNetworkStatus, error) // Status returns error if the network plugin is in error state Status() error }  实现了 NetworkPlugin interface 就可以新增一种 Kubernetes 的 Network plugin。这个 interface 也并没有具体容器网络的实现，而是做了一层封装，具体的容器网络由独立的二进制实现，比如官方提供的 bridge、host-local 或者第三方的 calico、flannel 等，也可以是自己定制的实现。\nK8S 支持两种 plugin：\n cniNetworkPlugin kubenetNetworkPlugin  下面讲述 plugin 是如何初始化和工作的\nkubelet 启动 kubelet 启动后会调用 run() 进入处理流程，在进入主处理流程之前的初始化阶段会根据用户配置的网络插件名选择对应的网络插件。\n// cmd/kubelet/app/server.go func run(s *options.KubeletServer, kubeDeps *kubelet.KubeletDeps) (err error) { ... // 创建 kubelete // 根据 kubelet 的运行参数运行 kubelet // 这里会根据用户配置的网络插件名选择网络插件 if err := RunKubelet(\u0026amp;s.KubeletConfiguration, kubeDeps, s.RunOnce, standaloneMode); err != nil { return err } ... }  // cmd/kubelet/app/server.go func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error { ... k, err := CreateAndInitKubelet(\u0026amp;kubeServer.KubeletConfiguration, kubeDeps, ...) ... }  func CreateAndInitKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, ...) { k, err = kubelet.NewMainKubelet(kubeCfg, kubeDeps, crOptions ... ) k.BirthCry() k.StartGarbageCollection() }  // NewMainKubelet instantiates a new Kubelet object along with all the required internal modules. // No initialization of Kubelet and its modules should happen here. func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies ...) { klet := \u0026amp;Kubelet{ hostname: hostname, hostnameOverridden: len(hostnameOverride) \u0026gt; 0, ... } switch containerRuntime { case kubetypes.DockerContainerRuntime: // Create and start the CRI shim running as a grpc server. streamingConfig := getStreamingConfig(kubeCfg, kubeDeps, crOptions) ds, err := dockershim.NewDockerService(kubeDeps.DockerClientConfig, crOptions.PodSandboxImage, streamingConfig, \u0026amp;pluginSettings, runtimeCgroups, kubeCfg.CgroupDriver, crOptions.DockershimRootDirectory, !crOptions.RedirectContainerStreaming) ... server := dockerremote.NewDockerServer(remoteRuntimeEndpoint, ds) if err := server.Start(); err != nil { return nil, err } case kubetypes.RemoteContainerRuntime: // No-op. break default: return nil, fmt.Errorf(\u0026quot;unsupported CRI runtime: %q\u0026quot;, containerRuntime) } // 向 k8s network plugin 通报 NET_PLUGIN_EVENT_POD_CIDR_CHANGE 事件 // 该事件将会被 NetworkPlugin interface 的 Event 方法捕获 if _, err := klet.updatePodCIDR(kubeCfg.PodCIDR); err != nil { klog.Errorf(\u0026quot;Pod CIDR update failed %v\u0026quot;, err) } ... }  目前只支持 CRI 为 docker。\n根据用户配置选择 CNI // pkg/kubelet/dockershim/docker_service.go // NOTE: Anything passed to DockerService should be eventually handled in another way when we switch to running the shim as a different process. func NewDockerService(config *ClientConfig, podSandboxImage string, ...) (DockerService, error) { ds := \u0026amp;dockerService{ client: c, os: kubecontainer.RealOS{}, podSandboxImage: podSandboxImage, streamingRuntime: \u0026amp;streamingRuntime{ client: client, execHandler: \u0026amp;NativeExecHandler{}, }, containerManager: cm.NewContainerManager(cgroupsName, client), checkpointManager: checkpointManager, startLocalStreamingServer: startLocalStreamingServer, networkReady: make(map[string]bool), } // Determine the hairpin mode. if err := effectiveHairpinMode(pluginSettings); err != nil { // This is a non-recoverable error. Returning it up the callstack will just // lead to retries of the same failure, so just fail hard. return nil, err } // 根据配置配置 CNI // dockershim currently only supports CNI plugins. pluginSettings.PluginBinDirs = cni.SplitDirs(pluginSettings.PluginBinDirString) cniPlugins := cni.ProbeNetworkPlugins(pluginSettings.PluginConfDir, pluginSettings.PluginBinDirs) // 加了一个默认的 CNI 插件 kubenet cniPlugins = append(cniPlugins, kubenet.NewPlugin(pluginSettings.PluginBinDirs)) netHost := \u0026amp;dockerNetworkHost{ \u0026amp;namespaceGetter{ds}, \u0026amp;portMappingGetter{ds}, } // 根据用户配置选择对应的网络插件对象，做 init() 初始化 plug, err := network.InitNetworkPlugin(cniPlugins, pluginSettings.PluginName, netHost, pluginSettings.HairpinMode, pluginSettings.NonMasqueradeCIDR, pluginSettings.MTU) ds.network = network.NewPluginManager(plug) klog.Infof(\u0026quot;Docker cri networking managed by %v\u0026quot;, plug.Name()) return ds, nil }  Hairpin 模式\n发夹式转发模式 (Hairpin mode)又称反射式转发模式 (Reflective Relay) ，指交换机可以将报文的接受端口同时作为发送端口, 即报文可以从它的入端口转发出去, 如下图所示:\nNewDockerService() 函数首先通过 effectiveHairpinMode() 计算出有效的 Hairpin 模式, 然后根据 NetworkPluginName 从插件列表中选择对应的网络插件对象.\nProbeNetworkPlugins() 根据配置的 CNI 插件的路径生成 network.NetworkPlugin interface 的实现 cniNetworkPlugin。\nInitNetworkPlugin() 负责从网络插件对象列表中根据用户配置的网络插件名选择对应的网络插件对象，调用插件的 init() 执行初始化。\n// pkg/kubelet/dockershim/network/plugins.go // InitNetworkPlugin inits the plugin that matches networkPluginName. Plugins must have unique names. func InitNetworkPlugin(plugins []NetworkPlugin, networkPluginName string, host Host, hairpinMode kubeletconfig.HairpinMode, nonMasqueradeCIDR string, mtu int) (NetworkPlugin, error) { // 如果用户没有配置网络插件名, 默认就是NoopNetworkPlugin, 不会提供任何容器网络 // NoopNetworkPlugin 是 NetworkPlugin interface 的实现 if networkPluginName == \u0026quot;\u0026quot; { // default to the no_op plugin plug := \u0026amp;NoopNetworkPlugin{} plug.Sysctl = utilsysctl.New() if err := plug.Init(host, hairpinMode, nonMasqueradeCIDR, mtu); err != nil { return nil, err } return plug, nil } ... chosenPlugin := pluginMap[networkPluginName] if chosenPlugin != nil { // 执行插件的初始化操作 err := chosenPlugin.Init(host, hairpinMode, nonMasqueradeCIDR, mtu) } ... }  通告 Pod CIDR 的更新 k8s 对 Pod 的管理是通过 runtime 来操作的，因此对 CIDR 的更新也是通过 runtime 实现。当 Pod 的 CIDR 更新时调用 runtime 的 UpdatePodCIDR()。\n// pkg/kubelet/kubelet_network.go // updatePodCIDR updates the pod CIDR in the runtime state if it is different // from the current CIDR. Return true if pod CIDR is actually changed. func (kl *Kubelet) updatePodCIDR(cidr string) (bool, error) { // 配置与当前状态比较，没有变化直接返回 podCIDR := kl.runtimeState.podCIDR() if podCIDR == cidr { return false, nil } // kubelet -\u0026gt; generic runtime -\u0026gt; runtime shim -\u0026gt; network plugin // docker/non-cri implementations have a passthrough UpdatePodCIDR if err := kl.getRuntime().UpdatePodCIDR(cidr); err != nil { // If updatePodCIDR would fail, theoretically pod CIDR could not change. // But it is better to be on the safe side to still return true here. return true, fmt.Errorf(\u0026quot;failed to update pod CIDR: %v\u0026quot;, err) } // 更新当前状态，以便以后比较 kl.runtimeState.setPodCIDR(cidr) return true, nil }  runtime 要先讲下 k8s runtime 的管理。\nk8s 通过 kubeGenericRuntimeManager 来做统一的 RC 管理，该类会调用对应的 RC shim 来做下发操作。\nkubelet 的 containerRuntime 是在 NewMainKubelet() 函数中如下代码片段配置的。\nruntime, err := kuberuntime.NewKubeGenericRuntimeManager( kubecontainer.FilterEventRecorder(kubeDeps.Recorder), ...) klet.containerRuntime = runtime klet.streamingRuntime = runtime klet.runner = runtime  // kuberuntime/kuberuntime_manager.go // UpdatePodCIDR is just a passthrough method to update the runtimeConfig of the shim // with the podCIDR supplied by the kubelet. func (m *kubeGenericRuntimeManager) UpdatePodCIDR(podCIDR string) error { // TODO(#35531): do we really want to write a method on this manager for each // field of the config? klog.Infof(\u0026quot;updating runtime config through cri with podcidr %v\u0026quot;, podCIDR) return m.runtimeService.UpdateRuntimeConfig( \u0026amp;runtimeapi.RuntimeConfig{ NetworkConfig: \u0026amp;runtimeapi.NetworkConfig{ PodCidr: podCIDR, }, }) }  kubeGenericRuntimeManager 的 runtimeService 在初始化时设置的是 instrumentedRuntimeService，这个结构是对 RuntimeService interface 的一个封装和实现，用来记录操作和错误的 metrics。\n// instrumentedRuntimeService wraps the RuntimeService and records the operations // and errors metrics. type instrumentedRuntimeService struct { service internalapi.RuntimeService }  而真正的 RuntimeService interface 的实现是在 NewMainKubelet() 的如下片段中赋值的。\nruntimeService, imageService, err := getRuntimeAndImageServices(remoteRuntimeEndpoint, remoteImageEndpoint, kubeCfg.RuntimeRequestTimeout) klet.runtimeService = runtimeService  remoteRuntimeEndpoint 是在 kubelet 启动命令中指定的值为 unix:///var/run/dockershim.sock ，kubelet 就是通过这个 socket 与 runtime 进行gRPC 通信的。保存在 KubeletFlags 中，该参数在\ntype KubeletFlags struct { KubeConfig string ... RemoteRuntimeEndpoint string RemoteImageEndpoint string }  getRuntimeAndImageServices() 调用 NewRemoteRuntimeService() 根据 RC 的 endpoint 创建一个 gRPC 的 client 封装到 RemoteRuntimeService 中，这是一个 internalapi.RuntimeService interface 的具体实现。\n// NewRemoteRuntimeService creates a new internalapi.RuntimeService. func NewRemoteRuntimeService(endpoint string, connectionTimeout time.Duration) (internalapi.RuntimeService, error) { addr, dailer, err := util.GetAddressAndDialer(endpoint) ctx, cancel := context.WithTimeout(context.Background(), connectionTimeout) defer cancel() conn, err := grpc.DialContext(ctx, addr, grpc.WithInsecure(), grpc.WithDialer(dailer), grpc.WithDefaultCallOptions(grpc.MaxCallRecvMsgSize(maxMsgSize))) return \u0026amp;RemoteRuntimeService{ timeout: connectionTimeout, runtimeClient: runtimeapi.NewRuntimeServiceClient(conn), lastError: make(map[string]string), errorPrinted: make(map[string]time.Time), }, nil }  runtime 主要提供两种服务：RuntimeService 和 ImageService 用来管理容器的镜像。 k8s 与 runtime 通过 RPC 通信，在配置 podCIDR 时调用的是 RuntimeService 的 UpdateRuntimeConfig rpc：\n// pkg/kubelet/apis/cri/runtime/v1alpha2/api.proto service RuntimeService { ... // UpdateRuntimeConfig updates the runtime configuration based on the given request. rpc UpdateRuntimeConfig(UpdateRuntimeConfigRequest) returns (UpdateRuntimeConfigResponse) {} }  这样网络配置就下发给了 runtime，runtime 调用 CNI 插件来做网络配置变更。\n// pkg/kubelet/dockershim/docker_service.go // UpdateRuntimeConfig updates the runtime config. Currently only handles podCIDR updates. func (ds *dockerService) UpdateRuntimeConfig(_ context.Context, r *runtimeapi.UpdateRuntimeConfigRequest) (*runtimeapi.UpdateRuntimeConfigResponse, error) { runtimeConfig := r.GetRuntimeConfig() if runtimeConfig == nil { return \u0026amp;runtimeapi.UpdateRuntimeConfigResponse{}, nil } klog.Infof(\u0026quot;docker cri received runtime config %+v\u0026quot;, runtimeConfig) if ds.network != nil \u0026amp;\u0026amp; runtimeConfig.NetworkConfig.PodCidr != \u0026quot;\u0026quot; { event := make(map[string]interface{}) event[network.NET_PLUGIN_EVENT_POD_CIDR_CHANGE_DETAIL_CIDR] = runtimeConfig.NetworkConfig.PodCidr ds.network.Event(network.NET_PLUGIN_EVENT_POD_CIDR_CHANGE, event) } return \u0026amp;runtimeapi.UpdateRuntimeConfigResponse{}, nil }  下图是 kubelet runtime UML\nkubenet plugin 实现 前面知道网络插件的接口是 NetworkPlugin interface，k8s kubenet 网络框架用 embed network.NoopNetworkPlugin 的 kubenetNetworkPlugin 实现了接口。\nkubenet 利用的是官方提供的三个 cni 类型插件: bridge, host-local, loopback (参考 cni plugins, cni ipam), 这个插件一般位于每个 Node 的 /opt/cni/bin 目录。\ntype kubenetNetworkPlugin struct { network.NoopNetworkPlugin host network.Host netConfig *libcni.NetworkConfig loConfig *libcni.NetworkConfig cniConfig libcni.CNI bandwidthShaper bandwidth.BandwidthShaper mu sync.Mutex //Mutex for protecting podIPs map, netConfig, and shaper initialization podIPs map[kubecontainer.ContainerID]string mtu int execer utilexec.Interface nsenterPath string hairpinMode kubeletconfig.HairpinMode // kubenet can use either hostportSyncer and hostportManager to implement hostports // Currently, if network host supports legacy features, hostportSyncer will be used, // otherwise, hostportManager will be used. hostportSyncer hostport.HostportSyncer hostportManager hostport.HostPortManager iptables utiliptables.Interface sysctl utilsysctl.Interface ebtables utilebtables.Interface // binDirs is passed by kubelet cni-bin-dir parameter. // kubenet will search for CNI binaries in DefaultCNIDir first, then continue to binDirs. binDirs []string nonMasqueradeCIDR string podCidr string gateway net.IP }  kubenet 直接利用了官方提供的三个 cni plugin:\n// pkg/kubelet/network/kubenet/kubenet_linux.go // CNI plugins required by kubenet in /opt/cni/bin or vendor directory var requiredCNIPlugins = [...]string{\u0026quot;bridge\u0026quot;, \u0026quot;host-local\u0026quot;, \u0026quot;loopback\u0026quot;}  kubenet 网络框架原理非常的简单, 主要利用 \u0026ldquo;bridge\u0026rdquo;, \u0026ldquo;host-local\u0026rdquo;, \u0026ldquo;loopback\u0026rdquo; (位于 /opt/cni/bin 目录下) 这三个 cni plugin主要的功能：\n 在每个 Node 上创建一个 cbr0 网桥 根据 PodCIDR 为每个 Pod 的 interface 分配一个 ip, 将该 interface 连接到 cbr0 网桥上.  当然, 对于 kubernetes 集群来说, 还需要解决两个问题:\n Node 的 PodCIDR 设置\nk8s kubenet 网络框架中，必须给每个 node 配置一个 podCIDR.\n那么, 每个 Node 的 PodCIDR 如何设置呢? 这个需要参考 kubenet 网络的配置文档了:\n The node must be assigned an IP subnet through either the \u0026ndash;pod-cidr kubelet command-line option or the \u0026ndash;allocate-node-cidrs=true \u0026ndash;cluster-cidr= controller-manager command-line options.\n 其实就是两种方式:\n 通过 \u0026ndash;pod-cidr 为每个 Node 上的 kubelet 配置好 PodCIDR 通过 \u0026ndash;allocate-node-cidrs=true \u0026ndash;cluster-cidr= 让 controller-manager 来为每个 Node 分配 PodCIDR.  Node 之间的路由设置\n虽然现在每个 Node 都配置好了 PodCIDR, 比如:\nNode1: 192.168.0.0/24 Node2: 192.168.1.0/24  但是 Node1 和 Node2 上的容器如何通信呢?\n It is typically used together with a cloud provider that sets up routing rules for communication between nodes, or in single-node environments.\n 通常情况下, kubenet 网络插件会跟 cloud provider 一起使用, 从而利用 cloud provider 来设置节点间的路由. kubenet 网络插件也可以用在单节点环境, 这样就不需要考虑 Node 间的路由了. 另外, 我们还可以通过实现一个 network controller 来保证 Node 间的路由.\n  kubenet Init // pkg/kubelet/dockershim/network/kubenet/kubenet_linux.go func NewPlugin(networkPluginDirs []string) network.NetworkPlugin { protocol := utiliptables.ProtocolIpv4 execer := utilexec.New() dbus := utildbus.New() sysctl := utilsysctl.New() iptInterface := utiliptables.New(execer, dbus, protocol) return \u0026amp;kubenetNetworkPlugin{ podIPs: make(map[kubecontainer.ContainerID]string), execer: utilexec.New(), iptables: iptInterface, sysctl: sysctl, binDirs: append([]string{DefaultCNIDir}, networkPluginDirs...), hostportSyncer: hostport.NewHostportSyncer(iptInterface), hostportManager: hostport.NewHostportManager(iptInterface), nonMasqueradeCIDR: \u0026quot;10.0.0.0/8\u0026quot;, } }  在前面的 InitNetworkPlugin() 流程中会调用各个插件的 Init() 来初始化插件。\n// pkg/kubelet/dockershim/network/kubenet/kubenet_linux.go func (plugin *kubenetNetworkPlugin) Init(host network.Host, hairpinMode kubeletconfig.HairpinMode, nonMasqueradeCIDR string, mtu int) error { ... // 确认加载了 br-netfilter，设置 bridge-nf-call-iptables=1 plugin.execer.Command(\u0026quot;modprobe\u0026quot;, \u0026quot;br-netfilter\u0026quot;).CombinedOutput() err := plugin.sysctl.SetSysctl(sysctlBridgeCallIPTables, 1) // 配置 loopback cni 插件 plugin.loConfig, err = libcni.ConfFromBytes([]byte(`{ \u0026quot;cniVersion\u0026quot;: \u0026quot;0.1.0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;kubenet-loopback\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;loopback\u0026quot; }`)) plugin.nsenterPath, err = plugin.execer.LookPath(\u0026quot;nsenter\u0026quot;) // 下发 SNAT 的 ipatable rule // Need to SNAT outbound traffic from cluster if err = plugin.ensureMasqRule(); err != nil { return err } return nil }   在 kubenet 中有个 MTU 的配置选项，network-plugin-mtu 指定 MTU，设置合理的 MTU 能有一个更好的网络性能。仅在 kubenet plugin 中支持。\n kubenet Event kubelet 启动到 NewMainKubelet 时, 根据用户配置通过 klet.updatePodCIDR(kubeCfg.PodCIDR) 向 k8s network plugin 通报 NET_PLUGIN_EVENT_POD_CIDR_CHANGE 事件, 该事件将会被 Event 方法捕获.\n// pkg/kubelet/network/kubenet/kubenet_linux.go const NET_CONFIG_TEMPLATE = `{ \u0026quot;cniVersion\u0026quot;: \u0026quot;0.1.0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;kubenet\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;bridge\u0026quot;, \u0026quot;bridge\u0026quot;: \u0026quot;%s\u0026quot;, \u0026quot;mtu\u0026quot;: %d, \u0026quot;addIf\u0026quot;: \u0026quot;%s\u0026quot;, \u0026quot;isGateway\u0026quot;: true, \u0026quot;ipMasq\u0026quot;: false, \u0026quot;hairpinMode\u0026quot;: %t, \u0026quot;ipam\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;host-local\u0026quot;, \u0026quot;subnet\u0026quot;: \u0026quot;%s\u0026quot;, \u0026quot;gateway\u0026quot;: \u0026quot;%s\u0026quot;, \u0026quot;routes\u0026quot;: [ { \u0026quot;dst\u0026quot;: \u0026quot;0.0.0.0/0\u0026quot; } ] } }` func (plugin *kubenetNetworkPlugin) Event(name string, details map[string]interface{}) { podCIDR, ok := details[network.NET_PLUGIN_EVENT_POD_CIDR_CHANGE_DETAIL_CIDR].(string) _, cidr, err := net.ParseCIDR(podCIDR) if err == nil { setHairpin := plugin.hairpinMode == kubeletconfig.HairpinVeth // Set bridge address to first address in IPNet cidr.IP[len(cidr.IP)-1] += 1 // 更新 cni 网络配置 // 从 NET_CONFIG_TEMPLATE 中看出, host-local ipam 的 subnet 就是 podCIDR // 这其实也就是为什么 k8s kubenet 网络插件需要为每个 node 分配 podCIDR 的原因 json := fmt.Sprintf(NET_CONFIG_TEMPLATE, BridgeName, plugin.mtu, network.DefaultInterfaceName, setHairpin, podCIDR, cidr.IP.String()) // 网络配置都保存在 netConfig 中 plugin.netConfig, err = libcni.ConfFromBytes([]byte(json)) if err == nil { klog.V(5).Infof(\u0026quot;CNI network config:\\n%s\u0026quot;, json) // Ensure cbr0 has no conflicting addresses; CNI's 'bridge' // plugin will bail out if the bridge has an unexpected one plugin.clearBridgeAddressesExcept(cidr) } plugin.podCidr = podCIDR plugin.gateway = cidr.IP } }   todo: Event() 也只是更新了 podCidr 和 netConfig，哪里下发了更新？\n 根据配置的改变设置 kubenetNetworkPlugin 对应的变量。\nkubenet SetUpPod 创建 Pod 的时候会调用该方法，该方法调用 setup() 来完成配置，这个接口最重要的功能是将容器的 eth0 接口加入到了 namespace 中\n// setup sets up networking through CNI using the given ns/name and sandbox ID. func (plugin *kubenetNetworkPlugin) setup(namespace string, name string, id kubecontainer.ContainerID, annotations map[string]string) error { // 添加 loopback interface 到 pod 的 network namespace // Bring up container loopback interface if _, err := plugin.addContainerToNetwork(plugin.loConfig, \u0026quot;lo\u0026quot;, namespace, name, id); err != nil { return err } // 添加 DefaultInterfaceName eth0 到 pod 的 network namespace // Hook container up with our bridge resT, err := plugin.addContainerToNetwork(plugin.netConfig, network.DefaultInterfaceName, namespace, name, id) if err != nil { return err } // Coerce the CNI result version res, err := cnitypes020.GetResult(resT) ip4 := res.IP4.IP.IP.To4() // 为了配置 hairpin 设置网卡混杂模式 ... plugin.podIPs[id] = ip4.String() // TODO: replace with CNI port-forwarding plugin // TODO: portMappings 的用途是什么？ portMappings, err := plugin.host.GetPodPortMappings(id.ID) if err != nil { return err } if portMappings != nil \u0026amp;\u0026amp; len(portMappings) \u0026gt; 0 { if err := plugin.hostportManager.Add(id.ID, \u0026amp;hostport.PodPortMapping{ Namespace: namespace, Name: name, PortMappings: portMappings, IP: ip4, HostNetwork: false, }, BridgeName); err != nil { return err } } return nil }  接着看看 addContainerToNetwork() 方法:\n// pkg/kubelet/dockershim/network/kubenet/kubenet_linux.go func (plugin *kubenetNetworkPlugin) addContainerToNetwork(config *libcni.NetworkConfig, ifName, namespace, name string, id kubecontainer.ContainerID) (cnitypes.Result, error) { rt, err := plugin.buildCNIRuntimeConf(ifName, id, true) if err != nil { return nil, fmt.Errorf(\u0026quot;Error building CNI config: %v\u0026quot;, err) } // The network plugin can take up to 3 seconds to execute, // so yield the lock while it runs. plugin.mu.Unlock() res, err := plugin.cniConfig.AddNetwork(config, rt) plugin.mu.Lock() return res, nil }  由前面 CNI 库接口可知, plugin.cniConfig.AddNetwork() 实际上调用的是 cni plugin 去实现容器网络配置. kubenet plugin 主要通过 loopback 和 bridge cni 插件将容器的 lo 和 eth0 添加到容器网络中. bridge 插件负责 Node 上 cbr0 的创建, 然后创建 veth 接口对, 通过 veth 接口对, 将容器添加到容器网络中. 另外, host-local IPAM plugin 负责为 eth0 分配 ip 地址.\nkubenet TearDownPod 删除 Pod 的时候会被调用。主要是通过函数 teardown() 实现。主要的流程是调用 CNI 删除网络配置。\n// Tears down as much of a pod's network as it can even if errors occur. Returns // an aggregate error composed of all errors encountered during the teardown. func (plugin *kubenetNetworkPlugin) teardown(namespace string, name string, id kubecontainer.ContainerID, podIP string) error { errList := []error{} if err := plugin.delContainerFromNetwork(plugin.netConfig, network.DefaultInterfaceName, namespace, name, id); err != nil { // This is to prevent returning error when TearDownPod is called twice on the same pod. This helps to reduce event pollution. if podIP != \u0026quot;\u0026quot; { klog.Warningf(\u0026quot;Failed to delete container from kubenet: %v\u0026quot;, err) } else { errList = append(errList, err) } } portMappings, err := plugin.host.GetPodPortMappings(id.ID) if err != nil { errList = append(errList, err) } else if portMappings != nil \u0026amp;\u0026amp; len(portMappings) \u0026gt; 0 { if err = plugin.hostportManager.Remove(id.ID, \u0026amp;hostport.PodPortMapping{ Namespace: namespace, Name: name, PortMappings: portMappings, HostNetwork: false, }); err != nil { errList = append(errList, err) } } return utilerrors.NewAggregate(errList) }  由前面 CNI 库接口可知, plugin.cniConfig.DelNetwork() 实际上调用的是 cni plugin 去删除容器网络配置. bridge 插件负责调用 host-local IPAM plugin 释放该容器的 ip, 然后删除容器的网络接口等.\nCNI plugin 实现 CNI plugin 是一种更通用的实现，允许用户自定义插件。cniNetworkPlugin 是 NetworkPlugin interface 的一个实现，具体的代码如下。\n// pkg/kubelet/dockershim/network/cni/cni.go type cniNetworkPlugin struct { network.NoopNetworkPlugin loNetwork *cniNetwork sync.RWMutex defaultNetwork *cniNetwork host network.Host execer utilexec.Interface nsenterPath string confDir string binDirs []string podCidr string }  通过 cniNetwork 类型的 loNetwork 和 defaultNetwork 来调用 CNI 插件，cniNetwork 定义如下。\n// pkg/kubelet/dockershim/network/cni/cni.go type cniNetwork struct { name string NetworkConfig *libcni.NetworkConfigList CNIConfig libcni.CNI }  CNI Init 在 NewDockerService() 函数中调用 ProbeNetworkPlugins() 根据配置的 CNI 插件的路径生成 network.NetworkPlugin interface 的实现 cniNetworkPlugin。\n// pkg/kubelet/dockershim/network/cni/cni.go func ProbeNetworkPlugins(confDir string, binDirs []string) []network.NetworkPlugin { old := binDirs binDirs = make([]string, 0, len(binDirs)) for _, dir := range old { if dir != \u0026quot;\u0026quot; { binDirs = append(binDirs, dir) } } plugin := \u0026amp;cniNetworkPlugin{ defaultNetwork: nil, loNetwork: getLoNetwork(binDirs), execer: utilexec.New(), confDir: confDir, binDirs: binDirs, } // sync NetworkConfig in best effort during probing. plugin.syncNetworkConfig() return []network.NetworkPlugin{plugin} }  主要是对 loNetwork 和 defaultNetwork 变量的配置。\n// pkg/kubelet/dockershim/network/cni/cni_others.go func getLoNetwork(binDirs []string) *cniNetwork { loConfig, err := libcni.ConfListFromBytes([]byte(`{ \u0026quot;cniVersion\u0026quot;: \u0026quot;0.2.0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;cni-loopback\u0026quot;, \u0026quot;plugins\u0026quot;:[{ \u0026quot;type\u0026quot;: \u0026quot;loopback\u0026quot; }] }`)) loNetwork := \u0026amp;cniNetwork{ name: \u0026quot;lo\u0026quot;, NetworkConfig: loConfig, CNIConfig: \u0026amp;libcni.CNIConfig{Path: binDirs}, } return loNetwork }  Init() 做的就是配置 defaultNetwork\nfunc getDefaultCNINetwork(confDir string, binDirs []string) (*cniNetwork, error) { // 从配置文件获取 confList network := \u0026amp;cniNetwork{ name: confList.Name, NetworkConfig: confList, CNIConfig: \u0026amp;libcni.CNIConfig{Path: binDirs}, } return network, nil } }  CNI Event 收到 NET_PLUGIN_EVENT_POD_CIDR_CHANGE 事件时只是更新了 podCidr 的值\nfunc (plugin *cniNetworkPlugin) Event(name string, details map[string]interface{}) { podCIDR, ok := details[network.NET_PLUGIN_EVENT_POD_CIDR_CHANGE_DETAIL_CIDR].(string) plugin.podCidr = podCIDR }  可见 k8s cni 网络方式并没有规定使用 podCidr 来配置 node 上容器的网络 ip 段, 而把 pod 的 ip 分配完全交给 IPAM, 这样使得 IPAM 更加灵活, 多样化和定制化\nCNI SetUpPod // pkg/kubelet/dockershim/network/cni/cni.go func (plugin *cniNetworkPlugin) SetUpPod(namespace string, name string, id kubecontainer.ContainerID, annotations, options map[string]string) error { ... // Windows doesn't have loNetwork. It comes only with Linux if plugin.loNetwork != nil { if _, err = plugin.addToNetwork(plugin.loNetwork, name, namespace, id, netnsPath, annotations, options); err != nil { return err } } _, err = plugin.addToNetwork(plugin.getDefaultNetwork(), name, namespace, id, netnsPath, annotations, options) return err }  func (plugin *cniNetworkPlugin) addToNetwork(network *cniNetwork, podName string, podNamespace string, podSandboxID kubecontainer.ContainerID, podNetnsPath string, annotations, options map[string]string) (cnitypes.Result, error) { netConf, cniNet := network.NetworkConfig, network.CNIConfig res, err := cniNet.AddNetworkList(netConf, rt) }  由前面 CNI 库接口可知, cninet.AddNetwork() 实际上调用的是底层用户配置的 cni plugin 去实现容器网络配置.\nCNI TearDownPod 与前面的流程类似，最终调用底层的 cni plugin 删掉配置。代码略。\nkubernets network plugin 安装 kubelet 有一个默认的 plugin，然后为整个集群提供一个默认的网络。当启动时探测到插件后就可以在 pod 整个生命周期里调换用。有两个启动参数：\n cni-bin-dir：启动时加载这个参数指定的路径里的 plugin network-plugin：插件的名字，要能匹配上面路径中的插件，比如 CNI 配置为 \u0026ldquo;cni\u0026rdquo;  network plugin 需求 plguin 除了要提供 NetworkPlugin interface 添加和删除 pod 的网络之外，还要实现对 kube-proxy 的支持。proxy 依赖 iptables，plugin 需要确保容器流量可以使用 iptables。比如 plugin 将容器添加到 Linux bridge，就需要通过 sysctl 设置 net/bridge/bridge-nf-call-iptables = 1 来确保 iptables proxy 功能正常。\n如果没有指定 network plugin，就使用 noop plugin 设置 net/bridge/bridge-nf-call-iptables=1。\nCNI 在 kubelet 命令行中 --network-plugin=cni 指定了采用 CNI 插件，kubelet 从 --cni-conf-dir（默认 /etc/cni/net.d）读取配置文件来配置 pod 网络。CNI 配置文件要遵循 CNI specification，配置文件中指定的 CNI 插件的执行程序要放在 \u0026ndash;cni-bin-dir (default /opt/cni/bin)。\n如果目录下有多个 CNI 配置文件，按字典序采用第一个配置文件。\n除了配置文件中指定的 CNI 插件外，K8S 还需要标准的 lo 插件。\nhostPort 支持 CNI plugin 支持 hostPort，可以采用官方的 portmap，也可以自己实现。\n需要在 cni-conf-dir 中开启 portMappings capability 来支持 hostPort。\n{ \u0026quot;name\u0026quot;: \u0026quot;k8s-pod-network\u0026quot;, \u0026quot;cniVersion\u0026quot;: \u0026quot;0.3.0\u0026quot;, \u0026quot;plugins\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;calico\u0026quot;, \u0026quot;log_level\u0026quot;: \u0026quot;info\u0026quot;, \u0026quot;datastore_type\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;nodename\u0026quot;: \u0026quot;127.0.0.1\u0026quot;, \u0026quot;ipam\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;host-local\u0026quot;, \u0026quot;subnet\u0026quot;: \u0026quot;usePodCidr\u0026quot; }, \u0026quot;policy\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;k8s\u0026quot; }, \u0026quot;kubernetes\u0026quot;: { \u0026quot;kubeconfig\u0026quot;: \u0026quot;/etc/cni/net.d/calico-kubeconfig\u0026quot; } }, { \u0026quot;type\u0026quot;: \u0026quot;portmap\u0026quot;, \u0026quot;capabilities\u0026quot;: {\u0026quot;portMappings\u0026quot;: true} } ] }  traffic shaping 支持 CNI plugin 支持 pod ingress 和 egress 整形，可以使用官方提供的 bandwidth 或自定义的插件。同样需要在配置文件（默认在 /etc/cni/net.d）中配置。\n{ \u0026quot;name\u0026quot;: \u0026quot;k8s-pod-network\u0026quot;, \u0026quot;cniVersion\u0026quot;: \u0026quot;0.3.0\u0026quot;, \u0026quot;plugins\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;calico\u0026quot;, \u0026quot;log_level\u0026quot;: \u0026quot;info\u0026quot;, \u0026quot;datastore_type\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;nodename\u0026quot;: \u0026quot;127.0.0.1\u0026quot;, \u0026quot;ipam\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;host-local\u0026quot;, \u0026quot;subnet\u0026quot;: \u0026quot;usePodCidr\u0026quot; }, \u0026quot;policy\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;k8s\u0026quot; }, \u0026quot;kubernetes\u0026quot;: { \u0026quot;kubeconfig\u0026quot;: \u0026quot;/etc/cni/net.d/calico-kubeconfig\u0026quot; } }, { \u0026quot;type\u0026quot;: \u0026quot;bandwidth\u0026quot;, \u0026quot;capabilities\u0026quot;: {\u0026quot;bandwidth\u0026quot;: true} } ] }  现在你可以向 pod 中添加 kubernetes.io/ingress-bandwidth 和 kubernetes.io/egress-bandwidth 的 annotations，例如：\napiVersion: v1 kind: Pod metadata: annotations: kubernetes.io/ingress-bandwidth: 1M kubernetes.io/egress-bandwidth: 1M ...  CNI 演进 Multi CNI and Containers with Multi Network Interfaces on Kubernetes with CNI-Genie\nmultus-cni\n参考 k8s network\nNetwork Plugins\n问题  dockerService 用途是什么？\n 在一个集群里不同的 Node 上可以配置不同的 plugin 吗？\n  ","date":1559001600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559001600,"objectID":"631156bfce891192f2b6a9b8eef15b66","permalink":"/post/cloud/k8s/201905-k8s-network-arch/","publishdate":"2019-05-28T00:00:00Z","relpermalink":"/post/cloud/k8s/201905-k8s-network-arch/","section":"post","summary":"Kubernetes本身不提供容器网络，但具有可扩展的网络框架","tags":["Kubernetes","CNI"],"title":"Kubernetes网络框架","type":"post"},{"authors":null,"categories":null,"content":" 人们对云计算提出了更高的要求，为大量项目构建运营环境的效率问题，缩短新业务的上线部署时间，大规模的计算机房快速迁移需求；提高服务器资源的利用率，同时确保相同的性能和可用性，又有降低成本的需求。相较于传统的虚拟化解决方案，容器云可以较好的实现上述目标。\n容器云的核心功能：\n 快速扩容 智能调度和编排 弹性伸缩  快速扩容  扩容速度：  VM - 扩容20个实例需要4分钟（扩容完成后需要再执行服务发布） docker - 扩容20个实例仅需30s，秒级别扩容（扩容完成即服务启动）  扩容速度提高 8~12倍 节约了用户手动操作申请/发布的成本  智能调度 调度系统是云集群的中央处理器，要解决的核心问题是为容器选择合适的宿主机。有如下的指标：\n 资源利用率，提高整体物理集群的资源利用率 业务可用性保障：业务容器容灾能力、保障运行业务的稳定高可用 并发调度能力：调度系统请求处理能力的体现  资源最大化利用  按CPU/Mem/IO等类型对服务进行调度，最大化资源利用 业务按需使用资源，提升资源利用率  混布与独占  在线服务与离线任务混布 重要业务资源池独占  容器编排  有调用关系的多个服务实例，优先部署到相同/相近的宿主机上 同服务实例打散，分布到不同宿主机上，提高服务可用性 高负载容器，自动迁移到低负载宿主机 自动化容器实例健康检查，异常实例自动迁移  调度计算 通过先过滤filter之后排序打分rank的方式找到最优的部署位置。\n在一批宿主机中先过滤掉超售的，然后考虑到打散、混部、减少碎片和负载均衡之后找到合适的宿主机\n调度SLA（Service Level Agreement）  高可用：99.999 调度成功率：99.99 并发调度：单机并发处理200+，并发调度机器1000+ 低延迟：TCP90 63ms HA：分布式调度，横向扩展，多IDC部署容灾 监控报警：Falcon  弹性收缩 周期收缩\n根据设定时间段伸缩（适合秒杀/直播等业务）\n监控伸缩\n 根据QPS/CPU等触发条件伸缩 线性可扩展的无状态服务  服务画像\n针对数据建模，描绘服务特征：\n 服务画像：仿照用户画像，根据服务数据，抽取服务Tag  QPS特征（高峰时段、QPS max/min等） 资源利用率 CPU密集型 or IO密集型   基于历史数据建模的服务画像可以做服务特征值的预测，比如QPS的预测：\n QPS预测：RNN LSTM 即使监控数据源完全不可用，无数据，也能较准确的扩缩容  异常处理  监控数据异常，怎么办？会不会因监控值偏低而一直缩容？ 监控数据有延迟，怎么办？ 监控数据没了，怎么办？  通过数据无关的缩容退避+熔断机制来保证异常情况下的正常运行：\n 针对监控数据偏低（异常）而触发持续缩容 数据无关，不关心数据是否异常 如果连续缩容，那么缩容速度会越来越慢 —\u0026gt; 退避 如果连续缩容次数超过阈值，一段时间内禁止缩容 —\u0026gt; 熔断  ","date":1557158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557158400,"objectID":"318d50b7d7643d72671620a688ce7837","permalink":"/post/cloud/container/201905-why-container-cloud/why-container/","publishdate":"2019-05-07T00:00:00+08:00","relpermalink":"/post/cloud/container/201905-why-container-cloud/why-container/","section":"post","summary":"容器云解决大量项目构建运营环境的效率问题","tags":["Docker","Cloud"],"title":"Why 容器云","type":"post"},{"authors":null,"categories":null,"content":" 组件 从 Docker 1.11 之后，Docker Daemon 被分成了多个模块以适应 OCI 标准。容器是由多个组件共同管理协同后才运行起来的，主要的组件有：\n Docker Docker Daemon Containerd RunC  图，容器组件图\n其中，containerd 独立负责容器运行时和生命周期（如创建、启动、停止、中止、信号处理、删除等），其他一些如镜像构建、卷管理、日志等由 Docker Daemon 的其他模块处理。\nDocker 作为 Client 接受用户的指令并将指令发送给 Docker daemon 处理。是 Docker API 的最上层封装，直接面向操作用户。\nDocker Daemon Docker 容器管理的守护进程，负责与 Docker client 交互，实现对 Docker 镜像和容器的管理。\n图，Docker daemon 的作用\n从Docker 1.11开始，通过如下命令启动\ndockerd  在 Hulk 中的进程如下\n/usr/bin/dockerd-current --exec-opt native.cgroupdriver=systemd --userland-proxy-path=/usr/libexec/docker/docker-proxy-current --init-path=/usr/libexec/docker/docker-init-current --seccomp-profile=/etc/docker/seccomp.json --selinux-enabled --log-driver=json-file --log-opt max-size=100m --signature-verification=false --live-restore  Containerd containerd是容器技术标准化之后的产物，为了能够兼容OCI 标准，将容器运行时及其管理功能从 Docker Daemon 剥离。理论上，即使不运行 dockerd，也能够直接通过 containerd 来管理容器。（当然，containerd 本身也只是一个守护进程，容器的实际运行时由后面介绍的 runC 控制。）\nContainerd 主要职责是镜像管理（镜像、元信息等）、容器执行（调用最终运行时（RunC）组件执行）。\n图，Containerd 架构\n图，Containerd 整体架构\ncontainerd 向上为 Docker Daemon 提供了 gRPC 接口，使得 Docker Daemon 屏蔽下面的结构变化，确保原有接口向下兼容。向下通过 containerd-shim 结合 runC，使得引擎可以独立升级，避免之前 Docker Daemon 升级会导致所有容器不可用的问题。\nDocker、containerd 和 containerd-shim 之间的关系，可以通过启动一个 Docker 容器，观察进程之间的关联。首先启动一个容器，\n# docker run -d busybox sleep 1000 94325941834e353c137113d78158a114df3332e8759fba5176bfbd56049f7221  然后通过pstree命令查看进程之间的父子关系（其中 197979 是dockerd的 PID）：\n# pstree -l -a -A 197979 dockerd-current --exec-opt native.cgroupdriver=systemd --userland-proxy-path=/usr/libexec/docker/docker-proxy-current --init-path=/usr/libexec/docker/docker-init-current --seccomp-profile=/etc/docker/seccomp.json --selinux-enabled --log-driver=json-file --log-opt max-size=100m --signature-verification=false --live-restore |-docker-containe -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-runc --runtime-args --systemd-cgroup=true | |-docker-containe 94325941834e353c137113d78158a114df3332e8759fba5176bfbd56049f7221 /var/run/docker/libcontainerd/94325941834e353c137113d78158a114df3332e8759fba5176bfbd56049f7221 docker-runc | | |-sleep 1000 | | `-9*[{docker-containe}]  由于 pstree 截断了进程名字，实际的进程名字是 docker-containerd-shim。Docker daemon 启动之后，dockerd 和 docker-containerd 进程一直存在。当启动容器之后，docker-containerd 进程（也是这里介绍的 containerd 组件）会创建 docker-containerd-shim 进程，其中的参数 94325941834e353c137113d78158a114df3332e8759fba5176bfbd56049f7221 就是要启动容器的 id。最后 docker-containerd-shim 子进程，已经是实际在容器中运行的进程（既 sleep 1000）。\nHulk 中的 containerd 和 containerd-shim 进程如下\n# ps aux | grep containerd root 134481 0.0 0.0 446412 5980 ? Sl 10:31 0:00 /usr/bin/docker-containerd-shim-current 94325941834e353c137113d78158a114df3332e8759fba5176bfbd56049f7221 /var/run/docker/libcontainerd/94325941834e353c137113d78158a114df3332e8759fba5176bfbd56049f7221 docker-runc root 197990 0.0 0.0 3571764 41852 ? Ssl May17 8:26 /usr/bin/docker-containerd-current -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-runc --runtime-args --systemd-cgroup=true  docker-containerd-shim 另一个参数，是一个和容器相关的目录 /var/run/docker/libcontainerd/94325941834e353c137113d78158a114df3332e8759fba5176bfbd56049f7221，里面的内容有：\n. ├── config.json ├── init-stderr ├── init-stdin └── init-stdout  其中包括了容器配置和标准输入、标准输出、标准错误三个管道文件。\ncontainerd 是dockerd和runc之间的一个中间交流组件。\nContainerd-shim Containerd-shim 是一个真实运行的容器的真实垫片载体，每启动一个容器都会起一个新的 containerd-shim 的一个进程，指定的三个参数：\n 容器id boundle目录（containerd的对应某个容器生成的目录，一般位于：/var/run/docker/libcontainerd/containerID）， 运行时二进制（默认为runc）来调用runc的api创建一个容器（比如创建容器：最后拼装的命令如下：runc create xxx）  RunC OCI 定义了容器运行时标准，runC 是 Docker 按照开放容器格式标准（OCF, Open Container Format）制定的一种具体实现。\nrunC 是从 Docker 的 libcontainer 中迁移而来的，实现了容器启停、资源隔离等功能。Docker 默认提供了 docker-runc 实现，事实上，通过 containerd 的封装，可以在 Docker Daemon 启动的时候指定 runc 的实现。\n我们可以通过启动 Docker Daemon 时增加\u0026ndash;add-runtime参数来选择其他的 runC 现。例如：\ndocker daemon --add-runtime \u0026quot;custom=/usr/local/bin/my-runc-replacement\u0026quot;  下面就让我们看下这几个模块如何工作。\n总结 图，各组件的调用关系\n可以查看 Docker update 流程分析 的代码调用流程。\n参考 Docker、Containerd、RunC\u0026hellip;：你应该知道的所有\n","date":1553385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553385600,"objectID":"3a73e25347e9c73457ff85efca8c4f74","permalink":"/post/cloud/container/201903-container-component/","publishdate":"2019-03-24T00:00:00Z","relpermalink":"/post/cloud/container/201903-container-component/","section":"post","summary":"Docker、Containerd、RunC...：你应该知道的所有","tags":["Docker"],"title":"Docker 容器组件","type":"post"},{"authors":null,"categories":null,"content":" API对象 在Kubernetes中API对象是以树形结构表示的，一个API对象在Etcd里完整资源路径，是由Group（API组）、Version（API版本）和Resource（API资源类型）三个部分组成的。\n如果现在要声明一个CronJob对象，那么YAML的开始部分会这么写，CronJob就是这个API对象的资源类型，Batch就是它们的组，v2alpha1就是它的版本\napiVersion: batch/v2alpha1 kind: CronJob ...  API解析 Kubernetes通过对API解析找到对应的对象，分为如下3步：\n 解析API的组 Kubernetes的对象分两种：  核心API对象（如Pod、Node），是不需要Group的，直接在 /api这个下面进行解析 非核心API对象，在 /apis 下先解析出Group，根据batch这个Group找到 /apis/batch，API Group的分类是以对象功能为依据的。  解析API对象的版本号 匹配API对象的资源类型  创建对象 在前面匹配到正确的版本之后，Kubernetes就知道要创建的是一个/apis/batch/v2alpha1下的CronJob对象，APIServer会继续创建这个Cronjob对象。创建过程如下图\n 当发起创建CronJob的POST请求之后，YAML的信息就被提交给了APIServer，APIServer的第一个功能就是过滤这个请求，并完成一些前置性的工作，比如授权、超时处理、审计等 请求进入MUX和Routes流程，MUX和Routes是APIServer完成URL和Handler绑定的场所。APIServer的Handler要做的事情，就是按照上面介绍的匹配过程，找到对应的CronJob类型定义。 根据这个CronJob类型定义，使用用户提交的YAML文件里的字段，创建一个CronJob对象。这个过程中，APIServer会把用户提交的YAML文件，转换成一个叫做Super Version的对象，它正是该API资源类型所有版本的字段全集，这样用户提交的不同版本的YAML文件，就都可以用这个SuperVersion对象来进行处理了。 APIServer会先后进行Admission（如Admission Controller 和 Initializer）和Validation操作（负责验证这个对象里的各个字段是否何方，被验证过得API对象都保存在APIServer里一个叫做Registry的数据结构中）。 APIServer会把验证过得API对象转换成用户最初提交的版本，进行系列化操作，并调用Etcd的API把它保存起来。\n  CRD API插件CRD（Custom Resource Definition） 允许用户在Kubernetes中添加一个跟Pod、Node类似的、新的API资源类型，即：自定义API资源\n举个栗子，添加一个叫Network的API资源类型，它的作用是一旦用户创建一个Network对象，那么Kubernetes就可以使用这个对象定义的网络参数，调用真实的网络插件，为用户创建一个真正的网络，这个过程分为两步\n 首先定义CRD  定义一个group为samplecrd.k8s.io， version为v1的API信息，指定了这个CR的资源类型叫做Network，定义的这个Network是属于一个Namespace的对象，类似于Pod。\napiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: networks.samplecrd.k8s.io spec: group: samplecrd.k8s.io version: v1 names: kind: Network plural: networks scope: Namespaced   对象实例化  实例化名为example-network的Network对象，API组是samplecrd.k8s.io，版本是v1。\napiVersion: samplecrd.k8s.io/v1 kind: Network metadata: name: example-network spec: cidr: \u0026quot;192.168.0.0/16\u0026quot; gateway: \u0026quot;192.168.0.1\u0026quot;  Network对象YAML文件，名叫example-network.yaml,API资源类型是Network，API组是samplecrd.k8s.io，版本是v1\nKubernetes的声明式API能够对API对象进行增量的更新操作：\n 定义好期望的API对象后，Kubernetes来尽力让对象的状态符合预期 允许多个YAML表达，以PATCH的方式对API对象进行修改，而不用关心原始YAML的内容  基于上面两种特性，Kubernetes可以实现基于API对象的更删改查，完成预期和定义的协调过程。\n因此Kubernetes项目编排能力的核心是声明式API。\nKubernetes编程范式即：如何使用控制器模式，同Kubernetes里的API对象的“增、删、改、查”进行协作，进而完成用户业务逻辑的编写过程。\nkubectl apply kubectl apply是声明式的请求，下面一个Deployment的YAML的例子\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80  然后用kubectl apply创建这个Deployment\n$ kubectl apply -f nginx.yaml  修改一下nginx里定义的镜像\napiVersion: apps/v1 kind: Deployment ... spec: containers: - name: nginx image: nginx:1.7.9  执行kubectl apply命令，触发滚动更新\n$ kubectl apply -f nginx.yaml  后面一次的 kubectl apply命令执行了一个对原有API对象的PATCH操作，这是声明式命令同时可以进行多个写操作，具有Merge的能力；而像 kubectl replace命令是用新的YAML替换旧的，这种响应式命令每次只能处理一次写操作。\n声明式API的应用 Istio通过声明式API实现对应用容器所在POD注入Sidecar，然后通过iptables劫持POD的进站和出站流量到Sidecar，Istio通过对Sidecar下发策略来实现对应用流量的管控，继而实现微服务治理。\n在微服务治理中，对Envoy容器的部署和对Envoy代理的配置，应用容器都是不感知的。Istio是使用Kubernetes的Dynamic Admission Control来实现的。\n在APIServer收到API对象的提交请求后，在正常处理这些操作之前会做一些初始化的操作，比如为某些pod或容器加上一些label。这些初始化操作是通过Kubernetes的Admission Controller实现的，在APIServer对象创建之后调用，但这种方式的缺陷是需要将Admission Controller的代码编译到APIServer中，这不是很方便。Kubernetes 1.7引入了热插拔的Admission机制，它就是Dynamic Admission Control，也叫做Initializer。\n如下定义的应用的Pod，包含一个myapp-container的容器。\napiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026amp;\u0026amp; sleep 3600']  Istio要做的就是在这个Pod YAML被提交给Kubernetes之后，在它对应的API对象里自动加上Envoy容器的配置，使对象变成如下的样子：\napiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026amp;\u0026amp; sleep 3600'] - name: envoy image: lyft/envoy:845747b88f102c0fd262ab234308e9e22f693a1 command: [\u0026quot;/usr/local/bin/envoy\u0026quot;] ...  这个pod多了一个envoy的容器，Istio具体的做法是\n 定义Envoy容器的Initializer，并以ConfigMap的方式保存到Kubernetes中 Istio将编写好的Initializer作为一个Pod部署在Kubernetes中  Envoy容器的ConfigMap定义，\napiVersion: v1 kind: ConfigMap metadata: name: envoy-initializer data: config: | containers: - name: envoy image: lyft/envoy:845747db88f102c0fd262ab234308e9e22f693a1 command: [\u0026quot;/usr/local/bin/envoy\u0026quot;] args: - \u0026quot;--concurrency 4\u0026quot; - \u0026quot;--config-path /etc/envoy/envoy.json\u0026quot; - \u0026quot;--mode serve\u0026quot; ports: - containerPort: 80 protocol: TCP resources: limits: cpu: \u0026quot;1000m\u0026quot; memory: \u0026quot;512Mi\u0026quot; requests: cpu: \u0026quot;100m\u0026quot; memory: \u0026quot;64Mi\u0026quot; volumeMounts: - name: envoy-conf mountPath: /etc/envoy volumes: - name: envoy-conf configMap: name: envoy  这个ConfigMap的data部分，正是一个Pod对象的一部分定义，其中可以看到Envoy容器对应的Container字段，以及一个用来声明Envoy配置文件的volumes字段。Initializer要做的就是把这部分Envoy相关的字段，自动添加到用户提交的Pod的API对象里。但是用户提交的Pod里本来就有containers和volumes字段，所以Kubernetes在处理这样的更新请求时，就必须使用类似于git merge这样的操作，才能将这两部分内容合并在一起。即Initializer更新用户的Pod对象时，必须使用PATCH API来完成。\nEnvoy Initializer的pod定义\napiVersion: v1 kind: Pod metadata: labels: app: envoy-initializer name: envoy-initializer spec: containers: - name: envoy-initializer image: envoy-initializer:0.0.1 imagePullPolicy: Always ```\t`envoy-initializer:0.0.1` 镜像是一个自定义控制器（Custom Controller）。Kubernetes的控制器实际上是一个死循环：它不断地获取实际状态，然后与期望状态作对比，并以此为依据决定下一步的操作。 对Initializer控制器，不断获取的实际状态，就是用户新创建的Pod，它期望的状态就是这个Pod里被添加了Envoy容器的定义。它的控制逻辑如下： ```go for { // 获取新创建的 Pod pod := client.GetLatestPod() // Diff 一下，检查是否已经初始化过 if !isInitialized(pod) { // 没有？那就来初始化一下 //istio要往这个Pod里合并的字段，就是ConfigMap里data字段的值 doSomething(pod) } } func doSomething(pod) { //调用APIServer拿到ConfigMap cm := client.Get(ConfigMap, \u0026quot;envoy-initializer\u0026quot;) //把ConfigMap里存在的containers和volumes字段，直接添加进一个空的Pod对象 newPod := Pod{} newPod.Spec.Containers = cm.Containers newPod.Spec.Volumes = cm.Volumes // Kubernetes的API库，提供一个方法使我们可以直接使用新旧两个Pod对象，生成 patch 数据 patchBytes := strategicpatch.CreateTwoWayMergePatch(pod, newPod) // 发起 PATCH 请求，修改这个 pod 对象 client.Patch(pod.Name, patchBytes) }  Envoy机制正是利用了Kubernetes能够对API对象做增量更新，这是Kubernetes声明式API的独特之处。\n参考 Dynamic Admission Control\n【Kubernetes】深入解析声明式API\n","date":1545753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545753600,"objectID":"a932d713bc6cbd242ccbb247be2576d4","permalink":"/post/cloud/k8s/201904-k8s-declarative-api/","publishdate":"2018-12-26T00:00:00+08:00","relpermalink":"/post/cloud/k8s/201904-k8s-declarative-api/","section":"post","summary":"声明式API是Kubernetes成为容器编排事实标准的利器","tags":["Kubernetes"],"title":"Kubernetes声明式API","type":"post"},{"authors":null,"categories":null,"content":" calico calico是一个比较有趣的虚拟网络解决方案，适用于容器的解决方案，完全利用路由规则实现动态组网，通过BGP协议通告路由。\ncalico的好处是endpoints组成的网络是单纯的三层网络，报文的流向完全通过路由规则控制，没有overlay等额外开销。\ncalico的endpoint可以漂移，并且实现了acl。\ncalico的缺点是路由的数目与容器数目相同，非常容易超过路由器、三层交换、甚至node的处理能力，从而限制了整个网络的扩张。\ncalico的每个node上会设置大量（海量)的iptables规则、路由，运维、排障难度大。\ncalico的原理决定了它不可能支持VPC，容器只能从calico设置的网段中获取ip。\ncalico目前的实现没有流量控制的功能，会出现少数容器抢占node多数带宽的情况。\ncalico的网络规模受到BGP网络规模的限制。\n名词解释  endpoint: 接入到calico网络中的网卡称为endpoint AS: 网络自治系统，通过BGP协议与其它AS网络交换路由信息 ibgp: AS内部的BGP Speaker，与同一个AS内部的ibgp、ebgp交换路由信息。 ebgp: AS边界的BGP Speaker，与同一个AS内部的ibgp、其它AS的ebgp交换路由信息。 workloadEndpoint: 虚拟机、容器使用的endpoint hostEndpoints: 物理机(node)的地址  calico在ip fabric中的部署方式 如果底层的网络是ip fabric的方式，三层网络是可靠的，只需要部署一套calico。\n剩下的关键点就是怎样设计BGP网络，calico over ip fabrics中给出两种设计方式: 1. AS per rack: 每个rack(机架)组成一个AS，每个rack的TOR交换机与核心交换机组成一个AS 2. AS per server: 每个node做为一个AS，TOR交换机组成一个transit AS\n这两种方式采用的是Use of BGP for routing in large-scale data centers中的建议。\nAS per rack  一个机架作为一个AS，分配一个AS号，node是ibgp，TOR交换机是ebgp node只与TOR交换机建立BGP连接，TOR交换机与机架上的所有node建立BGP连接 所有TOR交换机之间以node-to-node mesh方式建立BGP连接  TOR三层联通：\n每个机架上node的数目是有限的，BGP压力转移到了TOR交换机。当机架数很多，TOR交换机组成BGP mesh压力会过大。\nendpoints之间的通信过程:\nEndpointA发出报文 --\u0026gt; nodeA找到了下一跳地址nodeB --\u0026gt; 报文送到TOR交换机A --\u0026gt; 报文送到核心交换机 | v EndpointB收到了报文 \u0026lt;-- nodeB收到了报文 \u0026lt;-- TOR交换机B收到了报文 \u0026lt;-- 核心交换机将报文送达TOR交换机B  优化：“Downward Default model”减少需要记录的路由 Downward Default Model在上面的几种组网方式的基础上，优化了路由的管理。\n在上面的三种方式中，每个node、每个TOR交换机、每个核心交换机都需要记录全网路由。\n“Downward Default model”模式中: 1. 每个node向上(TOR)通告所有路由信息，而TOR向下(node)只通告一条默认路由 2. 每个TOR向上(核心交换机)通告所有路由，核心交换机向下(TOR)只通告一条默认路由 3. node只知晓本地的路由 4. TOR只知道接入到自己的所有node上的路由 5. 核心交换机知晓所有的路由\n这种模式减少了TOR交换机和node上的路由数量，但缺点是，发送到无效IP的流量必须到达核心交换机以后，才能被确定为无效。\nendpoints之间的通信过程:\nEndpointA发出报文 \u0026ndash;\u0026gt; nodeA默认路由到TOR交换机A \u0026ndash;\u0026gt; TOR交换机A默认路由到核心交换机 \u0026ndash;+ | v EndpointB收到了报文 \u0026lt;\u0026ndash; nodeB收到了报文 \u0026lt;\u0026ndash; TOR交换机B收到了报文 \u0026lt;\u0026ndash; 核心交换机找到了下一跳地址nodeB\ncalico验证 etcd 在节点上执行\ndocker run -d -p 2379:2379 -p 2380:2380 --name calico_etcd elcolio/etcd \\ -name etcd1 \\ -advertise-client-urls http://10.48.35.14:2379 \\ -listen-client-urls http://0.0.0.0:2379 \\ -initial-advertise-peer-urls http://10.48.35.14:2380 \\ -listen-peer-urls http://0.0.0.0:2380 \\ -initial-cluster-token etcd-cluster \\ -initial-cluster \u0026quot;etcd1=http://10.48.35.14:2380\u0026quot; \\ -initial-cluster-state new  查看集群状态\n# curl 10.48.35.14:2379/version etcd 2.0.10  设置多个节点共用相同的store\nvim /etc/docker/daemon.json { \u0026quot;cluster-store\u0026quot;:\u0026quot;etcd://10.48.35.14:2379\u0026quot;, }  重启docker服务\nsystemctl restart docker sudo service docker restart  etcd保存的calico信息 $ sudo etcdctl ls /calico /calico/v1 /calico/bgp /calico/ipam $ curl http://127.0.0.1:2379/v2/keys/ {\u0026quot;action\u0026quot;:\u0026quot;get\u0026quot;,\u0026quot;node\u0026quot;:{\u0026quot;dir\u0026quot;:true,\u0026quot;nodes\u0026quot;:[{\u0026quot;key\u0026quot;:\u0026quot;/docker\u0026quot;,\u0026quot;dir\u0026quot;:true,\u0026quot;modifiedIndex\u0026quot;:3,\u0026quot;createdIndex\u0026quot;:3},{\u0026quot;key\u0026quot;:\u0026quot;/calico\u0026quot;,\u0026quot;dir\u0026quot;:true,\u0026quot;modifiedIndex\u0026quot;:7,\u0026quot;createdIndex\u0026quot;:7}]}} $ curl 10.48.35.14:2379/v2/keys/calico/bgp/v1/host/k8s-/ip_addr_v4 | python -m json.tool % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 156 100 156 0 0 119k 0 --:--:-- --:--:-- --:--:-- 152k { \u0026quot;action\u0026quot;: \u0026quot;get\u0026quot;, \u0026quot;node\u0026quot;: { \u0026quot;createdIndex\u0026quot;: 74, \u0026quot;key\u0026quot;: \u0026quot;/calico/bgp/v1/host/k8s-/ip_addr_v4\u0026quot;, \u0026quot;modifiedIndex\u0026quot;: 74, \u0026quot;value\u0026quot;: \u0026quot;10.48.35.14\u0026quot; } } $ sudo etcdctl get /calico/bgp/v1/host/k8s-/ip_addr_v4 10.48.35.14  calico安装 直接下载预编译好的可执行文件就可以直接使用了\nsudo wget -O /bin/calicoctl https://github.com/projectcalico/calicoctl/releases/download/v1.6.1/calicoctl sudo chmod +x /bin/calicoctl curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.1.5/calicoctl chmod +x calicoctl mv calicoctl /user/local/bin/  下载 calico node 镜像\ndocker pull calico/node  启动calico服务: 在每台主机上均运行命令：\nsudo calicoctl node run --node-image=calico/node:v2.6.10 sudo calicoctl node run --ip=10.48.35.14 --name node01 --node-image quay.io/calico/node:v2.6.0 k8s-ep98 sudo calicoctl node run --ip=10.48.35.35 --name node02 --node-image quay.io/calico/node:v2.6.0  其中 calicoctl 命令里的 name，每台主机均可以根据自身 ip 来填写。 命令实际使用 calico/node 镜像启动了一个容器，执行输出内容如下：\nsudo calicoctl node run --name node01 --ip=10.48.32.5 Running command to load modules: modprobe -a xt_set ip6_tables Enabling IPv4 forwarding Enabling IPv6 forwarding Increasing conntrack limit Removing old calico-node container (if running). Running the following command to start calico-node: docker run --net=host --privileged --name=calico-node -d --restart=always -e IP=10.48.32.5 -e ETCD_ENDPOINTS=http://127.0.0.1:2379 -e NODENAME=node01 -e CALICO_NETWORKING_BACKEND=bird -e CALICO_LIBNETWORK_ENABLED=true -v /var/log/calico:/var/log/calico -v /var/run/calico:/var/run/calico -v /lib/modules:/lib/modules -v /run:/run -v /run/docker/plugins:/run/docker/plugins -v /var/run/docker.sock:/var/run/docker.sock quay.io/calico/node:latest Image may take a short time to download if it is not available locally. Container started, checking progress logs. 2019-03-03 05:40:10.424 [INFO][9] startup.go 256: Early log level set to info 2019-03-03 05:40:10.424 [INFO][9] startup.go 272: Using NODENAME environment for node name 2019-03-03 05:40:10.424 [INFO][9] startup.go 284: Determined node name: node01 2019-03-03 05:40:10.425 [INFO][9] startup.go 97: Skipping datastore connection test 2019-03-03 05:40:10.473 [INFO][9] startup.go 367: Building new node resource Name=\u0026quot;node01\u0026quot; 2019-03-03 05:40:10.473 [INFO][9] startup.go 382: Initialize BGP data 2019-03-03 05:40:10.473 [INFO][9] startup.go 476: Using IPv4 address from environment: IP=10.48.32.5 2019-03-03 05:40:10.476 [INFO][9] startup.go 509: IPv4 address 10.48.32.5 discovered on interface br0 2019-03-03 05:40:10.476 [INFO][9] startup.go 452: Node IPv4 changed, will check for conflicts 2019-03-03 05:40:10.477 [INFO][9] startup.go 647: No AS number configured on node resource, using global value 2019-03-03 05:40:10.477 [WARNING][9] startup_linux.go 47: Expected /var/lib/calico to be mounted into the container but it wasn't present. Node name may not be detected properly 2019-03-03 05:40:10.493 [INFO][9] startup.go 536: CALICO_IPV4POOL_NAT_OUTGOING is true (defaulted) through environment variable 2019-03-03 05:40:10.493 [INFO][9] startup.go 781: Ensure default IPv4 pool is created. IPIP mode: 2019-03-03 05:40:10.494 [INFO][9] startup.go 791: Created default IPv4 pool (192.168.0.0/16) with NAT outgoing true. IPIP mode: 2019-03-03 05:40:10.495 [INFO][9] startup.go 536: FELIX_IPV6SUPPORT is true (defaulted) through environment variable 2019-03-03 05:40:10.495 [INFO][9] startup_linux.go 79: IPv6 supported on this platform: true 2019-03-03 05:40:10.495 [INFO][9] startup.go 536: CALICO_IPV6POOL_NAT_OUTGOING is false (defaulted) through environment variable 2019-03-03 05:40:10.495 [INFO][9] startup.go 781: Ensure default IPv6 pool is created. IPIP mode: Never 2019-03-03 05:40:10.496 [INFO][9] startup.go 791: Created default IPv6 pool (fdb3:cdc5:a7b8::/48) with NAT outgoing false. IPIP mode: Never 2019-03-03 05:40:10.498 [INFO][9] startup.go 181: Using node name: node01 Starting libnetwork service Calico node started successfully  calico/node 中的进程\n$ sudo docker exec -it calico-node ps aux PID USER TIME COMMAND 1 root 0:00 /sbin/runsvdir -P /etc/service/enabled 198 root 0:00 runsv bird6 199 root 0:00 runsv confd 200 root 0:00 runsv bird 201 root 0:00 runsv felix 202 root 0:00 runsv libnetwork 203 root 0:00 svlogd /var/log/calico/confd 204 root 0:01 svlogd /var/log/calico/felix 205 root 0:00 svlogd -tt /var/log/calico/bird6 206 root 0:00 svlogd /var/log/calico/libnetwork 207 root 0:00 svlogd -tt /var/log/calico/bird 208 root 0:13 bird6 -R -s /var/run/calico/bird6.ctl -d -c /etc/calico/c 209 root 0:07 confd -confdir=/etc/calico/confd -interval=5 -watch --log 210 root 22:38 calico-felix 211 root 0:13 bird -R -s /var/run/calico/bird.ctl -d -c /etc/calico/con 212 root 0:05 libnetwork-plugin  创建calico网络 先创建两个网络：\n# sudo docker network create --driver calico --ipam-driver calico-ipam net1 # sudo docker network create --driver calico --ipam-driver calico-ipam net2  docker 创建网络的时候，会调用 calico 的网络驱动，由驱动完成具体的工作。注意这个网络是跨主机的，因此无论在哪台机器创建，在其他机器上都能看到：\n$ sudo docker network ls NETWORK ID NAME DRIVER SCOPE 0887c53db680 bridge bridge local 2e0533ba7fad host host local e8a0a439868f net1 calico global d8af5a88e634 net2 calico global 285e1438e0e7 none null local  创建calico网络的容器 然后分别在网络中运行容器：\nnode01:\nok sudo docker run -tid --name=test1 --net=net1 busybox sudo docker run --net net2 --name test2 -tid busybox sudo docker run -tid --name=test5 --net=net1 busybox  node02:\nsudo docker run -tid --name=test3 --net=net2 busybox sudo docker run --net net1 --name test4 -tid busybox  calico网络连通性 组网的拓扑如图\ntest1和test4都在net1中，可以连通。通过如下配置test1可以ping通test4。\n$ sudo docker exec -it test1 ping 192.168.105.195 -c 3 PING 192.168.105.195 (192.168.105.195): 56 data bytes 64 bytes from 192.168.105.195: seq=0 ttl=62 time=0.229 ms 64 bytes from 192.168.105.195: seq=1 ttl=62 time=0.274 ms 64 bytes from 192.168.105.195: seq=2 ttl=62 time=0.256 ms --- 192.168.105.195 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max = 0.229/0.253/0.274 ms  同一个网络 docker 会保存各自的名字和 IP 的对应关系，而不同网络的容器无法解析，而且不能相互通信。 test1与test2在不同的网络中，无法ping通。\n$ sudo docker exec -it test1 ping 192.168.246.196 -c 3 PING 192.168.246.196 (192.168.246.196): 56 data bytes --- 192.168.246.196 ping statistics --- 3 packets transmitted, 0 packets received, 100% packet loss  路由 node1 主机网卡\n$ ip addr show 16: cali6f588e66d6f@if15: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP link/ether 52:f7:01:67:89:90 brd ff:ff:ff:ff:ff:ff link-netnsid 2 18: cali3ea014c6cc8@if17: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP link/ether 02:2a:51:4f:26:c0 brd ff:ff:ff:ff:ff:ff link-netnsid 1  容器test1 网卡\n$ sudo docker exec test1 ip addr show 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 15: cali0@if16: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff inet 192.168.246.195/32 scope global cali0 valid_lft forever preferred_lft forever inet6 fe80::ecee:eeff:feee:eeee/64 scope link valid_lft forever preferred_lft forever   容器网卡cali0即15号网卡，对接host节点16号网卡； 也再次印证容器获取的是/32位主机地址； 注意容器网卡的mac地址”ee:ee:ee:ee:ee:ee”, 这是1个固定的特殊地址（所有calico生成的容器mac地址均一样），因为calico只关心三层的ip地址，而不关心二层mac地址  主机路由\n$ ip route default via 10.48.35.1 dev br0 10.48.35.0/24 dev br0 proto kernel scope link src 10.48.35.14 169.254.0.0/16 dev br0 scope link metric 1005 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 192.168.105.192/26 via 10.48.35.35 dev br0 proto bird blackhole 192.168.246.192/26 proto bird 192.168.246.195 dev cali6f588e66d6f scope link 192.168.246.196 dev cali3ea014c6cc8 scope link  calico默认在每个节点上创建一个 26 位掩码的子网，该子网可以有64个IP地址。在本机可以通过calico学习到到其他节点虚拟的路由，在本例中，如图第5条路由。\n最后两条路由是到本机容器的路由，发送到对应的网卡。\n第二条路由是到hulk容器的路由。\n容器test1 路由\n$ sudo docker exec test1 ip route default via 169.254.1.1 dev cali0 169.254.1.1 dev cali0 scope link   容器网关“169.254.1.1”是1个预留本地ip地址，通过cali0端口发送到网关； calico为简化网络配置,将容器的网关设置为1个固定的本地保留地址，容器内路由规则都是一样的，不需要动态更新； 确定下一跳后，容器会发送查询下一跳”169.254.1.1”的mac地址的ARP  这个 ARP 请求发到哪里了呢？要回答这个问题，就要知道 cali0 是 veth pair 的一端，其对端是主机上 caliXXXX 命名的 interface，可以通过 ethtool -S cali0 列出对端的 interface idnex。\n$ sudo docker exec test1 ip neigh show 169.254.1.1 dev cali0 lladdr 52:f7:01:67:89:90 used 0/0/0 probes 1 STALE  默认网卡 169.254.1.1 的mac地址是 cali0 对接网卡的mac地址，也就是host节点的16号网卡的mac地址。\n$ sudo tcpdump -i cali6f588e66d6f -e -nn 11:11:15.375778 ee:ee:ee:ee:ee:ee \u0026gt; 52:f7:01:67:89:90, ethertype ARP (0x0806), length 42: Request who-has 169.254.1.1 tell 192.168.246.195, length 28 11:11:15.375795 52:f7:01:67:89:90 \u0026gt; ee:ee:ee:ee:ee:ee, ethertype ARP (0x0806), length 42: Reply 169.254.1.1 is-at 52:f7:01:67:89:90, length 28  通过在host节点”cali6f588e66d6f”网卡上抓二层包发现，容器的”cali0”网卡（mac地址”ee:ee:ee:ee:ee:ee”）发出的request包，host节点的”cali6f588e66d6f”网卡直接以本地mac地址”52:f7:01:67:89:90”回复； 如果清除容器的arp表，可以更清晰的看到容器的arp请求报文被host节点对应的网卡响应\n换句话说，它把自己的 MAC 地址作为应答返回给容器。容器的后续报文 IP 地址还是目的容器，但是 MAC 地址就变成了主机上该 interface 的地址，也就是说所有的报文都会发给主机，然后主机根据 IP 地址进行转发。\n主机这个 interface 不管 ARP 请求的内容，直接用自己的 MAC 地址作为应答的行为被成为 ARP proxy，是 calico 开启的，可以通过下面的命令确认：\n$ cat /proc/sys/net/ipv4/conf/cali6f588e66d6f/proxy_arp 1  总的来说，可以认为 calico 把主机作为容器的默认网关来使用，所有的报文发到主机，然后主机根据路由表进行转发。和经典的网络架构不同的是，calico 并没有给默认网络配置一个 IP 地址（这样每个网络都会额外消耗一个 IP 资源，而且主机上也会增加对应的 IP 地址和路由信息），而是通过 arp proxy 和修改容器路由表来实现。\ncalico的使用 在calico中，IP被称为Endpoint，宿主机上的容器IP称为workloadEndpoint，物理机IP称为hostEndpoint。ipPool等一同被作为资源管理。\n查看默认的地址段:\n$ sudo calicoctl get ippool -o wide CIDR NAT IPIP 192.168.0.0/16 true false fd80:24e2:f998:72d6::/64 false false  会创建默认一个IP地址池为容器使用，这里用的是 192.168.0.0/16。\ncalico为每个宿主机的容器分配了一个网段 宿主机上每个容器都有一条对应的路由表项，下一跳是veth pair，\ncalico 并没有给默认网络配置一个 IP 地址（这样每个网络都会额外消耗一个 IP 资源，而且主机上也会增加对应的 IP 地址和路由信息）。 calico开启了arp proxy，这样每个宿主机端的veth pair都会回复容器对默认网关的arp请求。 主机这个 interface 不管 ARP 请求的内容，直接用自己的 MAC 地址作为应答的行为被成为 ARP proxy，是 calico 开启的，可以通过下面的命令确认：\n$ cat /proc/sys/net/ipv4/conf/calif24874aae57/proxy_arp 1  组件和架构 calico 做的事情： - 分配和管理 IP - 配置上容器的 veth pair 和容器内默认路由 - 根据集群网络情况实时更新节点上路由表\n从部署过程可以知道，除了 etcd 保存了数据之外，节点上也就只运行了一个 calico-node 的容器，所以推测是这个容器实现了上面所有的功能。calico/node 这个容器运行如下的进程\n[root@node00 ~]# docker exec -it calico-node sh / # ps aux PID USER TIME COMMAND 1 root 0:01 /sbin/runsvdir -P /etc/service/enabled 75 root 0:00 runsv felix 76 root 0:00 runsv bird 77 root 0:00 runsv bird6 78 root 0:00 runsv confd 79 root 0:00 runsv libnetwork 80 root 0:02 svlogd /var/log/calico/felix 81 root 30:49 calico-felix 82 root 0:00 svlogd /var/log/calico/confd 83 root 0:05 confd -confdir=/etc/calico/confd -interval=5 -watch --log-level=debug -node=http://172.17.8.100:2379 -client-key= -client-cert= -client-ca-keys= 84 root 0:00 svlogd -tt /var/log/calico/bird 85 root 0:20 bird -R -s /var/run/calico/bird.ctl -d -c /etc/calico/confd/config/bird.cfg 86 root 0:00 svlogd -tt /var/log/calico/bird6 87 root 0:18 bird6 -R -s /var/run/calico/bird6.ctl -d -c /etc/calico/confd/config/bird6.cfg 94 root 0:00 svlogd /var/log/calico/libnetwork 95 root 0:04 libnetwork-plugin  runsv 是一个 minimal 的 init 系统提供的命令，用来管理多个进程，可以看到它运行的进程包括：felix、bird、bird6、confd 和 libnetwork，这部分就介绍各个进程的功能。\nlibnetwork plugin libnetwork-plugin 是 calico 提供的 docker 网络插件，主要提供的是 IP 管理和网络管理的功能。\n默认情况下，当网络中出现第一个容器时，calico 会为容器所在的节点分配一段子网（子网掩码为 /26，比如192.168.196.128/26），后续出现在该节点上的容器都从这个子网中分配 IP 地址。这样做的好处是能够缩减节点上的路由表的规模，按照这种方式节点上 2^6 = 64 个 IP 地址只需要一个路由表项就行，而不是为每个 IP 单独创建一个路由表项。节点上创建的子网段可以在etcd 中 /calico/ipam/v2/host//ipv4/block/ 看到。\ncalico 还允许创建容器的时候指定 IP 地址，如果用户指定的 IP 地址不在节点分配的子网段中，calico 会专门为该地址添加一个 /32 的网段。\nBIRD BIRD（BIRD Internet Routing Daemon） 是一个常用的网络路由软件，支持很多路由协议（BGP、RIP、OSPF等），calico 用它在节点之间共享路由信息。\n关于 BIRD 如何配置 BGP 协议，可以参考官方文档，对应的配置文件在 /etc/calico/confd/config/ 目录。\nNOTE：至于为什么选择 BGP 协议而不是其他的路由协议，官网上也有介绍: Why BGP?\n默认所有的节点使用相同的 AS number 64512，因为 AS number 是一个32 比特的字段，所以有效取值范围是 [0-4294967295]，可以通过 calicoctl config get asNumber 命令查看当前节点使用的 AS number。\n默认情况下，每个 calico 节点会和集群中其他所有节点建立 BGP peer 连接，也就是说这是一个 O(n^2) 的增长趋势。在集群规模比较小的情况下，这种模式是可以接受的，但是当集群规模扩展到百个节点、甚至更多的时候，这样的连接数无疑会带来很大的负担。为了解决集群规模较大情况下 BGP client 连接数膨胀的问题，calico 引入了 RR（Router Reflector） 的功能。\nRR 的基本思想是选择一部分节点（一个或者多个）作为 Global BGP Peer，它们和所有的其他节点互联来交换路由信息，其他的节点只需要和 Global BGP Peer 相连就行，不需要之间再两两连接。更多的组网模式也是支持的，不管怎么组网，最核心的思想就是所有的节点能获取到整个集群的路由信息。\ncalico 对 BGP 的使用还是相对简单的，BGP 协议的原理不是一两句话能解释清楚的，以后有机会单独写篇文章来说吧。\nbird\nbird -R -s /var/run/calico/bird.ctl -d -c /etc/calico/confd/config/bird.cfg\n-R 选项指定启动后恢复 -s 指定通信的socket -d 指定 debug信息 -c 指定配置文件\nbird配置\n/ # cat /etc/calico/confd/config/bird.cfg # Generated by confd include \u0026quot;bird_aggr.cfg\u0026quot;; include \u0026quot;custom_filters.cfg\u0026quot;; include \u0026quot;bird_ipam.cfg\u0026quot;; router id 10.48.35.14; # Configure synchronization between routing tables and kernel. protocol kernel { learn; # Learn all alien routes from the kernel persist; # Don't remove routes on bird shutdown scan time 2; # Scan kernel routing table every 2 seconds import all; export filter calico_ipip; # Default is export none graceful restart; # Turn on graceful restart to reduce potential flaps in # routes when reloading BIRD configuration. With a full # automatic mesh, there is no way to prevent BGP from # flapping since multiple nodes update their BGP # configuration at the same time, GR is not guaranteed to # work correctly in this scenario. } # Watch interface up/down events. protocol device { debug { states }; scan time 2; # Scan interfaces every 2 seconds } protocol direct { debug { states }; interface -\u0026quot;cali*\u0026quot;, \u0026quot;*\u0026quot;; # Exclude cali* but include everything else. } # Template for all BGP clients template bgp bgp_template { debug { states }; description \u0026quot;Connection to BGP peer\u0026quot;; local as 64512; multihop; gateway recursive; # This should be the default, but just in case. import all; # Import all routes, since we don't know what the upstream # topology is and therefore have to trust the ToR/RR. export filter calico_pools; # Only want to export routes for workloads. next hop self; # Disable next hop processing and always advertise our # local address as nexthop source address 10.48.35.14; # The local address we use for the TCP connection add paths on; graceful restart; # See comment in kernel section about graceful restart. } # For peer /host/k8s-ep133./ip_addr_v4 protocol bgp Mesh_10_48_32_5 from bgp_template { neighbor 10.48.32.5 as 64512; } # For peer /host/k8s-ep98./ip_addr_v4 protocol bgp Mesh_10_48_35_35 from bgp_template { neighbor 10.48.35.35 as 64512; } # For peer /host/k8s-/ip_addr_v4 # Skipping ourselves (10.48.35.14)  confd 因为 bird 的配置文件会根据用户设置的变化而变化，因此需要一种动态的机制来实时维护配置文件并通知 bird 使用最新的配置，这就是 confd 的工作。confd 监听 etcd 的数据，用来更新 bird 的配置文件，并重新启动 bird 进程让它加载最新的配置文件。confd 的工作目录是 /etc/calico/confd，里面有三个目录：\nconf.d：confd 需要读取的配置文件，每个配置文件告诉 confd 模板文件在什么，最终生成的文件应该放在什么地方，更新时要执行哪些操作等 config：生成的配置文件最终放的目录 templates：模板文件，里面包括了很多变量占位符，最终会替换成 etcd 中具体的数据 具体的配置文件很多，我们只看一个例子：\n它会监听 etcd 的 /calico/bgp/v1 路径，一旦发现更新，就用其中的内容更新模板文件 bird.cfg.mesh.template，把新生成的文件放在 /etc/calico/confd/config/bird.cfg，文件改变之后还会运行 reload_cmd 指定的命令重启 bird 程序。\nNOTE：关于 confd 的使用和工作原理请参考它的官方 repo。\nfelix felix 负责最终网络相关的配置，也就是容器网络在 linux 上的配置工作，比如：\n更新节点上的路由表项 更新节点上的 iptables 表项 它的主要工作是从 etcd 中读取网络的配置，然后根据配置更新节点的路由和 iptables，felix 的代码在 github projectcalico/felix。\netcd etcd 已经在前面多次提到过，它是一个分布式的键值存储数据库，保存了 calico 网络元数据，用来协调 calico 网络多个节点。可以使用 etcdctl 命令行来读取 calico 在 etcd 中保存的数据：\netcdctl -C 172.17.8.100:2379 ls /calico /calico/ipam /calico/v1 /calico/bgp  每个目录保存的数据大致功能如下： - /calico/ipam：IP 地址分配管理，保存了节点上分配的各个子网段以及网段中 IP 地址的分配情况 - /calico/v1：profile 和 policy 的配置信息，节点上运行的容器 endpoint 信息（IP 地址、veth pair interface 的名字等）， - /calico/bgp：和 BGP 相关的信息，包括 mesh 是否开启，每个节点作为 gateway 通信的 IP 地址，AS number 等\n强大的防火墙功能 从前面的实验我们不仅知道了 calico 容器网络的报文流程是怎样的，还发现了一个事实：默认情况下，同一个网络的容器能通信（不管容器是不是在同一个主机上），不同网络的容器是无法通信的。\n这个行为是 calico 强大的防火墙实现的，默认情况下 calico 为每个网络创建一个 profile：\n$ sudo calicoctl get profile net1 -o yaml - apiVersion: v1 kind: profile metadata: name: net1 tags: - net1 spec: egress: - action: allow destination: {} source: {} ingress: - action: allow destination: {} source: tag: net1   profile 是和网络对应的，比如上面 metadata.name 的值是 net1，代表它匹配 net1网络，并应用到所有的 net1 网络容器中 calico 使用 label 来增加防火墙规则的灵活性，源地址和目的地址都可以通过 label 匹配 profile 中 metadata.tags 会应用到网络中所有的容器上 如果有定义，profile中的 metadata.labels 也会应用到网络中所有的容器上 spec 指定 profile 默认的网络规则，egress 没有限制，ingress 表示只运行 tag 为 net1 容器（也就是同一个网络的容器）的访问 每一个加入到网络的容器都会加上这个 profile，以此来实现网络之间的隔离。可以通过查看 endpoints 的详情得到它上面绑定的 profiles：\n$ sudo calicoctl get workloadEndpoint 6f588e66d6fef025fe7edb404ffb1684465b20f9454869089bcda08fbf5bc33e -o yaml - apiVersion: v1 kind: workloadEndpoint metadata: name: 6f588e66d6fef025fe7edb404ffb1684465b20f9454869089bcda08fbf5bc33e node: k8s- orchestrator: libnetwork workload: libnetwork spec: interfaceName: cali6f588e66d6f ipNetworks: - 192.168.246.195/32 mac: ee:ee:ee:ee:ee:ee profiles: - net1   iptables $ sudo iptables -L Chain INPUT (policy ACCEPT) target prot opt source destination cali-INPUT all -- anywhere anywhere /* cali:Cz_u1IQiXIMmKD4c */ KUBE-FIREWALL all -- anywhere anywhere Chain FORWARD (policy ACCEPT) target prot opt source destination cali-FORWARD all -- anywhere anywhere /* cali:wUHhoiAYhphO9Mso */ Chain OUTPUT (policy ACCEPT) target prot opt source destination cali-OUTPUT all -- anywhere anywhere /* cali:tVnHkvAo15HuiPy0 */ KUBE-FIREWALL all -- anywhere anywhere  用户也可以根据需求修改 profile 和 policy，可以参考官方教程。\n不过上面的防火墙都是针对网络的（网络中的容器的规则都是相同的），不能精细化到容器，也就是说只能做到网络之间的隔离和连通。不过 calico 也提供了对容器级别防火墙的支持，它主要是借助 docker 容器上的 label，通过匹配这些键值对来精细化控制防火墙。启动 docker label 支持需要在 calicoctl node run 命令运行时加上 \u0026ndash;use-docker-networking-container-labels 参数，而且一旦启用后原来的 profile 就被废弃不能用了（可以用纯 policy 实现原来的 profile 功能）。容器启动的时候需要添加上 label 用来作为 policy 的标识，比如 \u0026ndash;label org.projectcalico.label.role=frontend，具体的使用案例请参考这个教程。\n如果只要提供网络之间的隔离，可以使用 profile 和 policy；如果要实现精细化的容器之间的隔离，就需要启用容器的 label 功能了。在底层，calico 的 flelix 组件会实时跟踪 profile 和 policy 的内容，并更新各个节点的 iptables。\n总结 calico 的核心是通过维护路由规则实现容器的通信，路由信息的传播是 BIRD 软件通过 BGP 协议完成的，而节点上路由和防火墙规则是 felix 维护的。\n从 calico 本身的特性来说，它没有办法实现 VPC 网络，并且需要维护大量的路由表项和 iptables 表项，如果要部署在规模很大的生产环境中，需要预先规划系统的 iptables 和路由表项的上限。\n在我看来，calico 最大的优点有两个：直接三层互联的网络，不需要报文封装，因此性能更好而且能和原来的网络设施直接融合；强大的防火墙规则，利用 label 机制灵活地匹配容器，几乎可以设置任何需求的防火墙。\n但 calico 并非没有缺点，首先是它引入了 BGP 协议，虽然 bird 的配置很简单，但是运维这个系统需要熟悉 BGP 协议，这无疑会增加了人力、时间和金钱方面的投入；其次，calico 能支持的网络规模也有上限，虽然可以通过 Router Reflector 来缓解，但这么做又大大增加了网络规划、使用和排查的复杂度；最后 calico 无法用来实现 VPC 网络，IP 地址空间是所有租户共享的，租户之间是通过防火墙隔离的。\n针对flannel与calico网络方案做简单的对比 命令 创建/查看/更新/删除资源 分别使用creat/get/replace/delete来创建/查看/更新/删除资源。\n创建资源:\ncalicoctl create \u0026ndash;filename= [\u0026ndash;skip-exists] [\u0026ndash;config=] 资源使用yaml文件描述，可以创建以下资源:\nnode //物理机 bgpPeer //与本机建立了bgp连接的node hostEndpoint workloadEndpoint ipPool policy profile  查看资源:\ncalicoctl get ([--scope=\u0026lt;SCOPE\u0026gt;] [--node=\u0026lt;NODE\u0026gt;] [--orchestrator=\u0026lt;ORCH\u0026gt;] [--workload=\u0026lt;WORKLOAD\u0026gt;] (\u0026lt;KIND\u0026gt; [\u0026lt;NAME\u0026gt;]) | --filename=\u0026lt;FILENAME\u0026gt;) [--output=\u0026lt;OUTPUT\u0026gt;] [--config=\u0026lt;CONFIG\u0026gt;]  可以通过下面命令查看所有资源:\ncalicoctl get [资源类型］  例如:\ncalicoctl get node calicoctl delete node node01  IP地址管理 calicoctl ipam \u0026lt;command\u0026gt; [\u0026lt;args\u0026gt;...] release Release a calico assigned IP address. show Show details of a calico assigned IP address.  运行时设置 calicoctl config 获取和更改calico的配置项.\n日志 /var/log/calico/libnetwork/current\n参考资料 Calico网络的原理、组网方式与使用\nKubernetes Networking: Part 2 - Calico\ncalico: Frequently Asked Questions\n","date":1544140800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544140800,"objectID":"c492e2a2155a5aa6d3ca9ded5e9d4172","permalink":"/post/cloud/network/201812-calico/","publishdate":"2018-12-07T00:00:00Z","relpermalink":"/post/cloud/network/201812-calico/","section":"post","summary":"一种便捷强大的容器网络方案","tags":["Network"],"title":"Calico 架构与实践","type":"post"},{"authors":null,"categories":null,"content":" Kubernetes Kubernetes 是 Google 开源的容器集群管理系统，其提供应用部署、维护、 扩展机制等功能，利用 Kubernetes 能方便地管理跨机器运行容器化的应用，其主要功能如下：\n1) 使用 Docker 对应用程序包装 (package)、实例化 (instantiate)、运行 (run)。\n2) 以集群的方式运行、管理跨机器的容器。\n3) 解决 Docker 跨机器容器之间的通讯问题。\n4) Kubernetes 的自我修复机制使得容器集群总是运行在用户期望的状态。\nk8s 要做的不是dockerize，也不是containerize，而是作为一个集群操作系统，为此重新定义了可执行文件、进程、存储、网络的形态。\n整体结构 Master 集群控制节点，master节点上运行着一组关键的进程\n etcd，各个组件通信都并不是互相调用 API 来完成的，而是把状态写入 ETCD（相当于写入一个消息），其他组件通过监听 ETCD 的状态的的变化（相当于订阅消息），然后做后续的处理，然后再一次把更新的数据写入 ETCD。 api server，各个组件并不是直接访问 ETCD，而是访问一个代理，这个代理是通过标准的RESTFul API，重新封装了对 ETCD 接口调用，除此之外，这个代理还实现了一些附加功能，比如身份的认证、缓存等 Controller Manager 是实现任务调度的 Scheduler 是用来做资源调度的  Master 定义了 Kubernetes 集群 Master/API Server 的主要声明，包括 Pod Registry、Controller Registry、Service Registry、Endpoint Registry、Minion Registry、Binding Registry、RESTStorage 以及 Client, 是 client(Kubecfg) 调用 Kubernetes API，管理 Kubernetes 主要构件 Pods、Services、Minions、容器的入口。Master 由 API Server、Scheduler 以及 Registry 等组成。从下图可知 Master 的工作流主要分以下步骤：\n图片 - Master 主要构件及工作流\n1) Kubecfg 将特定的请求，比如创建 Pod，发送给 Kubernetes Client。\n2) Kubernetes Client 将请求发送给 API server。\n3) API Server 根据请求的类型，比如创建 Pod 时 storage 类型是 pods，然后依此选择何种 REST Storage API 对请求作出处理。\n4) REST Storage API 对的请求作相应的处理。\n5) 将处理的结果存入高可用键值存储系统 Etcd 中。\n6) 在 API Server 响应 Kubecfg 的请求后，Scheduler 会根据 Kubernetes Client 获取集群中运行 Pod 及 Minion 信息。\n7) 依据从 Kubernetes Client 获取的信息，Scheduler 将未分发的 Pod 分发到可用的 Minion 节点上。\nMinion Registry Minion Registry 负责跟踪 Kubernetes 集群中有多少 Minion(Host)。Kubernetes 封装 Minion Registry 成实现 Kubernetes API Server 的 RESTful API 接口 REST，通过这些 API，我们可以对 Minion Registry 做Create、Get、List、Delete 操作，由于 Minon 只能被创建或删除，所以不支持 Update 操作，并把 Minion 的相关配置信息存储到 etcd。除此之外，Scheduler 算法根据 Minion 的资源容量来确定是否将新建 Pod 分发到该 Minion 节点。\nPod Registry Pod Registry 负责跟踪 Kubernetes 集群中有多少 Pod 在运行，以及这些 Pod 跟 Minion 是如何的映射关系。将 Pod Registry 和 Cloud Provider 信息及其他相关信息封装成实现 Kubernetes API Server 的 RESTful API 接口 REST。通过这些 API，我们可以对 Pod 进行 Create、Get、List、Update、Delete 操作，并将 Pod 的信息存储到 etcd 中，而且可以通过 Watch 接口监视 Pod 的变化情况，比如一个 Pod 被新建、删除或者更新。\nService Registry Service Registry 负责跟踪 Kubernetes 集群中运行的所有服务。根据提供的 Cloud Provider 及 Minion Registry 信息把 Service Registry 封装成实现 Kubernetes API Server 需要的 RESTful API 接口 REST。利用这些接口，我们可以对 Service 进行 Create、Get、List、Update、Delete 操作，以及监视 Service 变化情况的 watch 操作，并把 Service 信息存储到 etcd。\nController Registry Controller Registry 负责跟踪 Kubernetes 集群中所有的 Replication Controller，Replication Controller 维护着指定数量的 pod 副本 (replicas) 拷贝，如果其中的一个容器死掉，Replication Controller 会自动启动一个新的容器，如果死掉的容器恢复，其会杀死多出的容器以保证指定的拷贝不变。通过封装 Controller Registry 为实现 Kubernetes API Server 的 RESTful API 接口 REST， 利用这些接口，我们可以对 Replication Controller 进行 Create、Get、List、Update、Delete 操作，以及监视 Replication Controller 变化情况的 watch 操作，并把 Replication Controller 信息存储到 etcd。\nEndpoints Registry Endpoints Registry 负责收集 Service 的 endpoint，比如 Name：\u0026rdquo;mysql\u0026rdquo;，Endpoints: [\u0026ldquo;10.10.1.1:1909\u0026rdquo;，\u0026rdquo;10.10.2.2:8834\u0026rdquo;]，同 Pod Registry，Controller Registry 也实现了 Kubernetes API Server 的 RESTful API 接口，可以做 Create、Get、List、Update、Delete 以及 watch 操作。\nBinding Registry Binding 包括一个需要绑定 Pod 的 ID 和 Pod 被绑定的 Host，Scheduler 写 Binding Registry 后，需绑定的 Pod 被绑定到一个 host。Binding Registry 也实现了 Kubernetes API Server 的 RESTful API 接口，但 Binding Registry 是一个 write-only 对象，所有只有 Create 操作可以使用， 否则会引起错误。\nScheduler Scheduler 收集和分析当前 Kubernetes 集群中所有 Minion 节点的资源 (内存、CPU) 负载情况，然后依此分发新建的 Pod 到 Kubernetes 集群中可用的节点。由于一旦 Minion 节点的资源被分配给 Pod，那这些资源就不能再分配给其他 Pod， 除非这些 Pod 被删除或者退出， 因此，Kubernetes 需要分析集群中所有 Minion 的资源使用情况，保证分发的工作负载不会超出当前该 Minion 节点的可用资源范围。具体来说，Scheduler 做以下工作：\n1) 实时监测 Kubernetes 集群中未分发的 Pod。\n2) 实时监测 Kubernetes 集群中所有运行的 Pod，Scheduler 需要根据这些 Pod 的资源状况安全地将未分发的 Pod 分发到指定的 Minion 节点上。\n3) Scheduler 也监测 Minion 节点信息，由于会频繁查找 Minion 节点，Scheduler 会缓存一份最新的信息在本地。\n4) 最后，Scheduler 在分发 Pod 到指定的 Minion 节点后，会把 Pod 相关的信息 Binding 写回 API Server。\nNode Node是工作主机，可以使物理主机、VM等。\n kubelet：负责管控docker容器，如启动/停止、监控运行状态等。 kube-proxy： 负责为pod提供代理。它会定期从etcd获取所有的service，并根据service信息创建代理。  kubelet 根据上图可知 Kubelet 是 Kubernetes 集群中每个 Minion 和 Master API Server 的连接点，Kubelet 运行在每个 Minion 上，是 Master API Server 和 Minion 之间的桥梁，接收 Master API Server 分配给它的 commands 和 work，与持久性键值存储 etcd、file、server 和 http 进行交互，读取配置信息。Kubelet 的主要工作是管理 Pod 和容器的生命周期，其包括 Docker Client、Root Directory、Pod Workers、Etcd Client、Cadvisor Client 以及 Health Checker 组件，具体工作如下：\n1) 通过 Worker 给 Pod 异步运行特定的 Action。\n2) 设置容器的环境变量。\n3) 给容器绑定 Volume。\n4) 给容器绑定 Port。\n5) 根据指定的 Pod 运行一个单一容器。\n6) 杀死容器。\n7) 给指定的 Pod 创建 network 容器。\n8) 删除 Pod 的所有容器。\n9) 同步 Pod 的状态。\n10) 从 Cadvisor 获取 container info、 pod info、root info、machine info。\n11) 检测 Pod 的容器健康状态信息。\n12) 在容器中运行命令。\nContainer Runtime（容器运行时） 每一个Node都会运行一个Container Runtime，其负责下载镜像和运行容器。Kubernetes本身并不停容器运行时环境，但提供了接口，可以插入所选择的容器运行时环境。kubelet使用Unix socket之上的gRPC框架与容器运行时进行通信，kubelet作为客户端，而CRI shim作为服务器。\nprotocol buffers API提供两个gRPC服务，ImageService和RuntimeService。ImageService提供拉取、查看、和移除镜像的RPC。RuntimeSerivce则提供管理Pods和容器生命周期管理的RPC，以及与容器进行交互(exec/attach/port-forward)。容器运行时能够同时管理镜像和容器（例如：Docker和Rkt），并且可以通过同一个套接字提供这两种服务。在Kubelet中，这个套接字通过–container-runtime-endpoint和–image-service-endpoint字段进行设置。Kubernetes CRI支持的容器运行时包括docker、rkt、cri-o、frankti、kata-containers和clear-containers等。\nProxy Proxy 是为了解决外部网络能够访问跨机器集群中容器提供的应用服务而设计的，从上图可知 Proxy 服务也运行在每个 Minion 上。Proxy 提供 TCP/UDP sockets 的 proxy，每创建一种 Service，Proxy 主要从 etcd 获取 Services 和 Endpoints 的配置信息，或者也可以从 file 获取，然后根据配置信息在 Minion 上启动一个 Proxy 的进程并监听相应的服务端口，当外部请求发生时，Proxy 会根据 Load Balancer 将请求分发到后端正确的容器处理。\n服务发现主要通过DNS实现。\n在Kubernetes中，kube proxy负责为Pod创建代理服务；引到访问至服务；并实现服务到Pod的路由和转发，以及通过应用的负载均衡。\nProxy不但解决了同一主宿机相同服务端口冲突的问题，还提供了Service转发服务端口对外提供服务的能力，Proxy后端使用了随机、轮循负载均衡算法。\n分层架构 Kubernetes设计理念和功能其实就是一个类似Linux的分层架构，如下图所示\n 核心层：Kubernetes最核心的功能，对外提供API构建高层的应用，对内提供插件式应用执行环境 应用层：部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS解析等） 管理层：系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态Provision等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy等） 接口层：kubectl命令行工具、客户端SDK以及集群联邦 生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴 Kubernetes外部：日志、监控、配置管理、CI、CD、Workflow、FaaS、OTS应用、ChatOps等 Kubernetes内部：CRI、CNI、CVI、镜像仓库、Cloud Provider、集群自身的配置和管理等  系统流程 一位大牛整理的 K8S 调用流程\n设计理念  声明式 VS 命令式, 声明式优点很多，一个很重要的点是：在分布式系统中，任何组件都可能随时出现故障。当组件恢复时，需要弄清楚要做什么，使用命令式 API 时，处理起来就很棘手。但是使用声明式 API ，组件只需查看 API 服务器的当前状态，即可确定它需要执行的操作。\n 显式的 API, Kubernetes 是透明的，它没有隐藏的内部 API。换句话说 Kubernetes 系统内部用来交互的 API 和我们用来与 Kubernetes 交互的 API 相同。这样做的好处是，当 Kubernetes 默认的组件无法满足我们的需求时，我们可以利用已有的 API 实现我们自定义的特性。\n 无侵入性, 感谢 Docker 容器技术的流行，使得 Kubernetes 为大家提供了无缝的使用方式。我们的应用达到镜像后, 不需要改动就可以遨游在 Kubernetes 集群中。Kubernetes 还提供存储 Secret、Configuration 等包含但不局限于密码、证书、容器镜像信息、应用启动参数能力。如此，Kubernetes 以一种友好的方式将这些东西注入 Pod，减少了大家的工作量，而无需重写或者很大幅度改变原有的应用代码。\n 为了实现这一目标，Kubernetes 引入了 PersistentVolumeClaim（PVC）和 PersistentVolume（PV）API 对象。这些对象将存储实现与存储使用分离。 PersistentVolumeClaim 对象用作用户以与实现无关的方式请求存储的方法，通过它来抹除对底层 PersistentVolume 的差异性。这样就使 Kubernetes 拥有了跨集群的移植能力。\n  容器编排系统的比较 在 Google 的一篇关于内部 Omega 调度系统的论文中，将调度系统分成三类：单体、二层调度和共享状态三种，按照它的分类方法，通常Google的 Borg被分到单体这一类，Mesos被当做二层调度，而Google自己的Omega被当做第三类“共享状态”。\n因为Kubernetes的大部分设计是延续 Borg的，而且Kubernetes的核心组件（Controller Manager和Scheduler）缺省也都是绑定部署在一起，状态也都是存储在ETCD里面的，所以通常大家会把Kubernetes也当做“单体”调度系统，我觉得 Kubernetes 的调度模型也完全是二层调度的，和 Mesos 一样，任务调度和资源的调度是完全分离的，Controller Manager承担任务调度的职责，而Scheduler则承担资源调度的职责。\nMesos 实际上Kubernetes和Mesos调度的最大区别在于资源调度请求的方式：\n主动 Push 方式。是 Mesos 采用的方式，就是 Mesos 的资源调度组件（Mesos Master）主动推送资源 Offer 给 Framework，Framework 不能主动请求资源，只能根据 Offer 的信息来决定接受或者拒绝。\n被动 Pull 方式。是 Kubernetes 的方式，资源调度组件 Scheduler 被动的响应 Controller Manager的资源请求。\n这两种方式带来的不同，我主要从一下 5 个方面来分析。另外注意，我所比较两者的优劣，都是从理论上做的分析，工程实现上会有差异，一些指标我也并没有实际测试过。\n1.资源利用率：Kubernetes 胜出\n理论上，Kubernetes 应该能实现更加高效的集群资源利用率，原因资源调度的职责完全是由Scheduler一个组件来完成的，它有充足的信息能够从全局来调配资源，然后而Mesos 却做不到，因为资源调度的职责被切分到Framework和Mesos Master两个组件上，Framework 在挑选 Offer 的时候，完全没有其他 Framework 工作负载的信息，所以也不可能做出最优的决策。\n举个例子，比如我们希望把对耗费 CPU的工作负载和耗费内存的工作负载尽可能调度到同一台主机上，在Mesos里面不太容易做到，因为他们分属不同的 Framework。\n2.扩展性：Mesos胜出\n从理论上讲，Mesos 的扩展性要更好一点。原因是Mesos的资源调度方式更容易让已经存在的任务调度迁移上来。举个例子，假设已经有了一个任务调度系统，比如 Spark，现在要迁移到集群调度平台上，理论上它迁移到 Mesos 比 Kubernetes 上更加容易。\n如果迁移到 Mesos ，没有改变原来的工作流程和逻辑，原来的逻辑是：来了一个作业请求，调度系统把任务拆分成小的任务，然后从资源池里面挑选一个节点来运行任务，并且记录挑选的节点 IP 和端口号，用来跟踪任务的状态。迁移到 Mesos 之后，还是一样的逻辑，唯一需要变化的是那个资源池，原来是自己管理的资源池，现在变成 Mesos 提供的Offer 列表。\n如果迁移到 Kubernetes，则需要修改原来的基本逻辑来适配 Kubernetes，资源的调度完全需要调用外部的组件来完成，并且这个变成异步的。\n3.灵活的任务调度策略：Mesos 胜出\nMesos 对各种任务的调度策略也支持的更好。举个例子，如果某一个作业，需要 All or Nothing 的策略，Mesos 是能够实现的，但是 Kubernetes 完全无法支持。所以All or Nothing 的意思是，价格整个作业如果需要运行 10 个任务，这 10个任务需要能够全部有资源开始执行，否则就一个都不执行。\n4.性能：Mesos 胜出\nMesos 的性能应该更好，因为资源调度组件，也就是 Mesos Master 把一部分资源调度的工作甩给 Framework了，承担的调度工作更加简单，从数据来看也是这样，在多年之前 Twitter 自己的 Mesos 集群就能够管理超过 8万个节点，而 Kubernetes 1.3 只能支持 5千个节点。\n5.调度延迟：Kubernetes 胜出\nKubernetes调度延迟会更好。因为Mesos的轮流给Framework提供Offer机制，导致会浪费很多时间在给不需要资源的 Framework 提供Offer。\nSwarm Kubernetes的优势 Kubernetes 作为容器编排的事实标准，主要强在以下方面：\n 理念的先进性 Kubernetes 并未将 Docker 作为架构的核心，而仅仅将它作为一个container runtime 实现，k8s的核心是cni、csi、cri、oci等。相对的，mesos是docker的使用者，也必然是docker特性的迁就者。K8S 架构有很强的扩展性，而Mesos则需要考虑 Docker 的支持程度。\n 声明式 API k8s系统的梳理了任务的形态以及任务之间的关系，并为未来留有余地，提供了声明式的 API。\n  云计算平台上的开发者们所关心的，并不是调度，也不是资源管管理，更不是网络或者存储，他们关心的只有一件事，那就是 Kubernetes 的 API，也就是声明式 API 和控制器模式。这个 API 独有的编程范式，即 Controller 和 Operator。作为一个云计算平台的用户，能够用一个 YAML 文件表达我开发的应用的最终运行状态，并且自动地对我的应用进行运维和管理。这种信赖关系，就是连接Kubernetes 项目和开发者们最重要的纽带。\n不同于一个只能生产资源的集群管理工具，Kubernetes 项目最大的价值，乃在于它从一开始就提倡的声明式 API 和以此为基础“控制器”模式。Kubernetes 项目为使用者提供了宝贵的 API 可扩展能力和良好的 API 编程范式，催生出了一个完全基于 Kubernetes API 构建出来的上层应用服务生态。可以说，正是这个生态的逐步完善与日趋成熟，才确立了 Kubernetes 项目如今在云平台领域牢不可破的领导地位，也间接宣告了竞品方案的边缘化。\n未来：应用交付的革命不会停止，Kubernetes 项目一直在做的，其实是在进一步清晰和明确“应用交付”这个亘古不变的话题。只不过，相比于交付一个容器和容器镜像， Kubernetes 项目正在尝试明确的定义云时代“应用”的概念。在这里，应用是一组容器的有机组合，同时也包括了应用运行所需的网络、存储的需求的描述。而像这样一个“描述”应用的 YAML 文件，放在 etcd 里存起来，然后通过控制器模型驱动整个基础设施的状态不断地向用户声明的状态逼近，就是 Kubernetes 的核心工作原理了。PS: 以后你给公有云一个yaml 文件就可以发布自己的应用了。\n横向扩展 几乎所有的集群调度系统都无法横向扩展（Scale Out），Mesos通过优化，一个集群能够管理 8 万个节点，Kubernetes 最新的1.15版本，集群管理节点的上限是 5000 个节点。\n所有的集群调度系统的架构都是无法横向扩展的，如果需要管理更多的服务器，唯一的办法就是创建多个集群。集群调度系统的架构看起来都是这个样子的：\n中间的 Scheduler（资源调度器）是最核心的组件，虽然通常是由多个（通常是3个）实例组成，但是都是单活的，也就是说只有一个节点工作，其他节点都处于 Standby 的状态。\n这是因为集群调度系统的“独立资源池”数量是 1，每一台服务器节点都是一个资源，每当资源消费者请求资源的时候，调度系统用来做调度算法的“独立资源池”是多大呢？答案应该是整个集群的资源组成的资源池，没有办法在切分了，因为:\n 调度系统的职责就是要在全局内找到最优的资源匹配。 另外，哪怕不需要找到最优的资源匹配，资源调度器对每一次资源请求，也没办法判断应该从哪一部分资源池中挑选资源。  正是因为这个原因，“独立资源池”数量是 1，所以集群调度系统无法做到横向扩展。\n优化 主要是 Scheduler 调度器的优化，主要体现在两个地方：\n 预选失败中断机制  一次调度过程在判断一个Node是否可作为目标机器主要分为三个阶段：\n预选阶段：硬性条件，过滤掉不满足条件的节点，这个过程称为Predicates。这是固定先后顺序的一系列过滤条件，任何一个predicate不符合则放弃该Node。\n优选阶段：软性条件，对通过的节点按照优先级排序，称之为Priorities。每一个priority都是一个影响因素，都有一定的权重。\n选定阶段：从优选列表中选择优先级最高的节点，称为Select。选择的Node即为最终部署Pod的机器。\n通过深入分析调度过程我们发现，调度器在预选阶段即使已经知道当前Node不符合某个过滤条件仍然会继续判断后续的过滤条件是否符合。试想如果有上万台Node节点，这些判断逻辑会浪费很多计算时间，也是调度器性能低下的一个重要因素。\n改进为“预选失败中断机制”，即一旦某个预选条件不满足，那么该Node即被立即放弃，后面的预选条件不再做判断计算，从而大大减少了计算量，调度性能也大大提升。如下图：\n 局部最优解  对于优化问题，尤其是最优化问题，总是希望找到全局最优的解或策略，但是当问题的复杂度过于高，要考虑的因素和处理的信息量过多的时候，我们往往会倾向于接受局部最优解，因为局部最优解的质量不一定都是差的。尤其是当我们有确定的评判标准标明得出的解是可以接受的话，通常会接收局部最优的结果。这样，从成本、效率等多方面考虑，才是实际工程中会采取的策略。\n当前调度策略中，每次调度调度器都会遍历集群中所有的Node，以便找出最优的节点，这在调度领域我们称之为BestFit算法，但是在生产环境中，我们是选取最优Node还是次优的Node其实并没有特别大的区别和影响，有时候我们还是避免每次选取最优的Node(例如我们集群为了解决新上线机器后狂在该机器上创建应用的问题就将最优解随机化)。换句话说，我们找出局部最优解就能满足我们的需求。\n假设集群一共1000个Node，一次调度过程PodA，这其中有700个Node都能通过Predicates(预选阶段)，那么就会把所有的Node遍历并找出这700个node，然后经过得分排序找出最优的Node节点NodeX；但是采用局部最优算法，即我们认为只要能找出N个Node，并在这N个Node中选择得分最高的Node即能满足我们的需求，比如我们默认找出100个可以通过Predicates(预选阶段)的Node即可，我们的最优解就在这100个Node中选择，当然全局最优解NodeX可能在也可能不在这100个Node中，但是我们在这100个Node中选择最优的NodeY也能满足我们的要求。最好的情况下我们在遍历100个Node就找出了这100个Node，也可能遍历了200个或者300个Node等等，这样我们可以大大减少计算时间，同时也不会对我们的调度结果产生太大的影响。\n参考 K8S 设计理念\nKubernetes整体结构\n","date":1542672000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542672000,"objectID":"04020d602faf16fa8cc35d08ca7b357c","permalink":"/post/cloud/k8s/201811-k8s-arch/","publishdate":"2018-11-20T00:00:00Z","relpermalink":"/post/cloud/k8s/201811-k8s-arch/","section":"post","summary":"Kubernetes 云计算操作系统","tags":["Kubernetes"],"title":"Kubernetes 框架","type":"post"},{"authors":null,"categories":null,"content":"支持一下功能：\n 照镜子 语音交互 天气预报 室内温湿度 新闻资讯 股票信息 支持定制更多功能  ","date":1535328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535328000,"objectID":"4271ecf2381076959da698bfcf06917c","permalink":"/project/mirror-project/","publishdate":"2018-08-27T00:00:00Z","relpermalink":"/project/mirror-project/","section":"project","summary":"这是一面高颜值、高智商、会说话的镜子","tags":["MagicMirror"],"title":"魔镜","type":"project"},{"authors":null,"categories":null,"content":"周期性获取火币网的多种交易对行情，可以方便的新增交易对、自定义行情周期，获取的结果存储到 MySQL 中。\n","date":1532649600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532649600,"objectID":"15169f7bb87fa24153ed41a267828478","permalink":"/project/blockchain-project/","publishdate":"2018-07-27T00:00:00Z","relpermalink":"/project/blockchain-project/","section":"project","summary":"获取火币网交易对行情并存储到数据库中。","tags":["BlockChain"],"title":"数字币交易所行情","type":"project"},{"authors":null,"categories":null,"content":" 最近在找对象，突发奇想做个基于区块链的相亲合约。\n听说有个程序员在婚恋网站上挂了1个多月都没人搭理他，然后给自己加了个 区块链工程师 的标签，几天后收到大量私信😝\n传统相亲平台的痛点  中介所收费贵，介绍的还不靠谱 百合网、珍爱网之流坑爹，看个消息要钱，还都是机器人发的 虚假信息多，各种酒拖 效率低  区块链相亲的优势 LoveChain可以解决三大婚恋社交问题，分别是安全性、激励性、产业生态。\n安全性 安全性问题又分为三类：用户身份的真实性、网站登记信息的私密性、以及线下实地见面的安全性。\n针对真实用户，运用 AI、大数据、生物识别认证等技术对用户的合法身份进行有效认证，结合百合佳缘积累的社交黑名单库，对可疑的用户进行拦截和清除。同时，反垃圾监测系统可以将可疑用户进行账号隔离，并对其发出的垃圾信息进行单边隐藏，保护合法的用户利益。\n针对用户敏感信息私密性的保护，其中提及一旦成为平台认证的合法用户，会有一个虚拟用户钱包，钱包由用户自己设置账号和密码， 只要把密钥安全存放，任何人都无法获取用户账户内的虚拟资产。用 户可以用自己的公私钥进行对自己的虚拟资产进行消费交易。\n针对线下活动，采用智能合约，质押数字资产存放于平台，只有利益相关方的密钥才可以转移价值，其它人无权转移。当有人违约时，按照智能合约的约定，质押数字资产会自动转移到守约者账户。当各方都遵循合约约定时，合约 履行完毕，质押数字资产自动退到质押方各自账户。\n线下智能约会场景\n激励性 由于婚恋社交的用户粘性比较低，因此可以通过代币奖励来提高积极性，比如通过注册账户、上传照片、主动发消息、身份认证、朋友圈发帖等方式获得激励点。这些代币平台可以应用于多种业务，包括情感咨询、婚礼婚庆甚至婚房等。\n产业生态 可以预见，未来的婚恋问题将向低结婚欲望、大龄未婚等发现发展，这方面的产业生态将发生改变，需要提前去布局。\n实现 基于以太坊，做了一个简单的demo\n请前往airdropfans区块恋体验。\n","date":1530576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530576000,"objectID":"00f7958e45e47773384e7dacb2299df3","permalink":"/post/blockchain/201807-contract-love/","publishdate":"2018-07-03T00:00:00Z","relpermalink":"/post/blockchain/201807-contract-love/","section":"post","summary":"免费发布和获取对方信息的靠谱婚恋平台","tags":["区块链","智能合约"],"title":"基于区块链智能合约的相亲","type":"post"},{"authors":null,"categories":null,"content":" 如果要问智能合约最适合做什么应用，还有比彩票（赌博）更好的答案吗？ 恐怕没有\n传统彩票的痛点  不透明，存在信任危机 效率低，为了小概率中奖而花费很多精力关注开奖日期，错失兑奖时间  区块链彩票的优势 通过区块链技术将传统的纸质彩票进行电子化，利用区块链技术的不可篡改性解决电子化彩票号码可以被篡改的问题。\n每家彩票站作为区块链的一个节点，将卖出的彩票写入区块链中，不可篡改。\n通过智能合约打造一个智能电子彩票交易系统\n当购买彩票的人确定好号码，下注的数量后。该智能交易系统会自动生成智能合约将购买的彩票号码，下注的数量，彩票的价格和中奖的赔率等条件写入智能合约。再者就是彩票公司的数字货币钱包和购彩者的数字货币钱包一并写入智能合约中，交由计算机自动执行。\n当购彩者确认购买后，智能合约自动从购彩者的数字货币钱包将彩票的购买费用转到彩票公司。当该期彩票开奖时，智能合约能够自动根据开奖的号码和每一位购彩者所购的号码来进行判断中奖的情况。如果有中奖，智能合约就能够根据中奖的金额，自动将奖金从彩票公司的数字货币钱包转到中奖者的数字货币钱包，整个过程不需要人为的参与，全程由计算机自动执行。\n这样一来，对于购买彩票的人来说，他们买完后就不需要花较大的精力去关注自己是否中奖，也不需要当心自己的中奖彩票不能及时兑奖而过期，因为智能合约能够自动进行兑奖。对于彩票发行公司来说，有了这个系统后，不仅不需要大量的人来进行兑奖工作，在减少人工成本的同时，很大程度的提高了工作的效率。甚至不需要实体的店面来经营彩票，大大降低了彩票经营管理的成本，提高利润和市场竞争力。\n实现 基于以太坊，做了一个彩票游戏。\n请前往airdropfans押宝游戏体验。\n","date":1528761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528761600,"objectID":"e4e8feaf3be61c36fe4abfeca66f735f","permalink":"/post/blockchain/201806-contract-casino/","publishdate":"2018-06-12T00:00:00Z","relpermalink":"/post/blockchain/201806-contract-casino/","section":"post","summary":"去中心化的、公平的彩票系统","tags":["区块链","智能合约"],"title":"基于区块链智能合约的彩票","type":"post"},{"authors":null,"categories":null,"content":" 目前各种电商、实体店都有很多优惠券发放，但是人们并没有充分利用起来，如果可以把优惠券在智能合约上流通起来，那商家实现了广告的目的，也得到了客流量，而顾客也能得到优惠。这个去中心化的业务场景可以用区块链智能合约实现。\n重新定义  重新定义优惠券的流通方式 重新定义公正透明的拍卖方式  智能合约具有如下特点： - 一种传播、验证或执行合同的协议 - 不需要第三方的可信交易 - 可追踪 - 不可逆转\n智能合约保证了拍卖的公正透明，无需人工参与\n架构设计 发起拍卖的流程\n参与竞拍\n","date":1525478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525478400,"objectID":"febf6d7b2661510ded1b03f3e35dc619","permalink":"/post/blockchain/201805-contract-coupon/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/post/blockchain/201805-contract-coupon/","section":"post","summary":"去中心化的优惠券交易系统","tags":["区块链","智能合约"],"title":"基于区块链智能合约的优惠券","type":"post"}]