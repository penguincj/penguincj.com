[{"authors":["admin"],"categories":null,"content":"专注于基础架构，Cloud Native 拥护者，希望成为坚守开发一线打磨匠艺的架构师。\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"专注于基础架构，Cloud Native 拥护者，希望成为坚守开发一线打磨匠艺的架构师。","tags":null,"title":"L CJ","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"d9975920f3fb24c07bdc68e5a0f70299","permalink":"/istio/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/istio/example/","section":"istio","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"2d3419a1f0960318ce200b192e83a046","permalink":"/istio/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/istio/example/example1/","section":"istio","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"46c1c50a9cb1a6cc9bdb431c8244fc44","permalink":"/istio/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/istio/example/example2/","section":"istio","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":null,"categories":null,"content":" Kubernetes 本身不提供容器网络, 但是实现了一套支持多种网络插件的框架代码, 通过调用网络插件来为容器设置网络环境。\n而约束网络插件的是 CNI（Container Network Interface），一种标准的容器网络接口，定义了如何将容器加入网络和将容器从网络中删除。\nCNI 接口由 runtime 在创建容器和删除容器时调用。具体的接口定义如下：\n// vendor/github.com/containernetworking/cni/libcni/api.go type CNI interface { AddNetworkList(net *NetworkConfigList, rt *RuntimeConf) (types.Result, error) DelNetworkList(net *NetworkConfigList, rt *RuntimeConf) error AddNetwork(net *NetworkConfig, rt *RuntimeConf) (types.Result, error) DelNetwork(net *NetworkConfig, rt *RuntimeConf) error }  Kubernetes plugin 接口 kubelet 是通过 NetworkPlugin interface 来调用底层的网络插件为容器设置网络环境.\n// kubelet/dockershim/network/plugins.go // Plugin is an interface to network plugins for the kubelet type NetworkPlugin interface { // Init initializes the plugin. This will be called exactly once // before any other methods are called. Init(host Host, hairpinMode kubeletconfig.HairpinMode, nonMasqueradeCIDR string, mtu int) error // Called on various events like: // NET_PLUGIN_EVENT_POD_CIDR_CHANGE Event(name string, details map[string]interface{}) // Name returns the plugin's name. This will be used when searching // for a plugin by name, e.g. Name() string // Returns a set of NET_PLUGIN_CAPABILITY_* Capabilities() utilsets.Int // SetUpPod is the method called after the infra container of // the pod has been created but before the other containers of the // pod are launched. SetUpPod(namespace string, name string, podSandboxID kubecontainer.ContainerID, annotations, options map[string]string) error // TearDownPod is the method called before a pod's infra container will be deleted TearDownPod(namespace string, name string, podSandboxID kubecontainer.ContainerID) error // GetPodNetworkStatus is the method called to obtain the ipv4 or ipv6 addresses of the container GetPodNetworkStatus(namespace string, name string, podSandboxID kubecontainer.ContainerID) (*PodNetworkStatus, error) // Status returns error if the network plugin is in error state Status() error }  实现了 NetworkPlugin interface 就可以新增一种 Kubernetes 的 Network plugin。这个 interface 也并没有具体容器网络的实现，而是做了一层封装，具体的容器网络由独立的二进制实现，比如官方提供的 bridge、host-local 或者第三方的 calico、flannel 等，也可以是自己定制的实现。\nK8S 支持两种 plugin：\n cniNetworkPlugin kubenetNetworkPlugin  下面讲述 plugin 是如何初始化和工作的\nkubelet 启动 kubelet 启动后会调用 run() 进入处理流程，在进入主处理流程之前的初始化阶段会根据用户配置的网络插件名选择对应的网络插件。\n// cmd/kubelet/app/server.go func run(s *options.KubeletServer, kubeDeps *kubelet.KubeletDeps) (err error) { ... // 创建 kubelete // 根据 kubelet 的运行参数运行 kubelet // 这里会根据用户配置的网络插件名选择网络插件 if err := RunKubelet(\u0026amp;s.KubeletConfiguration, kubeDeps, s.RunOnce, standaloneMode); err != nil { return err } ... }  // cmd/kubelet/app/server.go func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error { ... k, err := CreateAndInitKubelet(\u0026amp;kubeServer.KubeletConfiguration, kubeDeps, ...) ... }  func CreateAndInitKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, ...) { k, err = kubelet.NewMainKubelet(kubeCfg, kubeDeps, crOptions ... ) k.BirthCry() k.StartGarbageCollection() }  // NewMainKubelet instantiates a new Kubelet object along with all the required internal modules. // No initialization of Kubelet and its modules should happen here. func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies ...) { klet := \u0026amp;Kubelet{ hostname: hostname, hostnameOverridden: len(hostnameOverride) \u0026gt; 0, ... } switch containerRuntime { case kubetypes.DockerContainerRuntime: // Create and start the CRI shim running as a grpc server. streamingConfig := getStreamingConfig(kubeCfg, kubeDeps, crOptions) ds, err := dockershim.NewDockerService(kubeDeps.DockerClientConfig, crOptions.PodSandboxImage, streamingConfig, \u0026amp;pluginSettings, runtimeCgroups, kubeCfg.CgroupDriver, crOptions.DockershimRootDirectory, !crOptions.RedirectContainerStreaming) ... server := dockerremote.NewDockerServer(remoteRuntimeEndpoint, ds) if err := server.Start(); err != nil { return nil, err } case kubetypes.RemoteContainerRuntime: // No-op. break default: return nil, fmt.Errorf(\u0026quot;unsupported CRI runtime: %q\u0026quot;, containerRuntime) } // 向 k8s network plugin 通报 NET_PLUGIN_EVENT_POD_CIDR_CHANGE 事件 // 该事件将会被 NetworkPlugin interface 的 Event 方法捕获 if _, err := klet.updatePodCIDR(kubeCfg.PodCIDR); err != nil { klog.Errorf(\u0026quot;Pod CIDR update failed %v\u0026quot;, err) } ... }  目前只支持 CRI 为 docker。\n根据用户配置选择 CNI // pkg/kubelet/dockershim/docker_service.go // NOTE: Anything passed to DockerService should be eventually handled in another way when we switch to running the shim as a different process. func NewDockerService(config *ClientConfig, podSandboxImage string, ...) (DockerService, error) { ds := \u0026amp;dockerService{ client: c, os: kubecontainer.RealOS{}, podSandboxImage: podSandboxImage, streamingRuntime: \u0026amp;streamingRuntime{ client: client, execHandler: \u0026amp;NativeExecHandler{}, }, containerManager: cm.NewContainerManager(cgroupsName, client), checkpointManager: checkpointManager, startLocalStreamingServer: startLocalStreamingServer, networkReady: make(map[string]bool), } // Determine the hairpin mode. if err := effectiveHairpinMode(pluginSettings); err != nil { // This is a non-recoverable error. Returning it up the callstack will just // lead to retries of the same failure, so just fail hard. return nil, err } // 根据配置配置 CNI // dockershim currently only supports CNI plugins. pluginSettings.PluginBinDirs = cni.SplitDirs(pluginSettings.PluginBinDirString) cniPlugins := cni.ProbeNetworkPlugins(pluginSettings.PluginConfDir, pluginSettings.PluginBinDirs) // 加了一个默认的 CNI 插件 kubenet cniPlugins = append(cniPlugins, kubenet.NewPlugin(pluginSettings.PluginBinDirs)) netHost := \u0026amp;dockerNetworkHost{ \u0026amp;namespaceGetter{ds}, \u0026amp;portMappingGetter{ds}, } // 根据用户配置选择对应的网络插件对象，做 init() 初始化 plug, err := network.InitNetworkPlugin(cniPlugins, pluginSettings.PluginName, netHost, pluginSettings.HairpinMode, pluginSettings.NonMasqueradeCIDR, pluginSettings.MTU) ds.network = network.NewPluginManager(plug) klog.Infof(\u0026quot;Docker cri networking managed by %v\u0026quot;, plug.Name()) return ds, nil }  Hairpin 模式\n发夹式转发模式 (Hairpin mode)又称反射式转发模式 (Reflective Relay) ，指交换机可以将报文的接受端口同时作为发送端口, 即报文可以从它的入端口转发出去, 如下图所示:\nNewDockerService() 函数首先通过 effectiveHairpinMode() 计算出有效的 Hairpin 模式, 然后根据 NetworkPluginName 从插件列表中选择对应的网络插件对象.\nProbeNetworkPlugins() 根据配置的 CNI 插件的路径生成 network.NetworkPlugin interface 的实现 cniNetworkPlugin。\nInitNetworkPlugin() 负责从网络插件对象列表中根据用户配置的网络插件名选择对应的网络插件对象，调用插件的 init() 执行初始化。\n// pkg/kubelet/dockershim/network/plugins.go // InitNetworkPlugin inits the plugin that matches networkPluginName. Plugins must have unique names. func InitNetworkPlugin(plugins []NetworkPlugin, networkPluginName string, host Host, hairpinMode kubeletconfig.HairpinMode, nonMasqueradeCIDR string, mtu int) (NetworkPlugin, error) { // 如果用户没有配置网络插件名, 默认就是NoopNetworkPlugin, 不会提供任何容器网络 // NoopNetworkPlugin 是 NetworkPlugin interface 的实现 if networkPluginName == \u0026quot;\u0026quot; { // default to the no_op plugin plug := \u0026amp;NoopNetworkPlugin{} plug.Sysctl = utilsysctl.New() if err := plug.Init(host, hairpinMode, nonMasqueradeCIDR, mtu); err != nil { return nil, err } return plug, nil } ... chosenPlugin := pluginMap[networkPluginName] if chosenPlugin != nil { // 执行插件的初始化操作 err := chosenPlugin.Init(host, hairpinMode, nonMasqueradeCIDR, mtu) } ... }  通告 Pod CIDR 的更新 k8s 对 Pod 的管理是通过 runtime 来操作的，因此对 CIDR 的更新也是通过 runtime 实现。当 Pod 的 CIDR 更新时调用 runtime 的 UpdatePodCIDR()。\n// pkg/kubelet/kubelet_network.go // updatePodCIDR updates the pod CIDR in the runtime state if it is different // from the current CIDR. Return true if pod CIDR is actually changed. func (kl *Kubelet) updatePodCIDR(cidr string) (bool, error) { // 配置与当前状态比较，没有变化直接返回 podCIDR := kl.runtimeState.podCIDR() if podCIDR == cidr { return false, nil } // kubelet -\u0026gt; generic runtime -\u0026gt; runtime shim -\u0026gt; network plugin // docker/non-cri implementations have a passthrough UpdatePodCIDR if err := kl.getRuntime().UpdatePodCIDR(cidr); err != nil { // If updatePodCIDR would fail, theoretically pod CIDR could not change. // But it is better to be on the safe side to still return true here. return true, fmt.Errorf(\u0026quot;failed to update pod CIDR: %v\u0026quot;, err) } // 更新当前状态，以便以后比较 kl.runtimeState.setPodCIDR(cidr) return true, nil }  runtime 要先讲下 k8s runtime 的管理。\nk8s 通过 kubeGenericRuntimeManager 来做统一的 RC 管理，该类会调用对应的 RC shim 来做下发操作。\nkubelet 的 containerRuntime 是在 NewMainKubelet() 函数中如下代码片段配置的。\nruntime, err := kuberuntime.NewKubeGenericRuntimeManager( kubecontainer.FilterEventRecorder(kubeDeps.Recorder), ...) klet.containerRuntime = runtime klet.streamingRuntime = runtime klet.runner = runtime  // kuberuntime/kuberuntime_manager.go // UpdatePodCIDR is just a passthrough method to update the runtimeConfig of the shim // with the podCIDR supplied by the kubelet. func (m *kubeGenericRuntimeManager) UpdatePodCIDR(podCIDR string) error { // TODO(#35531): do we really want to write a method on this manager for each // field of the config? klog.Infof(\u0026quot;updating runtime config through cri with podcidr %v\u0026quot;, podCIDR) return m.runtimeService.UpdateRuntimeConfig( \u0026amp;runtimeapi.RuntimeConfig{ NetworkConfig: \u0026amp;runtimeapi.NetworkConfig{ PodCidr: podCIDR, }, }) }  kubeGenericRuntimeManager 的 runtimeService 在初始化时设置的是 instrumentedRuntimeService，这个结构是对 RuntimeService interface 的一个封装和实现，用来记录操作和错误的 metrics。\n// instrumentedRuntimeService wraps the RuntimeService and records the operations // and errors metrics. type instrumentedRuntimeService struct { service internalapi.RuntimeService }  而真正的 RuntimeService interface 的实现是在 NewMainKubelet() 的如下片段中赋值的。\nruntimeService, imageService, err := getRuntimeAndImageServices(remoteRuntimeEndpoint, remoteImageEndpoint, kubeCfg.RuntimeRequestTimeout) klet.runtimeService = runtimeService  remoteRuntimeEndpoint 是在 kubelet 启动命令中指定的值为 unix:///var/run/dockershim.sock ，kubelet 就是通过这个 socket 与 runtime 进行gRPC 通信的。保存在 KubeletFlags 中，该参数在\ntype KubeletFlags struct { KubeConfig string ... RemoteRuntimeEndpoint string RemoteImageEndpoint string }  getRuntimeAndImageServices() 调用 NewRemoteRuntimeService() 根据 RC 的 endpoint 创建一个 gRPC 的 client 封装到 RemoteRuntimeService 中，这是一个 internalapi.RuntimeService interface 的具体实现。\n// NewRemoteRuntimeService creates a new internalapi.RuntimeService. func NewRemoteRuntimeService(endpoint string, connectionTimeout time.Duration) (internalapi.RuntimeService, error) { addr, dailer, err := util.GetAddressAndDialer(endpoint) ctx, cancel := context.WithTimeout(context.Background(), connectionTimeout) defer cancel() conn, err := grpc.DialContext(ctx, addr, grpc.WithInsecure(), grpc.WithDialer(dailer), grpc.WithDefaultCallOptions(grpc.MaxCallRecvMsgSize(maxMsgSize))) return \u0026amp;RemoteRuntimeService{ timeout: connectionTimeout, runtimeClient: runtimeapi.NewRuntimeServiceClient(conn), lastError: make(map[string]string), errorPrinted: make(map[string]time.Time), }, nil }  runtime 主要提供两种服务：RuntimeService 和 ImageService 用来管理容器的镜像。 k8s 与 runtime 通过 RPC 通信，在配置 podCIDR 时调用的是 RuntimeService 的 UpdateRuntimeConfig rpc：\n// pkg/kubelet/apis/cri/runtime/v1alpha2/api.proto service RuntimeService { ... // UpdateRuntimeConfig updates the runtime configuration based on the given request. rpc UpdateRuntimeConfig(UpdateRuntimeConfigRequest) returns (UpdateRuntimeConfigResponse) {} }  这样网络配置就下发给了 runtime，runtime 调用 CNI 插件来做网络配置变更。\n// pkg/kubelet/dockershim/docker_service.go // UpdateRuntimeConfig updates the runtime config. Currently only handles podCIDR updates. func (ds *dockerService) UpdateRuntimeConfig(_ context.Context, r *runtimeapi.UpdateRuntimeConfigRequest) (*runtimeapi.UpdateRuntimeConfigResponse, error) { runtimeConfig := r.GetRuntimeConfig() if runtimeConfig == nil { return \u0026amp;runtimeapi.UpdateRuntimeConfigResponse{}, nil } klog.Infof(\u0026quot;docker cri received runtime config %+v\u0026quot;, runtimeConfig) if ds.network != nil \u0026amp;\u0026amp; runtimeConfig.NetworkConfig.PodCidr != \u0026quot;\u0026quot; { event := make(map[string]interface{}) event[network.NET_PLUGIN_EVENT_POD_CIDR_CHANGE_DETAIL_CIDR] = runtimeConfig.NetworkConfig.PodCidr ds.network.Event(network.NET_PLUGIN_EVENT_POD_CIDR_CHANGE, event) } return \u0026amp;runtimeapi.UpdateRuntimeConfigResponse{}, nil }  下图是 kubelet runtime UML\nkubenet plugin 实现 前面知道网络插件的接口是 NetworkPlugin interface，k8s kubenet 网络框架用 embed network.NoopNetworkPlugin 的 kubenetNetworkPlugin 实现了接口。\nkubenet 利用的是官方提供的三个 cni 类型插件: bridge, host-local, loopback (参考 cni plugins, cni ipam), 这个插件一般位于每个 Node 的 /opt/cni/bin 目录。\ntype kubenetNetworkPlugin struct { network.NoopNetworkPlugin host network.Host netConfig *libcni.NetworkConfig loConfig *libcni.NetworkConfig cniConfig libcni.CNI bandwidthShaper bandwidth.BandwidthShaper mu sync.Mutex //Mutex for protecting podIPs map, netConfig, and shaper initialization podIPs map[kubecontainer.ContainerID]string mtu int execer utilexec.Interface nsenterPath string hairpinMode kubeletconfig.HairpinMode // kubenet can use either hostportSyncer and hostportManager to implement hostports // Currently, if network host supports legacy features, hostportSyncer will be used, // otherwise, hostportManager will be used. hostportSyncer hostport.HostportSyncer hostportManager hostport.HostPortManager iptables utiliptables.Interface sysctl utilsysctl.Interface ebtables utilebtables.Interface // binDirs is passed by kubelet cni-bin-dir parameter. // kubenet will search for CNI binaries in DefaultCNIDir first, then continue to binDirs. binDirs []string nonMasqueradeCIDR string podCidr string gateway net.IP }  kubenet 直接利用了官方提供的三个 cni plugin:\n// pkg/kubelet/network/kubenet/kubenet_linux.go // CNI plugins required by kubenet in /opt/cni/bin or vendor directory var requiredCNIPlugins = [...]string{\u0026quot;bridge\u0026quot;, \u0026quot;host-local\u0026quot;, \u0026quot;loopback\u0026quot;}  kubenet 网络框架原理非常的简单, 主要利用 \u0026ldquo;bridge\u0026rdquo;, \u0026ldquo;host-local\u0026rdquo;, \u0026ldquo;loopback\u0026rdquo; (位于 /opt/cni/bin 目录下) 这三个 cni plugin主要的功能：\n 在每个 Node 上创建一个 cbr0 网桥 根据 PodCIDR 为每个 Pod 的 interface 分配一个 ip, 将该 interface 连接到 cbr0 网桥上.  当然, 对于 kubernetes 集群来说, 还需要解决两个问题:\n Node 的 PodCIDR 设置\nk8s kubenet 网络框架中，必须给每个 node 配置一个 podCIDR.\n那么, 每个 Node 的 PodCIDR 如何设置呢? 这个需要参考 kubenet 网络的配置文档了:\n The node must be assigned an IP subnet through either the \u0026ndash;pod-cidr kubelet command-line option or the \u0026ndash;allocate-node-cidrs=true \u0026ndash;cluster-cidr= controller-manager command-line options.\n 其实就是两种方式:\n 通过 \u0026ndash;pod-cidr 为每个 Node 上的 kubelet 配置好 PodCIDR 通过 \u0026ndash;allocate-node-cidrs=true \u0026ndash;cluster-cidr= 让 controller-manager 来为每个 Node 分配 PodCIDR.  Node 之间的路由设置\n虽然现在每个 Node 都配置好了 PodCIDR, 比如:\nNode1: 192.168.0.0/24 Node2: 192.168.1.0/24  但是 Node1 和 Node2 上的容器如何通信呢?\n It is typically used together with a cloud provider that sets up routing rules for communication between nodes, or in single-node environments.\n 通常情况下, kubenet 网络插件会跟 cloud provider 一起使用, 从而利用 cloud provider 来设置节点间的路由. kubenet 网络插件也可以用在单节点环境, 这样就不需要考虑 Node 间的路由了. 另外, 我们还可以通过实现一个 network controller 来保证 Node 间的路由.\n  kubenet Init // pkg/kubelet/dockershim/network/kubenet/kubenet_linux.go func NewPlugin(networkPluginDirs []string) network.NetworkPlugin { protocol := utiliptables.ProtocolIpv4 execer := utilexec.New() dbus := utildbus.New() sysctl := utilsysctl.New() iptInterface := utiliptables.New(execer, dbus, protocol) return \u0026amp;kubenetNetworkPlugin{ podIPs: make(map[kubecontainer.ContainerID]string), execer: utilexec.New(), iptables: iptInterface, sysctl: sysctl, binDirs: append([]string{DefaultCNIDir}, networkPluginDirs...), hostportSyncer: hostport.NewHostportSyncer(iptInterface), hostportManager: hostport.NewHostportManager(iptInterface), nonMasqueradeCIDR: \u0026quot;10.0.0.0/8\u0026quot;, } }  在前面的 InitNetworkPlugin() 流程中会调用各个插件的 Init() 来初始化插件。\n// pkg/kubelet/dockershim/network/kubenet/kubenet_linux.go func (plugin *kubenetNetworkPlugin) Init(host network.Host, hairpinMode kubeletconfig.HairpinMode, nonMasqueradeCIDR string, mtu int) error { ... // 确认加载了 br-netfilter，设置 bridge-nf-call-iptables=1 plugin.execer.Command(\u0026quot;modprobe\u0026quot;, \u0026quot;br-netfilter\u0026quot;).CombinedOutput() err := plugin.sysctl.SetSysctl(sysctlBridgeCallIPTables, 1) // 配置 loopback cni 插件 plugin.loConfig, err = libcni.ConfFromBytes([]byte(`{ \u0026quot;cniVersion\u0026quot;: \u0026quot;0.1.0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;kubenet-loopback\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;loopback\u0026quot; }`)) plugin.nsenterPath, err = plugin.execer.LookPath(\u0026quot;nsenter\u0026quot;) // 下发 SNAT 的 ipatable rule // Need to SNAT outbound traffic from cluster if err = plugin.ensureMasqRule(); err != nil { return err } return nil }   在 kubenet 中有个 MTU 的配置选项，network-plugin-mtu 指定 MTU，设置合理的 MTU 能有一个更好的网络性能。仅在 kubenet plugin 中支持。\n kubenet Event kubelet 启动到 NewMainKubelet 时, 根据用户配置通过 klet.updatePodCIDR(kubeCfg.PodCIDR) 向 k8s network plugin 通报 NET_PLUGIN_EVENT_POD_CIDR_CHANGE 事件, 该事件将会被 Event 方法捕获.\n// pkg/kubelet/network/kubenet/kubenet_linux.go const NET_CONFIG_TEMPLATE = `{ \u0026quot;cniVersion\u0026quot;: \u0026quot;0.1.0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;kubenet\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;bridge\u0026quot;, \u0026quot;bridge\u0026quot;: \u0026quot;%s\u0026quot;, \u0026quot;mtu\u0026quot;: %d, \u0026quot;addIf\u0026quot;: \u0026quot;%s\u0026quot;, \u0026quot;isGateway\u0026quot;: true, \u0026quot;ipMasq\u0026quot;: false, \u0026quot;hairpinMode\u0026quot;: %t, \u0026quot;ipam\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;host-local\u0026quot;, \u0026quot;subnet\u0026quot;: \u0026quot;%s\u0026quot;, \u0026quot;gateway\u0026quot;: \u0026quot;%s\u0026quot;, \u0026quot;routes\u0026quot;: [ { \u0026quot;dst\u0026quot;: \u0026quot;0.0.0.0/0\u0026quot; } ] } }` func (plugin *kubenetNetworkPlugin) Event(name string, details map[string]interface{}) { podCIDR, ok := details[network.NET_PLUGIN_EVENT_POD_CIDR_CHANGE_DETAIL_CIDR].(string) _, cidr, err := net.ParseCIDR(podCIDR) if err == nil { setHairpin := plugin.hairpinMode == kubeletconfig.HairpinVeth // Set bridge address to first address in IPNet cidr.IP[len(cidr.IP)-1] += 1 // 更新 cni 网络配置 // 从 NET_CONFIG_TEMPLATE 中看出, host-local ipam 的 subnet 就是 podCIDR // 这其实也就是为什么 k8s kubenet 网络插件需要为每个 node 分配 podCIDR 的原因 json := fmt.Sprintf(NET_CONFIG_TEMPLATE, BridgeName, plugin.mtu, network.DefaultInterfaceName, setHairpin, podCIDR, cidr.IP.String()) // 网络配置都保存在 netConfig 中 plugin.netConfig, err = libcni.ConfFromBytes([]byte(json)) if err == nil { klog.V(5).Infof(\u0026quot;CNI network config:\\n%s\u0026quot;, json) // Ensure cbr0 has no conflicting addresses; CNI's 'bridge' // plugin will bail out if the bridge has an unexpected one plugin.clearBridgeAddressesExcept(cidr) } plugin.podCidr = podCIDR plugin.gateway = cidr.IP } }   todo: Event() 也只是更新了 podCidr 和 netConfig，哪里下发了更新？\n 根据配置的改变设置 kubenetNetworkPlugin 对应的变量。\nkubenet SetUpPod 创建 Pod 的时候会调用该方法，该方法调用 setup() 来完成配置，这个接口最重要的功能是将容器的 eth0 接口加入到了 namespace 中\n// setup sets up networking through CNI using the given ns/name and sandbox ID. func (plugin *kubenetNetworkPlugin) setup(namespace string, name string, id kubecontainer.ContainerID, annotations map[string]string) error { // 添加 loopback interface 到 pod 的 network namespace // Bring up container loopback interface if _, err := plugin.addContainerToNetwork(plugin.loConfig, \u0026quot;lo\u0026quot;, namespace, name, id); err != nil { return err } // 添加 DefaultInterfaceName eth0 到 pod 的 network namespace // Hook container up with our bridge resT, err := plugin.addContainerToNetwork(plugin.netConfig, network.DefaultInterfaceName, namespace, name, id) if err != nil { return err } // Coerce the CNI result version res, err := cnitypes020.GetResult(resT) ip4 := res.IP4.IP.IP.To4() // 为了配置 hairpin 设置网卡混杂模式 ... plugin.podIPs[id] = ip4.String() // TODO: replace with CNI port-forwarding plugin // TODO: portMappings 的用途是什么？ portMappings, err := plugin.host.GetPodPortMappings(id.ID) if err != nil { return err } if portMappings != nil \u0026amp;\u0026amp; len(portMappings) \u0026gt; 0 { if err := plugin.hostportManager.Add(id.ID, \u0026amp;hostport.PodPortMapping{ Namespace: namespace, Name: name, PortMappings: portMappings, IP: ip4, HostNetwork: false, }, BridgeName); err != nil { return err } } return nil }  接着看看 addContainerToNetwork() 方法:\n// pkg/kubelet/dockershim/network/kubenet/kubenet_linux.go func (plugin *kubenetNetworkPlugin) addContainerToNetwork(config *libcni.NetworkConfig, ifName, namespace, name string, id kubecontainer.ContainerID) (cnitypes.Result, error) { rt, err := plugin.buildCNIRuntimeConf(ifName, id, true) if err != nil { return nil, fmt.Errorf(\u0026quot;Error building CNI config: %v\u0026quot;, err) } // The network plugin can take up to 3 seconds to execute, // so yield the lock while it runs. plugin.mu.Unlock() res, err := plugin.cniConfig.AddNetwork(config, rt) plugin.mu.Lock() return res, nil }  由前面 CNI 库接口可知, plugin.cniConfig.AddNetwork() 实际上调用的是 cni plugin 去实现容器网络配置. kubenet plugin 主要通过 loopback 和 bridge cni 插件将容器的 lo 和 eth0 添加到容器网络中. bridge 插件负责 Node 上 cbr0 的创建, 然后创建 veth 接口对, 通过 veth 接口对, 将容器添加到容器网络中. 另外, host-local IPAM plugin 负责为 eth0 分配 ip 地址.\nkubenet TearDownPod 删除 Pod 的时候会被调用。主要是通过函数 teardown() 实现。主要的流程是调用 CNI 删除网络配置。\n// Tears down as much of a pod's network as it can even if errors occur. Returns // an aggregate error composed of all errors encountered during the teardown. func (plugin *kubenetNetworkPlugin) teardown(namespace string, name string, id kubecontainer.ContainerID, podIP string) error { errList := []error{} if err := plugin.delContainerFromNetwork(plugin.netConfig, network.DefaultInterfaceName, namespace, name, id); err != nil { // This is to prevent returning error when TearDownPod is called twice on the same pod. This helps to reduce event pollution. if podIP != \u0026quot;\u0026quot; { klog.Warningf(\u0026quot;Failed to delete container from kubenet: %v\u0026quot;, err) } else { errList = append(errList, err) } } portMappings, err := plugin.host.GetPodPortMappings(id.ID) if err != nil { errList = append(errList, err) } else if portMappings != nil \u0026amp;\u0026amp; len(portMappings) \u0026gt; 0 { if err = plugin.hostportManager.Remove(id.ID, \u0026amp;hostport.PodPortMapping{ Namespace: namespace, Name: name, PortMappings: portMappings, HostNetwork: false, }); err != nil { errList = append(errList, err) } } return utilerrors.NewAggregate(errList) }  由前面 CNI 库接口可知, plugin.cniConfig.DelNetwork() 实际上调用的是 cni plugin 去删除容器网络配置. bridge 插件负责调用 host-local IPAM plugin 释放该容器的 ip, 然后删除容器的网络接口等.\nCNI plugin 实现 CNI plugin 是一种更通用的实现，允许用户自定义插件。cniNetworkPlugin 是 NetworkPlugin interface 的一个实现，具体的代码如下。\n// pkg/kubelet/dockershim/network/cni/cni.go type cniNetworkPlugin struct { network.NoopNetworkPlugin loNetwork *cniNetwork sync.RWMutex defaultNetwork *cniNetwork host network.Host execer utilexec.Interface nsenterPath string confDir string binDirs []string podCidr string }  通过 cniNetwork 类型的 loNetwork 和 defaultNetwork 来调用 CNI 插件，cniNetwork 定义如下。\n// pkg/kubelet/dockershim/network/cni/cni.go type cniNetwork struct { name string NetworkConfig *libcni.NetworkConfigList CNIConfig libcni.CNI }  CNI Init 在 NewDockerService() 函数中调用 ProbeNetworkPlugins() 根据配置的 CNI 插件的路径生成 network.NetworkPlugin interface 的实现 cniNetworkPlugin。\n// pkg/kubelet/dockershim/network/cni/cni.go func ProbeNetworkPlugins(confDir string, binDirs []string) []network.NetworkPlugin { old := binDirs binDirs = make([]string, 0, len(binDirs)) for _, dir := range old { if dir != \u0026quot;\u0026quot; { binDirs = append(binDirs, dir) } } plugin := \u0026amp;cniNetworkPlugin{ defaultNetwork: nil, loNetwork: getLoNetwork(binDirs), execer: utilexec.New(), confDir: confDir, binDirs: binDirs, } // sync NetworkConfig in best effort during probing. plugin.syncNetworkConfig() return []network.NetworkPlugin{plugin} }  主要是对 loNetwork 和 defaultNetwork 变量的配置。\n// pkg/kubelet/dockershim/network/cni/cni_others.go func getLoNetwork(binDirs []string) *cniNetwork { loConfig, err := libcni.ConfListFromBytes([]byte(`{ \u0026quot;cniVersion\u0026quot;: \u0026quot;0.2.0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;cni-loopback\u0026quot;, \u0026quot;plugins\u0026quot;:[{ \u0026quot;type\u0026quot;: \u0026quot;loopback\u0026quot; }] }`)) loNetwork := \u0026amp;cniNetwork{ name: \u0026quot;lo\u0026quot;, NetworkConfig: loConfig, CNIConfig: \u0026amp;libcni.CNIConfig{Path: binDirs}, } return loNetwork }  Init() 做的就是配置 defaultNetwork\nfunc getDefaultCNINetwork(confDir string, binDirs []string) (*cniNetwork, error) { // 从配置文件获取 confList network := \u0026amp;cniNetwork{ name: confList.Name, NetworkConfig: confList, CNIConfig: \u0026amp;libcni.CNIConfig{Path: binDirs}, } return network, nil } }  CNI Event 收到 NET_PLUGIN_EVENT_POD_CIDR_CHANGE 事件时只是更新了 podCidr 的值\nfunc (plugin *cniNetworkPlugin) Event(name string, details map[string]interface{}) { podCIDR, ok := details[network.NET_PLUGIN_EVENT_POD_CIDR_CHANGE_DETAIL_CIDR].(string) plugin.podCidr = podCIDR }  可见 k8s cni 网络方式并没有规定使用 podCidr 来配置 node 上容器的网络 ip 段, 而把 pod 的 ip 分配完全交给 IPAM, 这样使得 IPAM 更加灵活, 多样化和定制化\nCNI SetUpPod // pkg/kubelet/dockershim/network/cni/cni.go func (plugin *cniNetworkPlugin) SetUpPod(namespace string, name string, id kubecontainer.ContainerID, annotations, options map[string]string) error { ... // Windows doesn't have loNetwork. It comes only with Linux if plugin.loNetwork != nil { if _, err = plugin.addToNetwork(plugin.loNetwork, name, namespace, id, netnsPath, annotations, options); err != nil { return err } } _, err = plugin.addToNetwork(plugin.getDefaultNetwork(), name, namespace, id, netnsPath, annotations, options) return err }  func (plugin *cniNetworkPlugin) addToNetwork(network *cniNetwork, podName string, podNamespace string, podSandboxID kubecontainer.ContainerID, podNetnsPath string, annotations, options map[string]string) (cnitypes.Result, error) { netConf, cniNet := network.NetworkConfig, network.CNIConfig res, err := cniNet.AddNetworkList(netConf, rt) }  由前面 CNI 库接口可知, cninet.AddNetwork() 实际上调用的是底层用户配置的 cni plugin 去实现容器网络配置.\nCNI TearDownPod 与前面的流程类似，最终调用底层的 cni plugin 删掉配置。代码略。\nkubernets network plugin 安装 kubelet 有一个默认的 plugin，然后为整个集群提供一个默认的网络。当启动时探测到插件后就可以在 pod 整个生命周期里调换用。有两个启动参数：\n cni-bin-dir：启动时加载这个参数指定的路径里的 plugin network-plugin：插件的名字，要能匹配上面路径中的插件，比如 CNI 配置为 \u0026ldquo;cni\u0026rdquo;  network plugin 需求 plguin 除了要提供 NetworkPlugin interface 添加和删除 pod 的网络之外，还要实现对 kube-proxy 的支持。proxy 依赖 iptables，plugin 需要确保容器流量可以使用 iptables。比如 plugin 将容器添加到 Linux bridge，就需要通过 sysctl 设置 net/bridge/bridge-nf-call-iptables = 1 来确保 iptables proxy 功能正常。\n如果没有指定 network plugin，就使用 noop plugin 设置 net/bridge/bridge-nf-call-iptables=1。\nCNI 在 kubelet 命令行中 --network-plugin=cni 指定了采用 CNI 插件，kubelet 从 --cni-conf-dir（默认 /etc/cni/net.d）读取配置文件来配置 pod 网络。CNI 配置文件要遵循 CNI specification，配置文件中指定的 CNI 插件的执行程序要放在 \u0026ndash;cni-bin-dir (default /opt/cni/bin)。\n如果目录下有多个 CNI 配置文件，按字典序采用第一个配置文件。\n除了配置文件中指定的 CNI 插件外，K8S 还需要标准的 lo 插件。\nhostPort 支持 CNI plugin 支持 hostPort，可以采用官方的 portmap，也可以自己实现。\n需要在 cni-conf-dir 中开启 portMappings capability 来支持 hostPort。\n{ \u0026quot;name\u0026quot;: \u0026quot;k8s-pod-network\u0026quot;, \u0026quot;cniVersion\u0026quot;: \u0026quot;0.3.0\u0026quot;, \u0026quot;plugins\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;calico\u0026quot;, \u0026quot;log_level\u0026quot;: \u0026quot;info\u0026quot;, \u0026quot;datastore_type\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;nodename\u0026quot;: \u0026quot;127.0.0.1\u0026quot;, \u0026quot;ipam\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;host-local\u0026quot;, \u0026quot;subnet\u0026quot;: \u0026quot;usePodCidr\u0026quot; }, \u0026quot;policy\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;k8s\u0026quot; }, \u0026quot;kubernetes\u0026quot;: { \u0026quot;kubeconfig\u0026quot;: \u0026quot;/etc/cni/net.d/calico-kubeconfig\u0026quot; } }, { \u0026quot;type\u0026quot;: \u0026quot;portmap\u0026quot;, \u0026quot;capabilities\u0026quot;: {\u0026quot;portMappings\u0026quot;: true} } ] }  traffic shaping 支持 CNI plugin 支持 pod ingress 和 egress 整形，可以使用官方提供的 bandwidth 或自定义的插件。同样需要在配置文件（默认在 /etc/cni/net.d）中配置。\n{ \u0026quot;name\u0026quot;: \u0026quot;k8s-pod-network\u0026quot;, \u0026quot;cniVersion\u0026quot;: \u0026quot;0.3.0\u0026quot;, \u0026quot;plugins\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;calico\u0026quot;, \u0026quot;log_level\u0026quot;: \u0026quot;info\u0026quot;, \u0026quot;datastore_type\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;nodename\u0026quot;: \u0026quot;127.0.0.1\u0026quot;, \u0026quot;ipam\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;host-local\u0026quot;, \u0026quot;subnet\u0026quot;: \u0026quot;usePodCidr\u0026quot; }, \u0026quot;policy\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;k8s\u0026quot; }, \u0026quot;kubernetes\u0026quot;: { \u0026quot;kubeconfig\u0026quot;: \u0026quot;/etc/cni/net.d/calico-kubeconfig\u0026quot; } }, { \u0026quot;type\u0026quot;: \u0026quot;bandwidth\u0026quot;, \u0026quot;capabilities\u0026quot;: {\u0026quot;bandwidth\u0026quot;: true} } ] }  现在你可以向 pod 中添加 kubernetes.io/ingress-bandwidth 和 kubernetes.io/egress-bandwidth 的 annotations，例如：\napiVersion: v1 kind: Pod metadata: annotations: kubernetes.io/ingress-bandwidth: 1M kubernetes.io/egress-bandwidth: 1M ...  CNI 演进 Multi CNI and Containers with Multi Network Interfaces on Kubernetes with CNI-Genie\nmultus-cni\n参考 k8s network\nNetwork Plugins\n问题  dockerService 用途是什么？\n 在一个集群里不同的 Node 上可以配置不同的 plugin 吗？\n  ","date":1559001600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559001600,"objectID":"631156bfce891192f2b6a9b8eef15b66","permalink":"/post/cloud/k8s/201905-k8s-network-arch/","publishdate":"2019-05-28T00:00:00Z","relpermalink":"/post/cloud/k8s/201905-k8s-network-arch/","section":"post","summary":"Kubernetes本身不提供容器网络，但具有可扩展的网络框架","tags":["Kubernetes","CNI"],"title":"Kubernetes网络框架","type":"post"},{"authors":null,"categories":null,"content":" 人们对云计算提出了更高的要求，为大量项目构建运营环境的效率问题，缩短新业务的上线部署时间，大规模的计算机房快速迁移需求；提高服务器资源的利用率，同时确保相同的性能和可用性，又有降低成本的需求。相较于传统的虚拟化解决方案，容器云可以较好的实现上述目标。\n容器云的核心功能：\n 快速扩容 智能调度和编排 弹性伸缩  快速扩容  扩容速度：  VM - 扩容20个实例需要4分钟（扩容完成后需要再执行服务发布） docker - 扩容20个实例仅需30s，秒级别扩容（扩容完成即服务启动）  扩容速度提高 8~12倍 节约了用户手动操作申请/发布的成本  智能调度 调度系统是云集群的中央处理器，要解决的核心问题是为容器选择合适的宿主机。有如下的指标：\n 资源利用率，提高整体物理集群的资源利用率 业务可用性保障：业务容器容灾能力、保障运行业务的稳定高可用 并发调度能力：调度系统请求处理能力的体现  资源最大化利用  按CPU/Mem/IO等类型对服务进行调度，最大化资源利用 业务按需使用资源，提升资源利用率  混布与独占  在线服务与离线任务混布 重要业务资源池独占  容器编排  有调用关系的多个服务实例，优先部署到相同/相近的宿主机上 同服务实例打散，分布到不同宿主机上，提高服务可用性 高负载容器，自动迁移到低负载宿主机 自动化容器实例健康检查，异常实例自动迁移  调度计算 通过先过滤filter之后排序打分rank的方式找到最优的部署位置。\n在一批宿主机中先过滤掉超售的，然后考虑到打散、混部、减少碎片和负载均衡之后找到合适的宿主机\n调度SLA（Service Level Agreement）  高可用：99.999 调度成功率：99.99 并发调度：单机并发处理200+，并发调度机器1000+ 低延迟：TCP90 63ms HA：分布式调度，横向扩展，多IDC部署容灾 监控报警：Falcon  弹性收缩 周期收缩\n根据设定时间段伸缩（适合秒杀/直播等业务）\n监控伸缩\n 根据QPS/CPU等触发条件伸缩 线性可扩展的无状态服务  服务画像\n针对数据建模，描绘服务特征：\n 服务画像：仿照用户画像，根据服务数据，抽取服务Tag  QPS特征（高峰时段、QPS max/min等） 资源利用率 CPU密集型 or IO密集型   基于历史数据建模的服务画像可以做服务特征值的预测，比如QPS的预测：\n QPS预测：RNN LSTM 即使监控数据源完全不可用，无数据，也能较准确的扩缩容  异常处理  监控数据异常，怎么办？会不会因监控值偏低而一直缩容？ 监控数据有延迟，怎么办？ 监控数据没了，怎么办？  通过数据无关的缩容退避+熔断机制来保证异常情况下的正常运行：\n 针对监控数据偏低（异常）而触发持续缩容 数据无关，不关心数据是否异常 如果连续缩容，那么缩容速度会越来越慢 —\u0026gt; 退避 如果连续缩容次数超过阈值，一段时间内禁止缩容 —\u0026gt; 熔断  ","date":1557158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557158400,"objectID":"318d50b7d7643d72671620a688ce7837","permalink":"/post/cloud/container/201905-why-container-cloud/why-container/","publishdate":"2019-05-07T00:00:00+08:00","relpermalink":"/post/cloud/container/201905-why-container-cloud/why-container/","section":"post","summary":"容器云解决大量项目构建运营环境的效率问题","tags":["Docker","Cloud"],"title":"Why 容器云","type":"post"},{"authors":null,"categories":null,"content":" API对象 在Kubernetes中API对象是以树形结构表示的，一个API对象在Etcd里完整资源路径，是由Group（API组）、Version（API版本）和Resource（API资源类型）三个部分组成的。\n如果现在要声明一个CronJob对象，那么YAML的开始部分会这么写，CronJob就是这个API对象的资源类型，Batch就是它们的组，v2alpha1就是它的版本\napiVersion: batch/v2alpha1 kind: CronJob ...  API解析 Kubernetes通过对API解析找到对应的对象，分为如下3步：\n 解析API的组 Kubernetes的对象分两种：  核心API对象（如Pod、Node），是不需要Group的，直接在 /api这个下面进行解析 非核心API对象，在 /apis 下先解析出Group，根据batch这个Group找到 /apis/batch，API Group的分类是以对象功能为依据的。  解析API对象的版本号 匹配API对象的资源类型  创建对象 在前面匹配到正确的版本之后，Kubernetes就知道要创建的是一个/apis/batch/v2alpha1下的CronJob对象，APIServer会继续创建这个Cronjob对象。创建过程如下图\n 当发起创建CronJob的POST请求之后，YAML的信息就被提交给了APIServer，APIServer的第一个功能就是过滤这个请求，并完成一些前置性的工作，比如授权、超时处理、审计等 请求进入MUX和Routes流程，MUX和Routes是APIServer完成URL和Handler绑定的场所。APIServer的Handler要做的事情，就是按照上面介绍的匹配过程，找到对应的CronJob类型定义。 根据这个CronJob类型定义，使用用户提交的YAML文件里的字段，创建一个CronJob对象。这个过程中，APIServer会把用户提交的YAML文件，转换成一个叫做Super Version的对象，它正是该API资源类型所有版本的字段全集，这样用户提交的不同版本的YAML文件，就都可以用这个SuperVersion对象来进行处理了。 APIServer会先后进行Admission（如Admission Controller 和 Initializer）和Validation操作（负责验证这个对象里的各个字段是否何方，被验证过得API对象都保存在APIServer里一个叫做Registry的数据结构中）。 APIServer会把验证过得API对象转换成用户最初提交的版本，进行系列化操作，并调用Etcd的API把它保存起来。\n  CRD API插件CRD（Custom Resource Definition） 允许用户在Kubernetes中添加一个跟Pod、Node类似的、新的API资源类型，即：自定义API资源\n举个栗子，添加一个叫Network的API资源类型，它的作用是一旦用户创建一个Network对象，那么Kubernetes就可以使用这个对象定义的网络参数，调用真实的网络插件，为用户创建一个真正的网络，这个过程分为两步\n 首先定义CRD  定义一个group为samplecrd.k8s.io， version为v1的API信息，指定了这个CR的资源类型叫做Network，定义的这个Network是属于一个Namespace的对象，类似于Pod。\napiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: networks.samplecrd.k8s.io spec: group: samplecrd.k8s.io version: v1 names: kind: Network plural: networks scope: Namespaced   对象实例化  实例化名为example-network的Network对象，API组是samplecrd.k8s.io，版本是v1。\napiVersion: samplecrd.k8s.io/v1 kind: Network metadata: name: example-network spec: cidr: \u0026quot;192.168.0.0/16\u0026quot; gateway: \u0026quot;192.168.0.1\u0026quot;  Network对象YAML文件，名叫example-network.yaml,API资源类型是Network，API组是samplecrd.k8s.io，版本是v1\nKubernetes的声明式API能够对API对象进行增量的更新操作：\n 定义好期望的API对象后，Kubernetes来尽力让对象的状态符合预期 允许多个YAML表达，以PATCH的方式对API对象进行修改，而不用关心原始YAML的内容  基于上面两种特性，Kubernetes可以实现基于API对象的更删改查，完成预期和定义的协调过程。\n因此Kubernetes项目编排能力的核心是声明式API。\nKubernetes编程范式即：如何使用控制器模式，同Kubernetes里的API对象的“增、删、改、查”进行协作，进而完成用户业务逻辑的编写过程。\nkubectl apply kubectl apply是声明式的请求，下面一个Deployment的YAML的例子\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80  然后用kubectl apply创建这个Deployment\n$ kubectl apply -f nginx.yaml  修改一下nginx里定义的镜像\napiVersion: apps/v1 kind: Deployment ... spec: containers: - name: nginx image: nginx:1.7.9  执行kubectl apply命令，触发滚动更新\n$ kubectl apply -f nginx.yaml  后面一次的 kubectl apply命令执行了一个对原有API对象的PATCH操作，这是声明式命令同时可以进行多个写操作，具有Merge的能力；而像 kubectl replace命令是用新的YAML替换旧的，这种响应式命令每次只能处理一次写操作。\n声明式API的应用 Istio通过声明式API实现对应用容器所在POD注入Sidecar，然后通过iptables劫持POD的进站和出站流量到Sidecar，Istio通过对Sidecar下发策略来实现对应用流量的管控，继而实现微服务治理。\n在微服务治理中，对Envoy容器的部署和对Envoy代理的配置，应用容器都是不感知的。Istio是使用Kubernetes的Dynamic Admission Control来实现的。\n在APIServer收到API对象的提交请求后，在正常处理这些操作之前会做一些初始化的操作，比如为某些pod或容器加上一些label。这些初始化操作是通过Kubernetes的Admission Controller实现的，在APIServer对象创建之后调用，但这种方式的缺陷是需要将Admission Controller的代码编译到APIServer中，这不是很方便。Kubernetes 1.7引入了热插拔的Admission机制，它就是Dynamic Admission Control，也叫做Initializer。\n如下定义的应用的Pod，包含一个myapp-container的容器。\napiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026amp;\u0026amp; sleep 3600']  Istio要做的就是在这个Pod YAML被提交给Kubernetes之后，在它对应的API对象里自动加上Envoy容器的配置，使对象变成如下的样子：\napiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026amp;\u0026amp; sleep 3600'] - name: envoy image: lyft/envoy:845747b88f102c0fd262ab234308e9e22f693a1 command: [\u0026quot;/usr/local/bin/envoy\u0026quot;] ...  这个pod多了一个envoy的容器，Istio具体的做法是\n 定义Envoy容器的Initializer，并以ConfigMap的方式保存到Kubernetes中 Istio将编写好的Initializer作为一个Pod部署在Kubernetes中  Envoy容器的ConfigMap定义，\napiVersion: v1 kind: ConfigMap metadata: name: envoy-initializer data: config: | containers: - name: envoy image: lyft/envoy:845747db88f102c0fd262ab234308e9e22f693a1 command: [\u0026quot;/usr/local/bin/envoy\u0026quot;] args: - \u0026quot;--concurrency 4\u0026quot; - \u0026quot;--config-path /etc/envoy/envoy.json\u0026quot; - \u0026quot;--mode serve\u0026quot; ports: - containerPort: 80 protocol: TCP resources: limits: cpu: \u0026quot;1000m\u0026quot; memory: \u0026quot;512Mi\u0026quot; requests: cpu: \u0026quot;100m\u0026quot; memory: \u0026quot;64Mi\u0026quot; volumeMounts: - name: envoy-conf mountPath: /etc/envoy volumes: - name: envoy-conf configMap: name: envoy  这个ConfigMap的data部分，正是一个Pod对象的一部分定义，其中可以看到Envoy容器对应的Container字段，以及一个用来声明Envoy配置文件的volumes字段。Initializer要做的就是把这部分Envoy相关的字段，自动添加到用户提交的Pod的API对象里。但是用户提交的Pod里本来就有containers和volumes字段，所以Kubernetes在处理这样的更新请求时，就必须使用类似于git merge这样的操作，才能将这两部分内容合并在一起。即Initializer更新用户的Pod对象时，必须使用PATCH API来完成。\nEnvoy Initializer的pod定义\napiVersion: v1 kind: Pod metadata: labels: app: envoy-initializer name: envoy-initializer spec: containers: - name: envoy-initializer image: envoy-initializer:0.0.1 imagePullPolicy: Always ```\t`envoy-initializer:0.0.1` 镜像是一个自定义控制器（Custom Controller）。Kubernetes的控制器实际上是一个死循环：它不断地获取实际状态，然后与期望状态作对比，并以此为依据决定下一步的操作。 对Initializer控制器，不断获取的实际状态，就是用户新创建的Pod，它期望的状态就是这个Pod里被添加了Envoy容器的定义。它的控制逻辑如下： ```go for { // 获取新创建的 Pod pod := client.GetLatestPod() // Diff 一下，检查是否已经初始化过 if !isInitialized(pod) { // 没有？那就来初始化一下 //istio要往这个Pod里合并的字段，就是ConfigMap里data字段的值 doSomething(pod) } } func doSomething(pod) { //调用APIServer拿到ConfigMap cm := client.Get(ConfigMap, \u0026quot;envoy-initializer\u0026quot;) //把ConfigMap里存在的containers和volumes字段，直接添加进一个空的Pod对象 newPod := Pod{} newPod.Spec.Containers = cm.Containers newPod.Spec.Volumes = cm.Volumes // Kubernetes的API库，提供一个方法使我们可以直接使用新旧两个Pod对象，生成 patch 数据 patchBytes := strategicpatch.CreateTwoWayMergePatch(pod, newPod) // 发起 PATCH 请求，修改这个 pod 对象 client.Patch(pod.Name, patchBytes) }  Envoy机制正是利用了Kubernetes能够对API对象做增量更新，这是Kubernetes声明式API的独特之处。\n参考 Dynamic Admission Control\n【Kubernetes】深入解析声明式API\n","date":1545753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545753600,"objectID":"a932d713bc6cbd242ccbb247be2576d4","permalink":"/post/cloud/k8s/201904-k8s-declarative-api/","publishdate":"2018-12-26T00:00:00+08:00","relpermalink":"/post/cloud/k8s/201904-k8s-declarative-api/","section":"post","summary":"声明式API是Kubernetes成为容器编排事实标准的利器","tags":["Kubernetes"],"title":"Kubernetes声明式API","type":"post"},{"authors":null,"categories":null,"content":" Kubernetes Kubernetes 是 Google 开源的容器集群管理系统，其提供应用部署、维护、 扩展机制等功能，利用 Kubernetes 能方便地管理跨机器运行容器化的应用，其主要功能如下：\n1) 使用 Docker 对应用程序包装 (package)、实例化 (instantiate)、运行 (run)。\n2) 以集群的方式运行、管理跨机器的容器。\n3) 解决 Docker 跨机器容器之间的通讯问题。\n4) Kubernetes 的自我修复机制使得容器集群总是运行在用户期望的状态。\nk8s 要做的不是dockerize，也不是containerize，而是作为一个集群操作系统，为此重新定义了可执行文件、进程、存储、网络的形态。\n整体结构 Master 集群控制节点，master节点上运行着一组关键的进程\n etcd，各个组件通信都并不是互相调用 API 来完成的，而是把状态写入 ETCD（相当于写入一个消息），其他组件通过监听 ETCD 的状态的的变化（相当于订阅消息），然后做后续的处理，然后再一次把更新的数据写入 ETCD。 api server，各个组件并不是直接访问 ETCD，而是访问一个代理，这个代理是通过标准的RESTFul API，重新封装了对 ETCD 接口调用，除此之外，这个代理还实现了一些附加功能，比如身份的认证、缓存等 Controller Manager 是实现任务调度的 Scheduler 是用来做资源调度的  Master 定义了 Kubernetes 集群 Master/API Server 的主要声明，包括 Pod Registry、Controller Registry、Service Registry、Endpoint Registry、Minion Registry、Binding Registry、RESTStorage 以及 Client, 是 client(Kubecfg) 调用 Kubernetes API，管理 Kubernetes 主要构件 Pods、Services、Minions、容器的入口。Master 由 API Server、Scheduler 以及 Registry 等组成。从下图可知 Master 的工作流主要分以下步骤：\n图片 - Master 主要构件及工作流\n1) Kubecfg 将特定的请求，比如创建 Pod，发送给 Kubernetes Client。\n2) Kubernetes Client 将请求发送给 API server。\n3) API Server 根据请求的类型，比如创建 Pod 时 storage 类型是 pods，然后依此选择何种 REST Storage API 对请求作出处理。\n4) REST Storage API 对的请求作相应的处理。\n5) 将处理的结果存入高可用键值存储系统 Etcd 中。\n6) 在 API Server 响应 Kubecfg 的请求后，Scheduler 会根据 Kubernetes Client 获取集群中运行 Pod 及 Minion 信息。\n7) 依据从 Kubernetes Client 获取的信息，Scheduler 将未分发的 Pod 分发到可用的 Minion 节点上。\nMinion Registry Minion Registry 负责跟踪 Kubernetes 集群中有多少 Minion(Host)。Kubernetes 封装 Minion Registry 成实现 Kubernetes API Server 的 RESTful API 接口 REST，通过这些 API，我们可以对 Minion Registry 做Create、Get、List、Delete 操作，由于 Minon 只能被创建或删除，所以不支持 Update 操作，并把 Minion 的相关配置信息存储到 etcd。除此之外，Scheduler 算法根据 Minion 的资源容量来确定是否将新建 Pod 分发到该 Minion 节点。\nPod Registry Pod Registry 负责跟踪 Kubernetes 集群中有多少 Pod 在运行，以及这些 Pod 跟 Minion 是如何的映射关系。将 Pod Registry 和 Cloud Provider 信息及其他相关信息封装成实现 Kubernetes API Server 的 RESTful API 接口 REST。通过这些 API，我们可以对 Pod 进行 Create、Get、List、Update、Delete 操作，并将 Pod 的信息存储到 etcd 中，而且可以通过 Watch 接口监视 Pod 的变化情况，比如一个 Pod 被新建、删除或者更新。\nService Registry Service Registry 负责跟踪 Kubernetes 集群中运行的所有服务。根据提供的 Cloud Provider 及 Minion Registry 信息把 Service Registry 封装成实现 Kubernetes API Server 需要的 RESTful API 接口 REST。利用这些接口，我们可以对 Service 进行 Create、Get、List、Update、Delete 操作，以及监视 Service 变化情况的 watch 操作，并把 Service 信息存储到 etcd。\nController Registry Controller Registry 负责跟踪 Kubernetes 集群中所有的 Replication Controller，Replication Controller 维护着指定数量的 pod 副本 (replicas) 拷贝，如果其中的一个容器死掉，Replication Controller 会自动启动一个新的容器，如果死掉的容器恢复，其会杀死多出的容器以保证指定的拷贝不变。通过封装 Controller Registry 为实现 Kubernetes API Server 的 RESTful API 接口 REST， 利用这些接口，我们可以对 Replication Controller 进行 Create、Get、List、Update、Delete 操作，以及监视 Replication Controller 变化情况的 watch 操作，并把 Replication Controller 信息存储到 etcd。\nEndpoints Registry Endpoints Registry 负责收集 Service 的 endpoint，比如 Name：\u0026rdquo;mysql\u0026rdquo;，Endpoints: [\u0026ldquo;10.10.1.1:1909\u0026rdquo;，\u0026rdquo;10.10.2.2:8834\u0026rdquo;]，同 Pod Registry，Controller Registry 也实现了 Kubernetes API Server 的 RESTful API 接口，可以做 Create、Get、List、Update、Delete 以及 watch 操作。\nBinding Registry Binding 包括一个需要绑定 Pod 的 ID 和 Pod 被绑定的 Host，Scheduler 写 Binding Registry 后，需绑定的 Pod 被绑定到一个 host。Binding Registry 也实现了 Kubernetes API Server 的 RESTful API 接口，但 Binding Registry 是一个 write-only 对象，所有只有 Create 操作可以使用， 否则会引起错误。\nScheduler Scheduler 收集和分析当前 Kubernetes 集群中所有 Minion 节点的资源 (内存、CPU) 负载情况，然后依此分发新建的 Pod 到 Kubernetes 集群中可用的节点。由于一旦 Minion 节点的资源被分配给 Pod，那这些资源就不能再分配给其他 Pod， 除非这些 Pod 被删除或者退出， 因此，Kubernetes 需要分析集群中所有 Minion 的资源使用情况，保证分发的工作负载不会超出当前该 Minion 节点的可用资源范围。具体来说，Scheduler 做以下工作：\n1) 实时监测 Kubernetes 集群中未分发的 Pod。\n2) 实时监测 Kubernetes 集群中所有运行的 Pod，Scheduler 需要根据这些 Pod 的资源状况安全地将未分发的 Pod 分发到指定的 Minion 节点上。\n3) Scheduler 也监测 Minion 节点信息，由于会频繁查找 Minion 节点，Scheduler 会缓存一份最新的信息在本地。\n4) 最后，Scheduler 在分发 Pod 到指定的 Minion 节点后，会把 Pod 相关的信息 Binding 写回 API Server。\nNode Node是工作主机，可以使物理主机、VM等。\n kubelet：负责管控docker容器，如启动/停止、监控运行状态等。 kube-proxy： 负责为pod提供代理。它会定期从etcd获取所有的service，并根据service信息创建代理。  kubelet 根据上图可知 Kubelet 是 Kubernetes 集群中每个 Minion 和 Master API Server 的连接点，Kubelet 运行在每个 Minion 上，是 Master API Server 和 Minion 之间的桥梁，接收 Master API Server 分配给它的 commands 和 work，与持久性键值存储 etcd、file、server 和 http 进行交互，读取配置信息。Kubelet 的主要工作是管理 Pod 和容器的生命周期，其包括 Docker Client、Root Directory、Pod Workers、Etcd Client、Cadvisor Client 以及 Health Checker 组件，具体工作如下：\n1) 通过 Worker 给 Pod 异步运行特定的 Action。\n2) 设置容器的环境变量。\n3) 给容器绑定 Volume。\n4) 给容器绑定 Port。\n5) 根据指定的 Pod 运行一个单一容器。\n6) 杀死容器。\n7) 给指定的 Pod 创建 network 容器。\n8) 删除 Pod 的所有容器。\n9) 同步 Pod 的状态。\n10) 从 Cadvisor 获取 container info、 pod info、root info、machine info。\n11) 检测 Pod 的容器健康状态信息。\n12) 在容器中运行命令。\nContainer Runtime（容器运行时） 每一个Node都会运行一个Container Runtime，其负责下载镜像和运行容器。Kubernetes本身并不停容器运行时环境，但提供了接口，可以插入所选择的容器运行时环境。kubelet使用Unix socket之上的gRPC框架与容器运行时进行通信，kubelet作为客户端，而CRI shim作为服务器。\nprotocol buffers API提供两个gRPC服务，ImageService和RuntimeService。ImageService提供拉取、查看、和移除镜像的RPC。RuntimeSerivce则提供管理Pods和容器生命周期管理的RPC，以及与容器进行交互(exec/attach/port-forward)。容器运行时能够同时管理镜像和容器（例如：Docker和Rkt），并且可以通过同一个套接字提供这两种服务。在Kubelet中，这个套接字通过–container-runtime-endpoint和–image-service-endpoint字段进行设置。Kubernetes CRI支持的容器运行时包括docker、rkt、cri-o、frankti、kata-containers和clear-containers等。\nProxy Proxy 是为了解决外部网络能够访问跨机器集群中容器提供的应用服务而设计的，从上图可知 Proxy 服务也运行在每个 Minion 上。Proxy 提供 TCP/UDP sockets 的 proxy，每创建一种 Service，Proxy 主要从 etcd 获取 Services 和 Endpoints 的配置信息，或者也可以从 file 获取，然后根据配置信息在 Minion 上启动一个 Proxy 的进程并监听相应的服务端口，当外部请求发生时，Proxy 会根据 Load Balancer 将请求分发到后端正确的容器处理。\n服务发现主要通过DNS实现。\n在Kubernetes中，kube proxy负责为Pod创建代理服务；引到访问至服务；并实现服务到Pod的路由和转发，以及通过应用的负载均衡。\nProxy不但解决了同一主宿机相同服务端口冲突的问题，还提供了Service转发服务端口对外提供服务的能力，Proxy后端使用了随机、轮循负载均衡算法。\n分层架构 Kubernetes设计理念和功能其实就是一个类似Linux的分层架构，如下图所示\n 核心层：Kubernetes最核心的功能，对外提供API构建高层的应用，对内提供插件式应用执行环境 应用层：部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS解析等） 管理层：系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态Provision等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy等） 接口层：kubectl命令行工具、客户端SDK以及集群联邦 生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴 Kubernetes外部：日志、监控、配置管理、CI、CD、Workflow、FaaS、OTS应用、ChatOps等 Kubernetes内部：CRI、CNI、CVI、镜像仓库、Cloud Provider、集群自身的配置和管理等  系统流程 一位大牛整理的 K8S 调用流程\n设计理念  声明式 VS 命令式, 声明式优点很多，一个很重要的点是：在分布式系统中，任何组件都可能随时出现故障。当组件恢复时，需要弄清楚要做什么，使用命令式 API 时，处理起来就很棘手。但是使用声明式 API ，组件只需查看 API 服务器的当前状态，即可确定它需要执行的操作。\n 显式的 API, Kubernetes 是透明的，它没有隐藏的内部 API。换句话说 Kubernetes 系统内部用来交互的 API 和我们用来与 Kubernetes 交互的 API 相同。这样做的好处是，当 Kubernetes 默认的组件无法满足我们的需求时，我们可以利用已有的 API 实现我们自定义的特性。\n 无侵入性, 感谢 Docker 容器技术的流行，使得 Kubernetes 为大家提供了无缝的使用方式。我们的应用达到镜像后, 不需要改动就可以遨游在 Kubernetes 集群中。Kubernetes 还提供存储 Secret、Configuration 等包含但不局限于密码、证书、容器镜像信息、应用启动参数能力。如此，Kubernetes 以一种友好的方式将这些东西注入 Pod，减少了大家的工作量，而无需重写或者很大幅度改变原有的应用代码。\n 为了实现这一目标，Kubernetes 引入了 PersistentVolumeClaim（PVC）和 PersistentVolume（PV）API 对象。这些对象将存储实现与存储使用分离。 PersistentVolumeClaim 对象用作用户以与实现无关的方式请求存储的方法，通过它来抹除对底层 PersistentVolume 的差异性。这样就使 Kubernetes 拥有了跨集群的移植能力。\n  容器编排系统的比较 在 Google 的一篇关于内部 Omega 调度系统的论文中，将调度系统分成三类：单体、二层调度和共享状态三种，按照它的分类方法，通常Google的 Borg被分到单体这一类，Mesos被当做二层调度，而Google自己的Omega被当做第三类“共享状态”。\n因为Kubernetes的大部分设计是延续 Borg的，而且Kubernetes的核心组件（Controller Manager和Scheduler）缺省也都是绑定部署在一起，状态也都是存储在ETCD里面的，所以通常大家会把Kubernetes也当做“单体”调度系统，我觉得 Kubernetes 的调度模型也完全是二层调度的，和 Mesos 一样，任务调度和资源的调度是完全分离的，Controller Manager承担任务调度的职责，而Scheduler则承担资源调度的职责。\nMesos 实际上Kubernetes和Mesos调度的最大区别在于资源调度请求的方式：\n主动 Push 方式。是 Mesos 采用的方式，就是 Mesos 的资源调度组件（Mesos Master）主动推送资源 Offer 给 Framework，Framework 不能主动请求资源，只能根据 Offer 的信息来决定接受或者拒绝。\n被动 Pull 方式。是 Kubernetes 的方式，资源调度组件 Scheduler 被动的响应 Controller Manager的资源请求。\n这两种方式带来的不同，我主要从一下 5 个方面来分析。另外注意，我所比较两者的优劣，都是从理论上做的分析，工程实现上会有差异，一些指标我也并没有实际测试过。\n1.资源利用率：Kubernetes 胜出\n理论上，Kubernetes 应该能实现更加高效的集群资源利用率，原因资源调度的职责完全是由Scheduler一个组件来完成的，它有充足的信息能够从全局来调配资源，然后而Mesos 却做不到，因为资源调度的职责被切分到Framework和Mesos Master两个组件上，Framework 在挑选 Offer 的时候，完全没有其他 Framework 工作负载的信息，所以也不可能做出最优的决策。\n举个例子，比如我们希望把对耗费 CPU的工作负载和耗费内存的工作负载尽可能调度到同一台主机上，在Mesos里面不太容易做到，因为他们分属不同的 Framework。\n2.扩展性：Mesos胜出\n从理论上讲，Mesos 的扩展性要更好一点。原因是Mesos的资源调度方式更容易让已经存在的任务调度迁移上来。举个例子，假设已经有了一个任务调度系统，比如 Spark，现在要迁移到集群调度平台上，理论上它迁移到 Mesos 比 Kubernetes 上更加容易。\n如果迁移到 Mesos ，没有改变原来的工作流程和逻辑，原来的逻辑是：来了一个作业请求，调度系统把任务拆分成小的任务，然后从资源池里面挑选一个节点来运行任务，并且记录挑选的节点 IP 和端口号，用来跟踪任务的状态。迁移到 Mesos 之后，还是一样的逻辑，唯一需要变化的是那个资源池，原来是自己管理的资源池，现在变成 Mesos 提供的Offer 列表。\n如果迁移到 Kubernetes，则需要修改原来的基本逻辑来适配 Kubernetes，资源的调度完全需要调用外部的组件来完成，并且这个变成异步的。\n3.灵活的任务调度策略：Mesos 胜出\nMesos 对各种任务的调度策略也支持的更好。举个例子，如果某一个作业，需要 All or Nothing 的策略，Mesos 是能够实现的，但是 Kubernetes 完全无法支持。所以All or Nothing 的意思是，价格整个作业如果需要运行 10 个任务，这 10个任务需要能够全部有资源开始执行，否则就一个都不执行。\n4.性能：Mesos 胜出\nMesos 的性能应该更好，因为资源调度组件，也就是 Mesos Master 把一部分资源调度的工作甩给 Framework了，承担的调度工作更加简单，从数据来看也是这样，在多年之前 Twitter 自己的 Mesos 集群就能够管理超过 8万个节点，而 Kubernetes 1.3 只能支持 5千个节点。\n5.调度延迟：Kubernetes 胜出\nKubernetes调度延迟会更好。因为Mesos的轮流给Framework提供Offer机制，导致会浪费很多时间在给不需要资源的 Framework 提供Offer。\nSwarm Kubernetes的优势 Kubernetes 作为容器编排的事实标准，主要强在以下方面：\n 理念的先进性 Kubernetes 并未将 Docker 作为架构的核心，而仅仅将它作为一个container runtime 实现，k8s的核心是cni、csi、cri、oci等。相对的，mesos是docker的使用者，也必然是docker特性的迁就者。K8S 架构有很强的扩展性，而Mesos则需要考虑 Docker 的支持程度。\n 声明式 API k8s系统的梳理了任务的形态以及任务之间的关系，并为未来留有余地，提供了声明式的 API。\n  云计算平台上的开发者们所关心的，并不是调度，也不是资源管管理，更不是网络或者存储，他们关心的只有一件事，那就是 Kubernetes 的 API，也就是声明式 API 和控制器模式。这个 API 独有的编程范式，即 Controller 和 Operator。作为一个云计算平台的用户，能够用一个 YAML 文件表达我开发的应用的最终运行状态，并且自动地对我的应用进行运维和管理。这种信赖关系，就是连接Kubernetes 项目和开发者们最重要的纽带。\n不同于一个只能生产资源的集群管理工具，Kubernetes 项目最大的价值，乃在于它从一开始就提倡的声明式 API 和以此为基础“控制器”模式。Kubernetes 项目为使用者提供了宝贵的 API 可扩展能力和良好的 API 编程范式，催生出了一个完全基于 Kubernetes API 构建出来的上层应用服务生态。可以说，正是这个生态的逐步完善与日趋成熟，才确立了 Kubernetes 项目如今在云平台领域牢不可破的领导地位，也间接宣告了竞品方案的边缘化。\n未来：应用交付的革命不会停止，Kubernetes 项目一直在做的，其实是在进一步清晰和明确“应用交付”这个亘古不变的话题。只不过，相比于交付一个容器和容器镜像， Kubernetes 项目正在尝试明确的定义云时代“应用”的概念。在这里，应用是一组容器的有机组合，同时也包括了应用运行所需的网络、存储的需求的描述。而像这样一个“描述”应用的 YAML 文件，放在 etcd 里存起来，然后通过控制器模型驱动整个基础设施的状态不断地向用户声明的状态逼近，就是 Kubernetes 的核心工作原理了。PS: 以后你给公有云一个yaml 文件就可以发布自己的应用了。\n横向扩展 几乎所有的集群调度系统都无法横向扩展（Scale Out），Mesos通过优化，一个集群能够管理 8 万个节点，Kubernetes 最新的1.15版本，集群管理节点的上限是 5000 个节点。\n所有的集群调度系统的架构都是无法横向扩展的，如果需要管理更多的服务器，唯一的办法就是创建多个集群。集群调度系统的架构看起来都是这个样子的：\n中间的 Scheduler（资源调度器）是最核心的组件，虽然通常是由多个（通常是3个）实例组成，但是都是单活的，也就是说只有一个节点工作，其他节点都处于 Standby 的状态。\n这是因为集群调度系统的“独立资源池”数量是 1，每一台服务器节点都是一个资源，每当资源消费者请求资源的时候，调度系统用来做调度算法的“独立资源池”是多大呢？答案应该是整个集群的资源组成的资源池，没有办法在切分了，因为:\n 调度系统的职责就是要在全局内找到最优的资源匹配。 另外，哪怕不需要找到最优的资源匹配，资源调度器对每一次资源请求，也没办法判断应该从哪一部分资源池中挑选资源。  正是因为这个原因，“独立资源池”数量是 1，所以集群调度系统无法做到横向扩展。\n优化 主要是 Scheduler 调度器的优化，主要体现在两个地方：\n 预选失败中断机制  一次调度过程在判断一个Node是否可作为目标机器主要分为三个阶段：\n预选阶段：硬性条件，过滤掉不满足条件的节点，这个过程称为Predicates。这是固定先后顺序的一系列过滤条件，任何一个predicate不符合则放弃该Node。\n优选阶段：软性条件，对通过的节点按照优先级排序，称之为Priorities。每一个priority都是一个影响因素，都有一定的权重。\n选定阶段：从优选列表中选择优先级最高的节点，称为Select。选择的Node即为最终部署Pod的机器。\n通过深入分析调度过程我们发现，调度器在预选阶段即使已经知道当前Node不符合某个过滤条件仍然会继续判断后续的过滤条件是否符合。试想如果有上万台Node节点，这些判断逻辑会浪费很多计算时间，也是调度器性能低下的一个重要因素。\n改进为“预选失败中断机制”，即一旦某个预选条件不满足，那么该Node即被立即放弃，后面的预选条件不再做判断计算，从而大大减少了计算量，调度性能也大大提升。如下图：\n 局部最优解  对于优化问题，尤其是最优化问题，总是希望找到全局最优的解或策略，但是当问题的复杂度过于高，要考虑的因素和处理的信息量过多的时候，我们往往会倾向于接受局部最优解，因为局部最优解的质量不一定都是差的。尤其是当我们有确定的评判标准标明得出的解是可以接受的话，通常会接收局部最优的结果。这样，从成本、效率等多方面考虑，才是实际工程中会采取的策略。\n当前调度策略中，每次调度调度器都会遍历集群中所有的Node，以便找出最优的节点，这在调度领域我们称之为BestFit算法，但是在生产环境中，我们是选取最优Node还是次优的Node其实并没有特别大的区别和影响，有时候我们还是避免每次选取最优的Node(例如我们集群为了解决新上线机器后狂在该机器上创建应用的问题就将最优解随机化)。换句话说，我们找出局部最优解就能满足我们的需求。\n假设集群一共1000个Node，一次调度过程PodA，这其中有700个Node都能通过Predicates(预选阶段)，那么就会把所有的Node遍历并找出这700个node，然后经过得分排序找出最优的Node节点NodeX；但是采用局部最优算法，即我们认为只要能找出N个Node，并在这N个Node中选择得分最高的Node即能满足我们的需求，比如我们默认找出100个可以通过Predicates(预选阶段)的Node即可，我们的最优解就在这100个Node中选择，当然全局最优解NodeX可能在也可能不在这100个Node中，但是我们在这100个Node中选择最优的NodeY也能满足我们的要求。最好的情况下我们在遍历100个Node就找出了这100个Node，也可能遍历了200个或者300个Node等等，这样我们可以大大减少计算时间，同时也不会对我们的调度结果产生太大的影响。\n参考 K8S 设计理念\nKubernetes整体结构\n","date":1542672000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542672000,"objectID":"04020d602faf16fa8cc35d08ca7b357c","permalink":"/post/cloud/k8s/201811-k8s-arch/","publishdate":"2018-11-20T00:00:00Z","relpermalink":"/post/cloud/k8s/201811-k8s-arch/","section":"post","summary":"Kubernetes 云计算操作系统","tags":["Kubernetes"],"title":"Kubernetes 框架","type":"post"},{"authors":null,"categories":null,"content":"支持一下功能：\n 照镜子 语音交互 天气预报 室内温湿度 新闻资讯 股票信息 支持定制更多功能  ","date":1535328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535328000,"objectID":"4271ecf2381076959da698bfcf06917c","permalink":"/project/mirror-project/","publishdate":"2018-08-27T00:00:00Z","relpermalink":"/project/mirror-project/","section":"project","summary":"这是一面高颜值、高智商、会说话的镜子","tags":["MagicMirror"],"title":"魔镜","type":"project"},{"authors":null,"categories":null,"content":"周期性获取火币网的多种交易对行情，可以方便的新增交易对、自定义行情周期，获取的结果存储到 MySQL 中。\n","date":1532649600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532649600,"objectID":"15169f7bb87fa24153ed41a267828478","permalink":"/project/blockchain-project/","publishdate":"2018-07-27T00:00:00Z","relpermalink":"/project/blockchain-project/","section":"project","summary":"获取火币网交易对行情并存储到数据库中。","tags":["BlockChain"],"title":"数字币交易所行情","type":"project"},{"authors":null,"categories":null,"content":" 参考 https://cizixs.com/2018/06/25/kubernetes-resource-management/\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8a44249f398b4faebb7fe6dd615e2552","permalink":"/post/cloud/k8s/201812-k8s-resource-management/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/cloud/k8s/201812-k8s-resource-management/","section":"post","summary":"参考 https://cizixs.com/2018/06/25/kubernetes-resource-management/","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":" 以阅读 K8S 一个模块 kube-proxy 来讲讲我是如何阅读源码的\n架构 问题  Proxy 如何解决了同一主宿机相同服务端口冲突的问题？  参考 我是怎么阅读kubernetes源代码的？\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1801322119fd2ed28aed808b26e7e68e","permalink":"/post/cloud/k8s/201901-how-read-kubeproxy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/cloud/k8s/201901-how-read-kubeproxy/","section":"post","summary":"以阅读 K8S 一个模块 kube-proxy 来讲讲我是如何阅读源码的\n架构 问题  Proxy 如何解决了同一主宿机相同服务端口冲突的问题？  参考 我是怎么阅读kubernetes源代码的？","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":" 服务发现 Kubernetes 支持2种基本的服务发现模式 —— 环境变量和 DNS。\n环境变量 当 Pod 运行在 Node 上，kubelet 会为每个活跃的 Service 添加一组环境变量。 它同时支持 Docker links 兼容 变量（查看 makeLinkVariables）、简单的 {SVCNAME}_SERVICE_HOST 和 {SVCNAME}_SERVICE_PORT 变量，这里 Service 的名称需大写，横线被转换成下划线。\n举个例子，一个名称为 \u0026ldquo;redis-master\u0026rdquo; 的 Service 暴露了 TCP 端口 6379，同时给它分配了 Cluster IP 地址 10.0.0.11，这个 Service 生成了如下环境变量：\n服务发布 对一些应用（如 Frontend）的某些部分，可能希望通过外部（Kubernetes 集群外部）IP 地址暴露 Service。\nKubernetes ServiceTypes 允许指定一个需要的类型的 Service，默认是 ClusterIP 类型。\nType 的取值以及行为如下：\n ClusterIP：通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问，这也是默认的 ServiceType。 NodePort：通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务。NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 :，可以从集群的外部访问一个 NodePort 服务。 LoadBalancer：使用云提供商的负载均衡器，可以向外部暴露服务。外部的负载均衡器可以路由到 NodePort 服务和 ClusterIP 服务。 ExternalName：通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容（例如， foo.bar.example.com）。 没有任何类型代理被创建，这只有 Kubernetes 1.7 或更高版本的 kube-dns 才支持。  NodePort 类型 如果设置 type 的值为 \u0026ldquo;NodePort\u0026rdquo;，Kubernetes master 将从给定的配置范围内（默认：30000-32767）分配端口，每个 Node 将从该端口（每个 Node 上的同一端口）代理到 Service。该端口将通过 Service 的 spec.ports[*].nodePort 字段被指定。\n如果需要指定的端口号，可以配置 nodePort 的值，系统将分配这个端口，否则调用 API 将会失败（比如，需要关心端口冲突的可能性）。\n这可以让开发人员自由地安装他们自己的负载均衡器，并配置 Kubernetes 不能完全支持的环境参数，或者直接暴露一个或多个 Node 的 IP 地址。\n需要注意的是，Service 将能够通过 \u0026lt;NodeIP\u0026gt;:spec.ports[*].nodePort 和 spec.clusterIp:spec.ports[*].port 而对外可见。\nLoadBalancer 类型 使用支持外部负载均衡器的云提供商的服务，设置 type 的值为 \u0026ldquo;LoadBalancer\u0026rdquo;，将为 Service 提供负载均衡器。 负载均衡器是异步创建的，关于被提供的负载均衡器的信息将会通过 Service 的 status.loadBalancer 字段被发布出去。\nkind: Service apiVersion: v1 metadata: name: my-service spec: selector: app: MyApp ports: - protocol: TCP port: 80 targetPort: 9376 nodePort: 30061 clusterIP: 10.0.171.239 loadBalancerIP: 78.11.24.19 type: LoadBalancer status: loadBalancer: ingress: - ip: 146.148.47.155  来自外部负载均衡器的流量将直接打到 backend Pod 上，不过实际它们是如何工作的，这要依赖于云提供商。 在这些情况下，将根据用户设置的 loadBalancerIP 来创建负载均衡器。 某些云提供商允许设置 loadBalancerIP。如果没有设置 loadBalancerIP，将会给负载均衡器指派一个临时 IP。 如果设置了 loadBalancerIP，但云提供商并不支持这种特性，那么设置的 loadBalancerIP 值将会被忽略掉。\n外部 IP 如果外部的 IP 路由到集群中一个或多个 Node 上，Kubernetes Service 会被暴露给这些 externalIPs。 通过外部 IP（作为目的 IP 地址）进入到集群，打到 Service 的端口上的流量，将会被路由到 Service 的 Endpoint 上。 externalIPs 不会被 Kubernetes 管理，它属于集群管理员的职责范畴。\n根据 Service 的规定，externalIPs 可以同任意的 ServiceType 来一起指定。 在下面的例子中，my-service 可以在 80.11.12.10:80（外部 IP:端口）上被客户端访问。\nkind: Service apiVersion: v1 metadata: name: my-service spec: selector: app: MyApp ports: - name: http protocol: TCP port: 80 targetPort: 9376 externalIPs: - 80.11.12.10  参考 https://jimmysong.io/kubernetes-handbook/concepts/service.html\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2fceccb635675ed7b7632d4db67d86f0","permalink":"/post/cloud/k8s/201903-k8s-service-register.md/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/cloud/k8s/201903-k8s-service-register.md/","section":"post","summary":"服务发现 Kubernetes 支持2种基本的服务发现模式 —— 环境变量和 DNS。\n环境变量 当 Pod 运行在 Node 上，kubelet 会为每个活跃的 Service 添加一组环境变量。 它同时支持 Docker links 兼容 变量（查看 makeLinkVariables）、简单的 {SVCNAME}_SERVICE_HOST 和 {SVCNAME}_SERVICE_PORT 变量，这里 Service 的名称需大写，横线被转换成下划线。\n举个例子，一个名称为 \u0026ldquo;redis-master\u0026rdquo; 的 Service 暴露了 TCP 端口 6379，同时给它分配了 Cluster IP 地址 10.0.0.11，这个 Service 生成了如下环境变量：\n服务发布 对一些应用（如 Frontend）的某些部分，可能希望通过外部（Kubernetes 集群外部）IP 地址暴露 Service。\nKubernetes ServiceTypes 允许指定一个需要的类型的 Service，默认是 ClusterIP 类型。\nType 的取值以及行为如下：\n ClusterIP：通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问，这也是默认的 ServiceType。 NodePort：通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务。NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 :，可以从集群的外部访问一个 NodePort 服务。 LoadBalancer：使用云提供商的负载均衡器，可以向外部暴露服务。外部的负载均衡器可以路由到 NodePort 服务和 ClusterIP 服务。 ExternalName：通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容（例如， foo.","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":" 负载均衡的概念 负载均衡是将工作负载分布到多个服务器来提高网站、应用、数据库，来提高服务的性能和可靠性，是高可用网络基础架构的关键组件。\n图1，负载均衡示意图\n上图看到用户访问App时会先经过负载均衡（Load Balancer），再由负载均衡器将请求转发给后端服务器上的应用程序。\n负载均衡主要分为两种：四层和七层负载均衡。\n 四层负载均衡\n基于IP+端口的负载均衡，即在OSI第4层工作，就是通过发布三层的IP地址（VIP），然后加四层的端口号，来决定哪些流量需要做负载均衡，对需要处理的流量进行NAT处理，转发至后台服务器，并记录下这个TCP或者UDP的流量是由哪台服务器处理的，后续这个连接的所有流量都同样转发到同一台服务器处理。这种Load Balance不理解应用协议（如HTTP/FTP/MySQL等等）。\n 七层负载均衡\n工作在OSI的最高层，第7层应用层，就是在四层的基础上，再考虑应用层的特征，比如同一个Web服务器的负载均衡，除了根据VIP加80端口辨别是否需要处理的流量，还可根据七层的URL、浏览器类别、语言来决定是否要进行负载均衡。举个例子，如果你的Web服务器分成两组，一组是中文语言的，一组是英文语言的，那么七层负载均衡就可以当用户来访问你的域名时，自动辨别用户语言，然后选择对应的语言服务器组进行负载均衡处理。\n  图2，四层和七层负载均衡的区别\n四层负载均衡更像是路由器的工作方式，修改IP后转发即可；七层负载均衡需要获取到应用层信息，就需要与客户端建立连接并且解析应用层的内容，同时与服务端建立连接，将请求发送到对应的后端服务上。\nK8S 负载均衡 现在的后端服务从物理机向容器的运行环境转移，之前物理机的负载均衡已经不适用基于Kubernetes的容器网络了，Kubernetes也需要支持更加灵活和特性广泛的负载均衡。\n云环境下后端服务是运行在容器里的，容器是用Kubernetes通过Pod来编排和管理的，Pod是一组功能密切相关的容器的合集，提供一种服务。\nK8S 的负载均衡主要分为两个层面： - 集群内部层面，包括资源分配、内部流量的访问 - 外部访问集群\n调度层面 分配Pod到Node时就会考虑将一个Service的Pod打散分布，根据资源的容量和性能来部署Pod。作为调度过程的一部分，它需要充分考虑管理可用性，避免遇到资源瓶颈。达到整体运行良好的运行状况。\nService 我们知道 Pod 的生命周期是短暂的，会因为一些原因而被销毁而在别的 Node 重新创建，因此 Pod 的 IP 地址会变化，而其他服务无法知道新的 IP 地址，这就需要定义一个 Service 暴露给其他服务使用，一个 Service 可以有一个或多个 Pod 逻辑分组的抽象，这样前端应用程序向 Service IP 发起请求就可以了，而不需要关心具体的 Pod 的状态和 IP。Service IP 是固定不变的。\nService通过标签来选取服务后端，一般配合Replication Controller或者Deployment来保证后端容器的正常运行。\nService有三种类型：\n ClusterIP：默认类型，自动分配一个仅cluster内部可以访问的虚拟IP NodePort：在ClusterIP基础上为Service在每台机器上绑定一个端口，这样就可以通过:NodePort来访问该服务 LoadBalancer：在NodePort的基础上，借助cloud provider创建一个外部的负载均衡器，并将请求转发到:NodePort  另外，也可以将已有的服务以Service的形式加入到Kubernetes集群中来，只需要在创建Service的时候不指定Label selector，而是在Service创建好后手动为其添加endpoint。\nService 通过 VIP 实现的是一个4层负载均衡的功能。\nService 定义 一个 Service 在 Kubernetes 中是一个 REST 对象，和 Pod 类似。 像所有的 REST 对象一样， Service 定义可以基于 POST 方式，请求 apiserver 创建新的实例。 例如，假定有一组 Pod，它们对外暴露了 9376 端口，同时还被打上 \u0026ldquo;app=MyApp\u0026rdquo; 标签。\nkind: Service apiVersion: v1 metadata: name: my-service spec: selector: app: MyApp ports: - protocol: TCP port: 80 targetPort: 9376  上述配置将创建一个名称为 “my-service” 的 Service 对象，它会将请求代理到使用 TCP 端口 9376，并且具有标签 \u0026ldquo;app=MyApp\u0026rdquo; 的 Pod 上。 这个 Service 将被指派一个 IP 地址（通常称为 “Cluster IP”），它会被服务的代理使用（见下面）。 该 Service 的 selector 将会持续评估，处理结果将被 POST 到一个名称为 “my-service” 的 Endpoints 对象上。\n需要注意的是， Service 能够将一个接收端口映射到任意的 targetPort。 默认情况下，targetPort 将被设置为与 port 字段相同的值。 可能更有趣的是，targetPort 可以是一个字符串，引用了 backend Pod 的一个端口的名称。 但是，实际指派给该端口名称的端口号，在每个 backend Pod 中可能并不相同。 对于部署和设计 Service ，这种方式会提供更大的灵活性。 例如，可以在 backend 软件下一个版本中，修改 Pod 暴露的端口，并不会中断客户端的调用。\n Kubernetes Service 能够支持 TCP 和 UDP 协议，默认 TCP 协议。\n kube-proxy Kubernetes中通过Node上的kube-proxy实现负载分配，只能提供本地 Node 上 Pod 的分担，kube-proxy 负责为 Service 实现了一种 VIP（虚拟 IP）的形式。主要有三种实现方式：\n userspace，最开始的实现方式 iptables，目前的默认方式 ipvs，   userspace 代理模式\n该模式下kube-proxy会为每一个Service创建一个监听端口。发向Cluster IP的请求被Iptables规则重定向到Kube-proxy监听的端口上，Kube-proxy根据LB算法选择一个提供服务的Pod并和其建立链接，以将请求转发到Pod上。\n该模式下，Kube-proxy充当了一个四层Load balancer的角色。由于kube-proxy运行在userspace中，在进行转发处理时会增加两次内核和用户空间之间的数据拷贝，效率较另外两种模式低一些；好处是当后端的Pod不可用时，kube-proxy可以重试其他Pod。\n默认的策略是，通过 round-robin 算法来选择 backend Pod。 实现基于客户端 IP 的会话亲和性，可以通过设置 service.spec.sessionAffinity 的值为 \u0026ldquo;ClientIP\u0026rdquo; （默认值为 \u0026ldquo;None\u0026rdquo;）。\n图片 - userspace代理模式下Service概览图\n作为一个例子，考虑前面提到的图片处理应用程序。 当创建 backend Service 时，Kubernetes master 会给它指派一个虚拟 IP 地址，比如 10.0.0.1。 假设 Service 的端口是 1234，该 Service 会被集群中所有的 kube-proxy 实例观察到。 当代理看到一个新的 Service， 它会打开一个新的端口，建立一个从该 VIP 重定向到新端口的 iptables，并开始接收请求连接。\n当一个客户端连接到一个 VIP，iptables 规则开始起作用，它会重定向该数据包到 Service代理 的端口。 Service代理 选择一个 backend，并将客户端的流量代理到 backend 上。\n这意味着 Service 的所有者能够选择任何他们想使用的端口，而不存在冲突的风险。 客户端可以简单地连接到一个 IP 和端口，而不需要知道实际访问了哪些 Pod。\n iptables 代理模式\n为了避免增加内核和用户空间的数据拷贝操作，提高转发效率，Kube-proxy提供了iptables模式。在该模式下，Kube-proxy为service后端的每个Pod创建对应的iptables规则，直接将发向Cluster IP的请求重定向到一个Pod IP。\n该模式下Kube-proxy不承担四层代理的角色，只负责创建iptables规则。该模式的优点是较userspace模式效率更高，但不能提供灵活的LB策略，当后端Pod不可用时也无法进行重试。\n图片 - iptables代理模式下Service概览图\n再次考虑前面提到的图片处理应用程序。 当创建 backend Service 时，Kubernetes master 会给它指派一个虚拟 IP 地址，比如 10.0.0.1。 假设 Service 的端口是 1234，该 Service 会被集群中所有的 kube-proxy 实例观察到。 当代理看到一个新的 Service， 它会安装一系列的 iptables 规则，从 VIP 重定向到 per-Service 规则。 该 per-Service 规则连接到 per-Endpoint 规则，该 per-Endpoint 规则会重定向（目标 NAT）到 backend。\n当一个客户端连接到一个 VIP，iptables 规则开始起作用。一个 backend 会被选择（或者根据会话亲和性，或者随机），数据包被重定向到这个 backend。 不像 userspace 代理，数据包从来不拷贝到用户空间，kube-proxy 不是必须为该 VIP 工作而运行，并且客户端 IP 是不可更改的。 当流量打到 Node 的端口上，或通过负载均衡器，会执行相同的基本流程，但是在那些案例中客户端 IP 是可以更改的。\n ipvs 代理模式\n该模式和iptables类似，但是需要内核支持 ipvs，kube-proxy监控Pod的变化并创建相应的ipvs rules。ipvs也是在kernel模式下通过netfilter实现的，但采用了hash table来存储规则，因此在规则较多的情况下，Ipvs相对iptables转发效率更高。除此以外，ipvs支持更多的LB算法，例如：\n rr：轮询调度 lc：最小连接数 dh：目标哈希 sh：源哈希 sed：最短期望延迟 nq： 不排队调度   注意： ipvs模式假定在运行kube-proxy之前在节点上都已经安装了IPVS内核模块。当kube-proxy以ipvs代理模式启动时，kube-proxy将验证节点上是否安装了IPVS模块，如果未安装，则kube-proxy将回退到iptables代理模式。\n 图片 - ipvs代理模式下Service概览图\n  缺点 为 VIP 使用 userspace 代理，将只适合小型到中型规模的集群，不能够扩展到上千 Service 的大型集群。 查看 最初设计方案 获取更多细节。\n使用 userspace 代理，隐藏了访问 Service 的数据包的源 IP 地址。 这使得一些类型的防火墙无法起作用。 iptables 代理不会隐藏 Kubernetes 集群内部的 IP 地址，但却要求客户端请求必须通过一个负载均衡器或 Node 端口。\n todo: userspace会引起源 IP 的修改？为啥 iptables 代理需要配置 loadbalancer 或者 NodePort？\n Type 字段支持嵌套功能 —— 每一层需要添加到上一层里面。 不会严格要求所有云提供商（例如，GCE 就没必要为了使一个 LoadBalancer 能工作而分配一个 NodePort，但是 AWS 需要 ），但当前 API 是强制要求的。\nIngress Ingress 是从Kubernetes集群外部访问集群内部服务的入口，service 和 pod 仅可在集群内部网络中通过 IP 地址访问。所有到达边界路由器的流量或被丢弃或被转发到其他地方。从概念上讲，可能像下面这样：\n internet | ------------ [ Services ]  Ingress是授权入站连接到达集群服务的规则集合。\n internet | [ Ingress ] --|-----|-- [ Services ]  你可以给Ingress配置提供外部可访问的URL、负载均衡、SSL、基于名称的虚拟主机等。用户通过POST Ingress资源到API server的方式来请求ingress。 Ingress controller负责实现Ingress，通常使用负载平衡器，它还可以配置边界路由和其他前端，这有助于以HA方式处理流量。\nIngress Resource 最简化的Ingress配置：\n1: apiVersion: extensions/v1beta1 2: kind: Ingress 3: metadata: 4: name: test-ingress 5: spec: 6: rules: 7: - http: 8: paths: 9: - path: /testpath 10: backend: 11: serviceName: test 12: servicePort: 80   如果你没有配置Ingress controller就将其POST到API server不会有任何用处\n 配置说明\n1-4行：跟Kubernetes的其他配置一样，ingress的配置也需要apiVersion，kind和metadata字段。配置文件的详细说明请查看部署应用, 配置容器和 使用resources.\n5-7行: Ingress spec 中包含配置一个loadbalancer或proxy server的所有信息。最重要的是，它包含了一个匹配所有入站请求的规则列表。目前ingress只支持http规则。\n8-9行：每条http规则包含以下信息：一个host配置项（比如for.bar.com，在这个例子中默认是*），path列表（比如：/testpath），每个path都关联一个backend(比如test:80)。在loadbalancer将流量转发到backend之前，所有的入站请求都要先匹配host和path。\n10-12行：正如 services doc中描述的那样，backend是一个service:port的组合。Ingress的流量被转发到它所匹配的backend。\n全局参数：为了简单起见，Ingress示例中没有全局参数，请参阅资源完整定义的api参考。 在所有请求都不能跟spec中的path匹配的情况下，请求被发送到Ingress controller的默认后端，可以指定全局缺省backend。\nIngress controllers 为了使Ingress正常工作，集群中必须运行Ingress controller。 这与其他类型的控制器不同，其他类型的控制器通常作为kube-controller-manager二进制文件的一部分运行，在集群启动时自动启动。 你需要选择最适合自己集群的Ingress controller或者自己实现一个。\n kubernetes当前支持并维护GCE和nginx两种controller. F5（公司）支持并维护 F5 BIG-IP Controller for Kubernetes. Kong 同时支持并维护社区版与企业版的 Kong Ingress Controller for Kubernetes. Traefik 是功能齐全的 ingress controller(Let’s Encrypt, secrets, http2, websocket…), Containous 也对其提供商业支持。 Istio 使用CRD Gateway来控制Ingress流量。  Ingress类型 单Service Ingress Kubernetes中已经存在一些概念可以暴露单个service（查看替代方案），但是你仍然可以通过Ingress来实现，通过指定一个没有rule的默认backend的方式。\ningress.yaml定义文件：\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: test-ingress spec: backend: serviceName: testsvc servicePort: 80  使用kubectl create -f命令创建，然后查看ingress：\n$ kubectl get ing NAME RULE BACKEND ADDRESS test-ingress - testsvc:80 107.178.254.228  107.178.254.228就是Ingress controller为了实现Ingress而分配的IP地址。RULE列表示所有发送给该IP的流量都被转发到了BACKEND所列的Kubernetes service上。\nloadbalancer\n如前面描述的那样，kubernetes pod中的IP只在集群网络内部可见，我们需要在边界设置一个东西，让它能够接收ingress的流量并将它们转发到正确的端点上。这个东西一般是高可用的loadbalancer。使用Ingress能够允许你将loadbalancer的个数降低到最少，例如，假如你想要创建这样的一个设置：\nfoo.bar.com -\u0026gt; 178.91.123.132 -\u0026gt; / foo s1:80 / bar s2:80  你需要一个这样的ingress：\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: test spec: rules: - host: foo.bar.com http: paths: - path: /foo backend: serviceName: s1 servicePort: 80 - path: /bar backend: serviceName: s2 servicePort: 80  使用kubectl create -f创建完ingress后：\n$ kubectl get ing NAME RULE BACKEND ADDRESS test - foo.bar.com /foo s1:80 /bar s2:80  只要服务（s1，s2）存在，Ingress controller就会将提供一个满足该Ingress的特定loadbalancer实现。 这一步完成后，您将在Ingress的最后一列看到loadbalancer的地址。\n基于名称的虚拟主机 Name-based的虚拟主机在同一个IP地址下拥有多个主机名。\nfoo.bar.com --| |-\u0026gt; foo.bar.com s1:80 | 178.91.123.132 | bar.foo.com --| |-\u0026gt; bar.foo.com s2:80  下面这个ingress说明基于Host header的后端loadbalancer的路由请求：\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: test spec: rules: - host: foo.bar.com http: paths: - backend: serviceName: s1 servicePort: 80 - host: bar.foo.com http: paths: - backend: serviceName: s2 servicePort: 80  默认backend\n一个没有rule的ingress，如前面章节中所示，所有流量都将发送到一个默认backend。你可以用该技巧通知loadbalancer如何找到你网站的404页面，通过制定一些列rule和一个默认backend的方式。如果请求header中的host不能跟ingress中的host匹配，并且/或请求的URL不能与任何一个path匹配，则流量将路由到你的默认backend。\nIstio Istio Sidecar Proxy Cluster IP解决了服务之间相互访问的问题，但从上面Kube-proxy的三种模式可以看到，Cluster IP的方式只提供了服务发现和基本的LB功能。如果要为服务间的通信应用灵活的路由规则以及提供Metrics collection，distributed tracing等服务管控功能,就必须得依靠Istio提供的服务网格能力了。\n在Kubernetes中部署Istio后，Istio通过iptables和Sidecar Proxy接管服务之间的通信，服务间的相互通信不再通过Kube-proxy，而是通过Istio的Sidecar Proxy进行。请求流程是这样的：Client发起的请求被iptables重定向到Sidecar Proxy，Sidecar Proxy根据从控制面获取的服务发现信息和路由规则，选择一个后端的Server Pod创建链接，代理并转发Client的请求。\nIstio Sidecar Proxy和Kube-proxy的userspace模式的工作机制类似，都是通过在用户空间的一个代理来实现客户端请求的转发和后端多个Pod之间的负载均衡。两者的不同点是：Kube-Proxy工作在四层，而Sidecar Proxy则是一个七层代理，可以针对HTTP，GRPS等应用层的语义进行处理和转发，因此功能更为强大，可以配合控制面实现更为灵活的路由规则和服务管控功能。\n图片 - Istio Sidecar Proxy 流量转发模型\ngRPC负载均衡 参考 grpc-load-balancing-on-kubernetes-without-tears\nService\nistio 控制 Ingress 流量\n问题  Service 的 IP 地址如何配置？\n通过 Service 的 spec.clusterIP 来指定集群的 IP\n 为何不使用 round-robin DNS 而使用 VIP？\n 长久以来，DNS 库都没能认真对待 DNS TTL、缓存域名查询结果 很多应用只查询一次 DNS 并缓存了结果，就算应用和库能够正确查询解析，每个客户端反复重解析造成的负载也是非常难以管理的  到 Service 的流量如何路由过来的？Service 分布在不同的 Node 上，在不同的 Node 上有相同的 cluster IP？\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"20096e836fb62a357ff08e2d832c8b56","permalink":"/post/cloud/k8s/201904-k8s-loadbalancer.md/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/cloud/k8s/201904-k8s-loadbalancer.md/","section":"post","summary":"负载均衡的概念 负载均衡是将工作负载分布到多个服务器来提高网站、应用、数据库，来提高服务的性能和可靠性，是高可用网络基础架构的关键组件。\n图1，负载均衡示意图\n上图看到用户访问App时会先经过负载均衡（Load Balancer），再由负载均衡器将请求转发给后端服务器上的应用程序。\n负载均衡主要分为两种：四层和七层负载均衡。\n 四层负载均衡\n基于IP+端口的负载均衡，即在OSI第4层工作，就是通过发布三层的IP地址（VIP），然后加四层的端口号，来决定哪些流量需要做负载均衡，对需要处理的流量进行NAT处理，转发至后台服务器，并记录下这个TCP或者UDP的流量是由哪台服务器处理的，后续这个连接的所有流量都同样转发到同一台服务器处理。这种Load Balance不理解应用协议（如HTTP/FTP/MySQL等等）。\n 七层负载均衡\n工作在OSI的最高层，第7层应用层，就是在四层的基础上，再考虑应用层的特征，比如同一个Web服务器的负载均衡，除了根据VIP加80端口辨别是否需要处理的流量，还可根据七层的URL、浏览器类别、语言来决定是否要进行负载均衡。举个例子，如果你的Web服务器分成两组，一组是中文语言的，一组是英文语言的，那么七层负载均衡就可以当用户来访问你的域名时，自动辨别用户语言，然后选择对应的语言服务器组进行负载均衡处理。\n  图2，四层和七层负载均衡的区别\n四层负载均衡更像是路由器的工作方式，修改IP后转发即可；七层负载均衡需要获取到应用层信息，就需要与客户端建立连接并且解析应用层的内容，同时与服务端建立连接，将请求发送到对应的后端服务上。\nK8S 负载均衡 现在的后端服务从物理机向容器的运行环境转移，之前物理机的负载均衡已经不适用基于Kubernetes的容器网络了，Kubernetes也需要支持更加灵活和特性广泛的负载均衡。\n云环境下后端服务是运行在容器里的，容器是用Kubernetes通过Pod来编排和管理的，Pod是一组功能密切相关的容器的合集，提供一种服务。\nK8S 的负载均衡主要分为两个层面： - 集群内部层面，包括资源分配、内部流量的访问 - 外部访问集群\n调度层面 分配Pod到Node时就会考虑将一个Service的Pod打散分布，根据资源的容量和性能来部署Pod。作为调度过程的一部分，它需要充分考虑管理可用性，避免遇到资源瓶颈。达到整体运行良好的运行状况。\nService 我们知道 Pod 的生命周期是短暂的，会因为一些原因而被销毁而在别的 Node 重新创建，因此 Pod 的 IP 地址会变化，而其他服务无法知道新的 IP 地址，这就需要定义一个 Service 暴露给其他服务使用，一个 Service 可以有一个或多个 Pod 逻辑分组的抽象，这样前端应用程序向 Service IP 发起请求就可以了，而不需要关心具体的 Pod 的状态和 IP。Service IP 是固定不变的。\nService通过标签来选取服务后端，一般配合Replication Controller或者Deployment来保证后端容器的正常运行。\nService有三种类型：\n ClusterIP：默认类型，自动分配一个仅cluster内部可以访问的虚拟IP NodePort：在ClusterIP基础上为Service在每台机器上绑定一个端口，这样就可以通过:NodePort来访问该服务 LoadBalancer：在NodePort的基础上，借助cloud provider创建一个外部的负载均衡器，并将请求转发到:NodePort  另外，也可以将已有的服务以Service的形式加入到Kubernetes集群中来，只需要在创建Service的时候不指定Label selector，而是在Service创建好后手动为其添加endpoint。\nService 通过 VIP 实现的是一个4层负载均衡的功能。","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":" 参考 《程序员的数学基础课》笔记\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"66506d7580574e01fa600d1c1ed5c7dc","permalink":"/post/note/math/programmer-math-note/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/note/math/programmer-math-note/","section":"post","summary":"参考 《程序员的数学基础课》笔记","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":" 参考 http://qiankunli.github.io/2019/01/24/source_parse.html\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9393dfa0c8955bc8024f818db308d108","permalink":"/post/tech/how-to-read-code/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/tech/how-to-read-code/","section":"post","summary":"参考 http://qiankunli.github.io/2019/01/24/source_parse.html","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":" 编码规则 Protobuf消息由字段（field）构成，每个字段有其规则（rule）、数据类型（type）、字段名（name）、tag，以及选项（option）。比如下面这段代码描述了由多个字段构成的 Person 消息：\nmessage Person { string name = 1; int32 id = 2; // Unique ID number for this person. string email = 3; enum PhoneType { MOBILE = 0; HOME = 1; WORK = 2; } message PhoneNumber { string number = 1; PhoneType type = 2; } repeated PhoneNumber phones = 4; google.protobuf.Timestamp last_updated = 5; }  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c7b32cdb33691f0d5af8073f7c7a8b83","permalink":"/post/tech/proto-note/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/tech/proto-note/","section":"post","summary":" 编码规则 Protobuf消息由字段（field）构成，每个字段有其规则（rule）、数据类型（type）、字段名（name）、tag，以及选项（option）。比如下面这段代码描述了由多个字段构成的 Person 消息：\nmessage Person { string name = 1; int32 id = 2; // Unique ID number for this person. string email = 3; enum PhoneType { MOBILE = 0; HOME = 1; WORK = 2; } message PhoneNumber { string number = 1; PhoneType type = 2; } repeated PhoneNumber phones = 4; google.protobuf.Timestamp last_updated = 5; }  ","tags":null,"title":"","type":"post"}]