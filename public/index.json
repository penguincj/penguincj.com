[{"authors":["admin"],"categories":null,"content":"专注于基础架构，喜欢 Cloud Native，希望成为坚守开发一线打磨匠艺的架构师。\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"专注于基础架构，喜欢 Cloud Native，希望成为坚守开发一线打磨匠艺的架构师。","tags":null,"title":"L CJ","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"d9975920f3fb24c07bdc68e5a0f70299","permalink":"/istio/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/istio/example/","section":"istio","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"2d3419a1f0960318ce200b192e83a046","permalink":"/istio/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/istio/example/example1/","section":"istio","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"46c1c50a9cb1a6cc9bdb431c8244fc44","permalink":"/istio/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/istio/example/example2/","section":"istio","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":null,"categories":null,"content":" 当前的数据中心基本都是采用 Clos 架构，可以更好的支持东西向流量无阻塞转发，适用于数据中心的分布式应用。Clos 架构可以只用盒式交换机堆出一个大规模的网络，这种组网具有以低成本而且方便水平扩展的优势。数据中心的网络需求单一，因此很多公司自研盒式交换机，白盒交换机生态系统也因此得到了很大的发展，随着标准的制定和越来越多的上下游厂商支持，自研白盒交换机的难度越来越低。\nSONiC （Software for Open Networking in the Cloud） 是有微软开源的适用于数据中心云网络的交换机系统，目前托管在 OCP，支持多种厂商的硬件和 ASIC 转发芯片，基于 Linux 系统。提供在数据中心广泛应用的 BGP 和 RDMA 等功能。并且有一个活跃的社区来共建丰富多样的功能，可以帮助用户根据自己的需求灵活定制实现自己的网络功能。\n技术架构 SONiC是第一个将单一的交换机软件分解成多个组件的解决方案，方便开发和扩展新功能。基于现有的开源技术，如容器 Docker，key-value 数据库 Redis，Quagga BGP 和 LLDPD 等协议，以及配置化工具 Ansible。\n交换机抽象接口 SAI（Switch Abstraction Interface） 为网络硬件供应商定义了标准化编程接口，通过 SAI 的接口可以对交换机硬件下发配置和获取状态，能够在无需任何修改的前提下，使得软件支持多个交换机芯片。\nSONiC 系统框架如下图：\n主要有以下几个核心部分组成：\n 最底层的是交换机硬件，包括电源、风扇、光模块等外设，核心是交换芯片 ASIC 在硬件之上的是操作系统 Linux 内核，运行着系统和外设的驱动，核心是 ASIC 驱动 在系统之上的用户空间，运行着 SONiC 核心程序，包括对接 SAI 接口和 ASIC 接口，维护交换机状态数据库 最上一层是各种网络应用程序，像 BGP、SNMP 等，从下层的数据库获取交换机状态信息，同时根据协议状态修改交换机的状态。这些应用程序广泛采用了开源的组件。  SONiC 关键的组件都以容器的方式运行，下面介绍 SONiC 核心的组件 SwSS\nSwSS SwSS（Switch state Servic） 是 SONiC 最重要的组件提供了数据库通讯接口，和网络应用和交换机硬件的状态标识。\n网络应用层（netwowrk applications）通过对象库接口可以从 APP_DB 读取和写入。例如：netlink route syncer, quagga FPM route syncer，ACL、QoS、load balancer等。\nOrchestration agents 通过对象库接口可以在 APP 和 ASIC 数据库之间读取和写入数据。Orchestration agents 负责必要的逻辑整理，并将应用层数据传送到 SAI objects\nThe syncd process 在 ASIC_DB and the SAI SDK 之间进行读取和写入 SAI objects.\n key-value 键值型数据库：Redis\n 数据可全部放入内存，适用于频繁访问数据的场景 SONiC 在 Redis 中创建多个表分别用来记录配置、运行状态、数据统计等 Redis 支持对表项的订阅，可以由数据库的同步机制来驱动应用程序接受数据。  用于提供一个独立于语言的接口，数据可持久化、复制和进行多进程通信的方法。Redis 作为底层数据库引擎。\n netwowrk applications\n使用 SwSS API，SONiC 网络应用不需要知道底层通信细节，完全独立于硬件。网络应用之需要关注他们需要的数据，并不需要关心其他实现细节。\n比如需要 SwSS 实现功能有：3层2层桥接、路由、ACL，QOS，遥测流、隧道、链路聚合、负载均衡和基于策略的路由等等。\n Orchestration Agent\n这个功能实现了 APP_DB 和 ASIC_DB 中表之间的逻辑转化和拷贝。\n每个 ASIC 表必须只有一个 producer ，这就是 orchestration agent 来实现。只能由 orchestration agent 来写入 ASIC_DB table.\n syncd\nswitch sync 是个守护进程，在 ASIC_DB 表 和一个 SAI 兼容的 ASIC SDK 之间进行数据拷贝。 每个 SAI SDK 实例只有一 syncd 过程。\n Database 的实现\nSwSS 在 redis 中实现表概念是通过关键字做前缀命名的。\napp_table 设计成：route_table 和 neigh_table。\nASIC_ tables 是从 SAI 头文件中创建出来. 如： asic_sai_unicast_route_entry_t 和 asic_sai_neighbor_entry_t。\nSwSS 提供了数据库通讯接口，和网络应用和交换机硬件的状态标识。包含以下功能模块：fpmsyncd neighsyncd intfsyncd portsyncd orchagent swssconfig cfgmgr\n各个模块管理以下功能：\n teamsyncd/teamsyncd fpmsyncd/fpmsyncd intfsyncd/intfsyncd cfgmgr/intfmgrd cfgmgr/vlanmgrd neighsyncd/neighsyncd portsyncd/portsyncd orchagent/orchagent orchagent/routeresync swssconfig/swssconfig swssconfig/swssplayer   总结\nSonic-swss 为各个模快加载配置，执行用户操操作，写入到APPL_DB数据，并通过NETLINK与内核进行通讯。\n白盒交换机落地 白盒交换机的落地一般有如下几个阶段：\n 需求分析 软硬件选型 ODM 定制和生产 软件开发 整机测试 上线部署  需求分析 自研可以先从 ToR 开始，然后才是 Leaf、Spine，因为 ToR 相对来讲功能单一一些，出故障后影响面最小，部署量也是最大的，方便后面上线灰度测试。\n首先对 ToR 功能需求梳理，主要是二三层交换路由功能，主要的路由协议是 BGP，有一些功能不一定非要在 ToR 上实现，可以在网络的其他环节配置，如 ACL 可以在 Spine 上配置。然后对需求进行重要性和优先级排序，需求是分批实现的。\n软硬件选型 根据前面的需求可以指导软硬件的选型，软件选型的逻辑：\n 功能要满足大部分需求，最好不需要很大的开发量就可以满足需求；另外软件框架的性能要好，这样很多性能问题才能有优化的空间。 硬件支持和适配，软件要能做到与硬件解耦，方便切换硬件芯片，不至于被硬件厂家绑定。 系统扩展性，方便根据自己的应用场景和需求灵活定制开发功能，比如在交换机上做网络监控。 开源社区，一个氛围活跃的社区可以帮助自研交换机的小团队解决很多问题，共建社区良性发展。  硬件选型的逻辑：\n 功能，主要是转发芯片的功能要满足当前和以后潜在的需求，还有一些网络监控的需求，比如 INT（Inband Network Telemetry）、芯片丢包的抓取能力等。还要关注芯片厂商后续升级换代芯片的 Roadmap。 性能，转发芯片的吞吐，buffer 大小，64 字节小包线速转发，CPU 和内存是否足够强，以支持后续的开发和扩展。 稳定性，交换机作为网络最重要的节点，硬件的稳定性比软件的稳定性更加重要，这个一般要采用有经验的厂商来避免出问题。 服务，互联网自研交换机的团队一般不会很大，需要芯片和硬件厂商提供很多底层的支持，包括硬件和驱动的测试和故障定位。选取一个可靠的队友相当重要。  软硬件选型要结合自己的应用场景综合考虑，比如通用的 ToR 场景下使用常见的转发芯片就可以满足需求，而一些网络节点，比如网关可以采用可编程芯片灵活控制，这样以硬件的高性能和软件的灵活性可以创造出创新的解决方案。\nODM 定制和生产 根据硬件选型就可以选择 ODM 厂商了，最好让硬件设计成模块化的，CPU 和 内存方便移植和扩展的，这样无论是定位问题、维修还是后期的提升性能都很方便。另外就是选取靠谱的 ODM 厂商很重要。\n软件开发 软件开发应该聚焦在特别重要和紧急的需求上，多关注社区的 Roadmap，如果是标注的功能，以后社区可能也做这个功能，最好采用社区的版本，方便与社区同步。\n可以聚焦在网络监控功能的开发上，因为这种功能需求大家都不太一样，社区可能不会去做。网络监控一直是最大的痛点，这个做好了也是自研交换机价值的最大体现。\n整机测试 测试工作将是自研交换机所有工作的重点，因为不像商用交换机有很大的测试团队和经过市场验证的环节，即使是开源系统 SONiC 也存在很多 bug，因为各家的应用场景不一样，各个用户和厂商只会测试自己关注的功能和模块。就需要用户自己充分测试来保证系统的稳定性。\n测试是一种重复耗时、枯燥且容易出错但是又极其重要的工作，互联网公司的自研交换机团队一般没有多少人，需要搭建一套自动化测试环境，通过编写测试脚本来提高测试效率。也要积极争取厂商的测试资源，他们有成熟的测试团队，可以帮助发现和解决问题。\n上线部署 在前面的这么多工作做完之后，终于可以落地部署了，在部署之前要做好部署规划，除了功能验证之外还要与公司现有的网络监控平台对接。提前设想一些问题发生后，如何发现、处理和恢复网络故障。甚至要比商用交换机有更强的监控机制，充分了解设备的运行状态，给用户以足够的部署信心。还要注意在开始灰度阶段选择上线的业务最好是离线业务场景，对网络的质量不是特别敏感，这样出问题也能将损失降低。最好是按照一定的节奏灰度部署，开始量太大了容易出现大问题，量太少了又不容易发现问题。\n","date":1564444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564444800,"objectID":"c7911ececdc5c45f99c64e3055abd4f1","permalink":"/post/cloud/network/201907-sonic/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/post/cloud/network/201907-sonic/","section":"post","summary":"介绍 SONiC 技术架构和白盒交换机落地实施的步骤","tags":["Network"],"title":"SONiC - 云计算开放网络系统","type":"post"},{"authors":null,"categories":null,"content":" 随着容器和云原生的发展，对网络提出了新的要求，无论是平台的分布化以及业务的微服务化，都需要一个强大的网络来支撑大规模虚拟节点以及微服务之间的通信。\n具体到技术层面，云原生对于网络的要求，一是基础的二三层网络联通，二是4~7层的高级网络功能。\n本文主要描述适用于容器的二三层网络，该网络需要满足如下的需求：\n 大规模，连接成千上万的容器 高性能（低延时、高带宽） 高可靠 高效的网络监控，及早发现故障，跟踪故障和故障自愈  拓扑 服务器的双网卡做 bond 绑定之后双归到两台的不同的 ToR 上，两台 ToR 组成去堆叠组网，这样的基础物理网络具有高可靠性、大带宽的优点。\n虚拟网络 服务器内部连接容器的虚拟网络基于 OVS 和智能网卡的方案，兼顾了 OVS 的 SDN 转发灵活控制的特点，又可以将 Openflow 流表 offload 到智能网卡上，通过硬件转发来提高转发性能。\n容器网络由 CNI 创建和删除，在 Kubernetes 创建应用 Pod 之前配置网络时调用。\nOVS OpenvSwitch 以其丰富的功能，作为多层虚拟交换机，已经广泛应用于云环境中。Open vSwitch的主要功能是为物理机上的VM提供二层网络接入，和云环境中的其它物理交换机 并行工作在Layer 2。\nSR-IOV SR-IOV(PCI-SIG Single Root I/O Virtualization and Sharing)是PCI-SIG组织发布的规范。\nSR-IOV 设计目标：通过为虚拟机提供独立的I/O地址空间、中断和DMA流而避免VMM的介入；允许设备支持多个虚拟功能，并且最小化每个附加功能的硬件成本。\nSR-IOV引入了两个PCIe的function types\nPhysical Functions(PFs)：包括管理SR-IOV功能在内的所有PCIe function。\nVirtual Functions(VFs)：一部分轻量级的PCIe function，只能进行必要的数据操作和配置。\nSR-IOV机制提供独立多个可配置的VF，每一个VF具有独立的PCIe配置空间。VF是“轻量级”PCIe功能，包含数据传输所需的资源，提供了一种数据输入和输出的机制。\n虚拟机中的VF驱动程序应该是半虚拟化驱动程序(知道它在虚拟化环境中)，并且只执行它可用的操作。\nVF驱动程序是一个专门的驱动程序，它只有某些可用的功能，例如能够在VF中配置DMA描述符、配置MAC地址、VLAN标签等。\n通常，VF提供发送和接收数据的能力, 以及执行复位的功能，复位仅影响VF本身，而不影响整个物理设备。对于超出VF复位或发送和接收数据的动作，VF驱动程序需要与主驱动程序通信。\n容器 将 SR-IOV 的 VF 加入到容器的 namespace 中，将 VF 对应的 rep 口加入到 OVS 网桥上。这样就连接了宿主机与容器网络。\n容器的 IP 由 IPAM 分配，一般一个 ToR 下有两个 24 位网段的地址来支持最多 506 个容器 IP。\n容器的网关配置在 ToR 上，在容器中设置默认路由，下一跳为网关地址。\n宿主机 通过 OVS 流表来转发内部的流量，通过将 OVS 流表 offload 到智能网卡来提高转发性能、降低时延、增大吞吐。\n对每个容器都增加一条流表，当收到目的 IP 为容器 IP 的报文时，修改报文目的 MAC 为容器的 MAC 然后转发给容器的 VTEP 端口\n宿主机路由表配置一条到宿主机网关的默认路由，宿主机与容器的网段一般是不同的，网关也是不同的，但都是落在 ToR 上。\n从容器发出的流量首先发送到宿主机的 OVS bridge，bridge 查看是否有匹配的流表，如果是到本宿主机内部的流量则直接从对应的 VTEP 发送给容器。如果没有匹配则通过宿主机上的默认路由发送给网关，即 ToR 交换机。\n物理网络 ToR ToR 使用 去堆叠方案 方案，具有很高的可靠性，在其上生成到服务器和容器的 32 位主机路由，所有到服务器和容器的流量都是经过三层路由转发。\n交换机作为服务器和容器的网关，交换机上有：\n 到本机下联服务器、容器 IP 的 32 位主机路由 到其他服务器、容器的 24 位网段路由  推荐在 ToR 上运行 EBGP 协议来发布路由。\n总结 该方案基于智能网卡构建的基础网络具有很高的性能和控制的灵活性，不需要做 Overlay，具有很高的稳定性，网络拓扑简单，物理网络无需改造。缺点是智能网卡成本高。\n","date":1563753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563753600,"objectID":"8b56a99fb5da5f5a25a6739220edb4f5","permalink":"/post/cloud/network/201907-container-network/","publishdate":"2019-07-22T00:00:00Z","relpermalink":"/post/cloud/network/201907-container-network/","section":"post","summary":"基于 OVS、SR-IOV、BGP 组网的高性能、大规模的可靠网络","tags":["Network"],"title":"高性能、高可靠的容器网络","type":"post"},{"authors":null,"categories":null,"content":" 基础网络就好比建筑的地基一样，是保证业务，乃至公司正常运转的第一道防线，要做到基础网络的高可用，这个目标看起来简单，却并不是那么容易做到。本文介绍一种双上联去堆叠设计，为高可用的目标增加了强有力的支撑。\n为什么去堆叠？ 为了支持业务的高可用，物理网络架构往往采用双上行接入的方式，ToR 交换机采用了堆叠机构，从 Server 的接入 link 到 ToR 的层面实现了双活备份，当一台TOR的故障或者单网卡，单条网线的故障时，对于业务系统的可用性影响降到最低，当然对上层业务也会有一定影响，会出现高水位时性能的损失，但是不会出现服务不可用。\n传统的双活接入的方式主要有如下几种：\n 堆叠（常用，各厂商私有） VPC（思科私有） M-LAG（各厂商私有）  传统双活接入技术的缺点：\n 多设备统一控制面，可靠性低、升级困难 数量受限，规模受限 横向连接浪费端口  上述的技术都是要求两台交换机之间通过协议交互共享转发表项信息，甚至堆叠技术要求两台 ToR 在管理和转发层面都 merge 成为一台设备，这在无形中增加了技术复杂度和出问题的概率。实际运行过程中也体现了这一点，两台 ToR 由于堆叠系统的软硬件 bug 原因导致同时宕机的情况屡见不鲜，对业务系统的稳定性形成了很大的挑战。\n需要一种技术能够增加带宽提高网卡冗余的同时，真正解决了交换机接入硬件和软件同时冗余。目前在国内的 BAT 都有一些落地实践。\n去堆叠组网 服务器双网卡采用 bond + mod4，依靠捆绑实现分担与保护，交换机依靠 ECMP 实现负载分担与保护，此方式依赖：\n 接入交换机独立，但作为网关，需要配置相同的 IP \u0026amp; MAC 服务器网卡捆绑做负载分担 服务器网卡与交换机运行 LACP 交换机做 LACP 欺骗，两台交换机配置相同的 LACP SystemID，不同的 PortID 交换机支持 ARP 代理，所有的 ARP 请求都以网关 MAC 回应，将交换机的流量都转成路由转发 交换机将 ARP 转化为主机路由并发布 服务器网卡双发 ARP，确保交换机都学习到 ARP  ","date":1560729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560729600,"objectID":"cdc02c38bde87ca40f377dbfa74cd204","permalink":"/post/cloud/network/201906-stackless/","publishdate":"2019-06-17T00:00:00Z","relpermalink":"/post/cloud/network/201906-stackless/","section":"post","summary":"去堆叠才能实现网络真正的高可用","tags":["Network"],"title":"基于 Arp to Host 的去堆叠高可用网络","type":"post"},{"authors":null,"categories":null,"content":" 根据前面的文章 容器组件 我们知道组件之间的关系如下，本文通过 docker update 命令来追寻各组件之间的调用关系。\n图，docker 组件之间的调用关系\n比如新增一个自定义的属性 cacheProperty，并且支持该属性的 update。\ndocker update --cacheProperty test abebf7571666  docker client 在调用上面的 docker update 命令时首先调用的是 docker client，会进入到如下的流程。\n// cli/command/container/update.go func runUpdate(dockerCli *command.DockerCli, opts *updateOptions) error { resources := containertypes.Resources{ ... CPURealtimePeriod: opts.cpuRealtimePeriod, CacheProperty: opts.cacheProperty, } updateConfig := containertypes.UpdateConfig{ Resources: resources, RestartPolicy: restartPolicy, } for _, container := range opts.containers { r, err := dockerCli.Client().ContainerUpdate(ctx, container, updateConfig) ...... } }  // client/container_update.go func (cli *Client) ContainerUpdate(ctx context.Context, containerID string, updateConfig container.UpdateConfig) (container.ContainerUpdateOKBody, error) { serverResp, err := cli.post(ctx, \u0026quot;/containers/\u0026quot;+containerID+\u0026quot;/update\u0026quot;, nil, updateConfig, nil) }  上面的流程组装了一个 /containers/\u0026quot;+containerID+\u0026quot;/update 的API 请求。\nfunc (cli *Client) post(ctx context.Context, path string, query url.Values, obj interface{}, headers map[string][]string) (serverResponse, error) { body, headers, err := encodeBody(obj, headers) if err != nil { return serverResponse{}, err } return cli.sendRequest(ctx, \u0026quot;POST\u0026quot;, path, query, body, headers) }  func (cli *Client) doRequest(ctx context.Context, req *http.Request) (serverResponse, error) { serverResp := serverResponse{statusCode: -1} resp, err := ctxhttp.Do(ctx, cli.client, req) ... }  server //api/server/router/container/container.go func (r *containerRouter) initRoutes() { router.NewPostRoute(\u0026quot;/containers/{name:.*}/update\u0026quot;, r.postContainerUpdate), }  // container_routes.go func (s *containerRouter) postContainerUpdate(ctx context.Context, w http.ResponseWriter, r *http.Request, vars map[string]string) error { resp, err := s.backend.ContainerUpdate(name, hostConfig) }  // daemon/update.go func (daemon *Daemon) ContainerUpdate(name string, hostConfig *container.HostConfig) (container.ContainerUpdateOKBody, error) { var warnings []string if err := daemon.update(name, hostConfig); err != nil { return container.ContainerUpdateOKBody{Warnings: warnings}, err } }  func (daemon *Daemon) update(name string, hostConfig *container.HostConfig) error { // 更新container的配置 if err := container.UpdateContainer(hostConfig); err != nil { restoreConfig = true return errCannotUpdate(container.ID, err) } // 实时更新continer的状态 if container.IsRunning() \u0026amp;\u0026amp; !container.IsRestarting() { if err := daemon.containerd.UpdateResources(container.ID, toContainerdResources(hostConfig.Resources)); err != nil { restoreConfig = true return errCannotUpdate(container.ID, err) } } }  // container/container_unix.go func (container *Container) UpdateContainer(hostConfig *containertypes.HostConfig) error { container.Lock() defer container.Unlock() // update resources of container resources := hostConfig.Resources cResources := \u0026amp;container.HostConfig.Resources if resources.BlkioWeight != 0 { cResources.BlkioWeight = resources.BlkioWeight } if resources.cacheProperty != \u0026quot;\u0026quot; { cResources.CacheProperty = resources.CacheProperty } }  // libcontainerd/client_linux.go func (clnt *client) UpdateResources(containerID string, resources Resources) error { ... _, err = clnt.remote.apiClient.UpdateContainer(context.Background(), \u0026amp;containerd.UpdateContainerRequest{ Id: containerID, Pid: InitFriendlyName, Resources: (*containerd.UpdateResource)(\u0026amp;resources), }) }  通过 RPC 向 containerd 调用 UpdateContainer，proto 定义如下\n// containerd/api/grpc/types/api.proto service API { ... rpc UpdateContainer(UpdateContainerRequest) returns (UpdateContainerResponse) {} }  proto 生成的代码如下\n// containerd/api/grpc/types/api.pb.go func (c *aPIClient) UpdateContainer(ctx context.Context, in *UpdateContainerRequest, opts ...grpc.CallOption) (*UpdateContainerResponse, error) { out := new(UpdateContainerResponse) err := grpc.Invoke(ctx, \u0026quot;/types.API/UpdateContainer\u0026quot;, in, out, c.cc, opts...) if err != nil { return nil, err } return out, nil }  // daemon/update_linux.go func toContainerdResources(resources container.Resources) libcontainerd.Resources { var r libcontainerd.Resources ... r.MemoryReservation = uint64(resources.MemoryReservation) r.KernelMemoryLimit = uint64(resources.KernelMemory) r.CacheProperty = uint64(resources.CacheProperty) return r }  需要在 containerd.UpdateResource 中增加 CacheProperty 字段属性。\n// containerd/api/grpc/types/api.pb.go type UpdateResource struct { CpuPeriod uint64 `protobuf:\u0026quot;varint,3,opt,name=cpuPeriod\u0026quot; json:\u0026quot;cpuPeriod,omitempty\u0026quot;` CpuQuota uint64 `protobuf:\u0026quot;varint,4,opt,name=cpuQuota\u0026quot; json:\u0026quot;cpuQuota,omitempty\u0026quot;` ... CacheProperty uint64 `protobuf:\u0026quot;varint,19,opt,name=...\u0026quot; json:\u0026quot;...,omitempty\u0026quot;` }  containerd containerd 作为 gRPC server，接收 dockerd 的请求，具体的流程如下。\n// api/grpc/server/server.go func (s *apiServer) UpdateContainer(ctx context.Context, r *types.UpdateContainerRequest) (*types.UpdateContainerResponse, error) { e := \u0026amp;supervisor.UpdateTask{} e.WithContext(ctx) e.ID = r.Id e.State = runtime.State(r.Status) if r.Resources != nil { rs := r.Resources e.Resources = \u0026amp;runtime.Resource{} if rs.CpuShares != 0 { e.Resources.CPUShares = int64(rs.CpuShares) } } s.sv.SendTask(e) }  // supervisor/update.go func (s *Supervisor) updateContainer(t *UpdateTask) error { i, ok := s.containers[t.ID] if !ok { return ErrContainerNotFound } container := i.container if t.State != \u0026quot;\u0026quot; { switch t.State { case runtime.Running: if err := container.Resume(); err != nil { return err } s.notifySubscribers(Event{ ID: t.ID, Type: StateResume, Timestamp: time.Now(), }) case runtime.Paused: if err := container.Pause(); err != nil { return err } s.notifySubscribers(Event{ ID: t.ID, Type: StatePause, Timestamp: time.Now(), }) default: return ErrUnknownContainerStatus } return nil } if t.Resources != nil { return container.UpdateResources(t.Resources) } return nil }  // runtime/container_linux.go func (c *container) UpdateResources(r *Resource) error { sr := ocs.Resources{ Memory: \u0026amp;ocs.Memory{ Limit: u64Ptr(uint64(r.Memory)), Reservation: u64Ptr(uint64(r.MemoryReservation)), Swap: u64Ptr(uint64(r.MemorySwap)), Kernel: u64Ptr(uint64(r.KernelMemory)), KernelTCP: u64Ptr(uint64(r.KernelTCPMemory)), }, CPU: \u0026amp;ocs.CPU{ Shares: u64Ptr(uint64(r.CPUShares)), Quota: u64Ptr(uint64(r.CPUQuota)), Period: u64Ptr(uint64(r.CPUPeriod)), Cpus: \u0026amp;r.CpusetCpus, Mems: \u0026amp;r.CpusetMems, RealtimePeriod: u64Ptr(uint64(r.CPURealtimePeriod)), RealtimeRuntime: u64Ptr(uint64(r.CPURealtimeRuntime)), }, BlockIO: \u0026amp;ocs.BlockIO{ Weight: \u0026amp;r.BlkioWeight, }, } srStr := bytes.NewBuffer(nil) if err := json.NewEncoder(srStr).Encode(\u0026amp;sr); err != nil { return err } args := c.runtimeArgs args = append(args, \u0026quot;update\u0026quot;, \u0026quot;-r\u0026quot;, \u0026quot;-\u0026quot;, c.id) cmd := exec.Command(c.runtime, args...) cmd.Stdin = srStr b, err := cmd.CombinedOutput() if err != nil { return fmt.Errorf(string(b)) } return nil }  通过 exec 调用 runc 的命令行。\nrunc // update.go var updateCommand = cli.Command{ Name: \u0026quot;update\u0026quot;, Usage: \u0026quot;update container resource constraints\u0026quot;, ArgsUsage: `\u0026lt;container-id\u0026gt;`, Action: func(context *cli.Context) error { container, err := getContainer(context) if err != nil { return err } r := specs.Resources{ Memory: \u0026amp;specs.Memory{ Limit: u64Ptr(0), Reservation: u64Ptr(0), Swap: u64Ptr(0), Kernel: u64Ptr(0), KernelTCP: u64Ptr(0), }, } } } config.Cgroups.Resources.BlkioWeight = *r.BlockIO.Weight config.Cgroups.Resources.CpuPeriod = int64(*r.CPU.Period) }  自此 runc 接收到配置的更新并做相应的处理。后续的流程就不分析了。\n","date":1560556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560556800,"objectID":"d20b0e1194cbf3fb27a5deaa9ad39d24","permalink":"/post/cloud/container/201906-docker-update/","publishdate":"2019-06-15T00:00:00Z","relpermalink":"/post/cloud/container/201906-docker-update/","section":"post","summary":"以 docker update 流程分析各组件之间的调用关系","tags":["Docker"],"title":"Docker update 流程分析","type":"post"},{"authors":null,"categories":null,"content":" 功能简介 kube-proxy 运行在 kubernetes 集群中每个 worker 节点上，负责实现 service 这个概念提供的功能。kube-proxy 会把访问 service VIP 的请求转发到运行的 pods 上，实现负载均衡。\n当用户创建 service 的时候，endpointController 会根据 service 的 selector 找到对应的 pod，然后生成 endpoints 对象保存到 etcd 中。kube-proxy 的主要工作就是监听 etcd（通过 apiserver 的接口，而不是直接读取 etcd），来实时更新节点上的 iptables。\nservice 有关的信息保存在 etcd 的 /registry/services 目录，比如在我的集群中，这个目录的内容是这样的：\n~]$ etcdctl ls --recursive /registry/services /registry/services/endpoints /registry/services/endpoints/default /registry/services/endpoints/default/whoami /registry/services/endpoints/default/kubernetes /registry/services/endpoints/kube-system /registry/services/endpoints/kube-system/kube-controller-manager /registry/services/endpoints/kube-system/container-log /registry/services/endpoints/kube-system/container-terminal /registry/services/endpoints/kube-system/kube-scheduler /registry/services/endpoints/kube-system/kube-dns /registry/services/specs /registry/services/specs/default /registry/services/specs/default/kubernetes /registry/services/specs/default/whoami /registry/services/specs/kube-system /registry/services/specs/kube-system/kube-dns /registry/services/specs/kube-system/container-log /registry/services/specs/kube-system/container-terminal  架构 kube proxy是部署在 node 上的为应用容器提供代理转发的功能，\n在Kubernetes中，kube proxy负责为Pod创建代理服务；引到访问至服务；并实现服务到Pod的路由和转发，以及通过应用的负载均衡。\nservice 和 kube-proxy 原理 在 kubernetes 集群中，网络是非常基础也非常重要的一部分。对于大规模的节点和容器来说，要保证网络的连通性、网络转发的高效，同时能做的 ip 和 port 自动化分配和管理，并让用户用直观简单的方式来访问需要的应用，这是需要复杂且细致设计的。\nkubernetes 在这方面下了很大的功夫，它通过 service、dns、ingress 等概念，解决了服务发现、负载均衡的问题，也大大简化了用户的使用和配置。\n跨主机网络配置：flannel 一直以来，kubernetes 并没有专门的网络模块负责网络配置，它需要用户在主机上已经配置好网络。kubernetes 对网络的要求是：容器之间（包括同一台主机上的容器，和不同主机的容器）可以互相通信，容器和集群中所有的节点也能直接通信。\n至于具体的网络方案，用户可以自己选择，目前使用比较多的是 flannel，因为它比较简单，而且刚好满足 kubernetes 对网络的要求。我们会使用 flannel vxlan 模式，具体的配置我在博客之前有文章介绍过，这里不再赘述。\n以后 kubernetes 网络的发展方向是希望通过插件的方式来集成不同的网络方案， CNI 就是这一努力的结果，flannel 也能够通过 CNI 插件的形式使用。\nkube-proxy 和 service 配置好网络之后，集群是什么情况呢？我们可以创建 pod，也能通过 ReplicationController 来创建特定副本的 pod（这是更推荐也是生产上要使用的方法，即使某个 rc 中只有一个 pod 实例）。可以从集群中获取每个 pod ip 地址，然后也能在集群内部直接通过 podIP:Port 来获取对应的服务。\n但是还有一个问题：pod 是经常变化的，每次更新 ip 地址都可能会发生变化，如果直接访问容器 ip 的话，会有很大的问题。而且进行扩展的时候，rc 中会有新的 pod 创建出来，出现新的 ip 地址，我们需要一种更灵活的方式来访问 pod 的服务。\nService 和 cluster IP 针对这个问题，kubernetes 的解决方案是“服务”（service），每个服务都一个固定的虚拟 ip（这个 ip 也被称为 cluster IP），自动并且动态地绑定后面的 pod，所有的网络请求直接访问服务 ip，服务会自动向后端做转发。Service 除了提供稳定的对外访问方式之外，还能起到负载均衡（Load Balance）的功能，自动把请求流量分布到后端所有的服务上，服务可以做到对客户透明地进行水平扩展（scale）。\n而实现 service 这一功能的关键，就是 kube-proxy。kube-proxy 运行在每个节点上，监听 API Server 中服务对象的变化，通过管理 iptables 来实现网络的转发。\n实例启动和测试 我们可以在终端上启动 kube-proxy，也可以使用诸如 systemd 这样的工具来管理它，比如下面就是一个简单的 kube-proxy.service 配置文件\n[root@localhost]# cat /usr/lib/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Proxy Service Documentation=http://kubernetes.com After=network.target Wants=network.target [Service] Type=simple EnvironmentFile=-/etc/sysconfig/kube-proxy ExecStart=/usr/bin/kube-proxy \\ --master=http://172.17.8.100:8080 \\ --v=4 \\ --proxy-mode=iptables TimeoutStartSec=0 Restart=on-abnormal [Install] WantedBy=multi-user.target  为了方便测试，我们创建一个 rc，里面有三个 pod。这个 pod 运行的是 cizixs/whoami 容器，它是一个简单的 HTTP 服务器，监听在 3000 端口，访问它会返回容器的 hostname。\n[root@localhost ~]# cat whoami-rc.yml apiVersion: v1 kind: ReplicationController metadata: name: whoami spec: replicas: 3 selector: app: whoami template: metadata: name: whoami labels: app: whoami env: dev spec: containers: - name: whoami image: cizixs/whoami:v0.5 ports: - containerPort: 3000 env: - name: MESSAGE value: viola  我们为每个 pod 设置了两个 label：app=whoami 和 env=dev，这两个标签很重要，也是后面服务进行绑定 pod 的关键。\n为了使用 service，我们还要定义另外一个文件，并通过 kubectl create -f ./whoami-svc.yml 来创建出来对象：\napiVersion: v1 kind: Service metadata: labels: name: whoami name: whoami spec: ports: - port: 3000 targetPort: 3000 protocol: TCP selector: app: whoami env: dev  其中 selector 告诉 kubernetes 这个 service 和后端哪些 pod 绑定在一起，这里包含的键值对会对所有 pod 的 labels 进行匹配，只要完全匹配，service 就会把 pod 作为后端。也就是说，service 和 rc 并不是对应的关系，一个 service 可能会使用多个 rc 管理的 pod 作为后端应用。\nports 字段指定服务的端口信息：\n port：虚拟 ip 要绑定的 port，每个 service 会创建出来一个虚拟 ip，通过访问 vip:port 就能获取服务的内容。这个 port 可以用户随机选取，因为每个服务都有自己的 vip，也不用担心冲突的情况 targetPort：pod 中暴露出来的 port，这是运行的容器中具体暴露出来的端口，一定不能写错 protocol：提供服务的协议类型，可以是 TCP 或者 UDP  创建之后可以列出 service ，发现我们创建的 service 已经分配了一个虚拟 ip (10.10.10.28)，这个虚拟 ip 地址是不会变化的（除非 service 被删除）。查看 service 的详情可以看到它的 endpoints 列出，对应了具体提供服务的 pod 地址和端口。\n[root@localhost ~]# kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.10.10.1 \u0026lt;none\u0026gt; 443/TCP 19d whoami 10.10.10.28 \u0026lt;none\u0026gt; 3000/TCP 1d [root@localhost ~]# kubectl describe svc whoami Name: whoami Namespace: default Labels: name=whoami Selector: app=whoami Type: ClusterIP IP: 10.10.10.28 Port: \u0026lt;unset\u0026gt; 3000/TCP Endpoints: 10.11.32.6:3000,10.13.192.4:3000,10.16.192.3:3000 Session Affinity: None No events.  默认的 service 类型是 ClusterIP，这个也可以从上面输出看出来。在这种情况下，只能从集群内部访问这个 IP，不能直接从集群外部访问服务。如果想对外提供服务，我们后面会讲解决方案。\n测试一下，访问 service 服务的时候可以看到它会随机地访问后端的 pod，给出不同的返回：\n[root@localhost ~]# curl http://10.10.10.28:3000 viola from whoami-8fpqp [root@localhost ~]# curl http://10.10.10.28:3000 viola from whoami-c0x6h [root@localhost ~]# curl http://10.10.10.28:3000 viola from whoami-8fpqp [root@localhost ~]# curl http://10.10.10.28:3000 viola from whoami-dc9ds  默认情况下，服务会随机转发到可用的后端。如果希望保持会话（同一个 client 永远都转发到相同的 pod），可以把 service.spec.sessionAffinity 设置为 ClientIP。\nNOTE: 需要注意的是，服务分配的 cluster IP 是一个虚拟 ip，如果你尝试 ping 这个 IP 会发现它没有任何响应，这也是刚接触 kubernetes service 的人经常会犯的错误。实际上，这个虚拟 IP 只有和它的 port 一起的时候才有作用，直接访问它，或者想访问该 IP 的其他端口都是徒劳。\n外部能够访问的服务 上面创建的服务只能在集群内部访问，这在生产环境中还不能直接使用。如果希望有一个能直接对外使用的服务，可以使用 NodePort 或者 LoadBalancer 类型的 Service。我们先说说 NodePort ，它的意思是在所有 worker 节点上暴露一个端口，这样外部可以直接通过访问 nodeIP:Port 来访问应用。\n我们先把刚才创建的服务删除：\n[root@localhost ~]# kubectl delete rc whoami replicationcontroller \u0026quot;whoami\u0026quot; deleted [root@localhost ~]# kubectl delete svc whoami service \u0026quot;whoami\u0026quot; deleted [root@localhost ~]# kubectl get pods,svc,rc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.10.10.1 \u0026lt;none\u0026gt; 443/TCP 14h  对我们原来的 Service 配置文件进行修改，把 spec.type 写成 NodePort 类型：\n[root@localhost ~]# cat whoami-svc.yml apiVersion: v1 kind: Service metadata: labels: name: whoami name: whoami spec: ports: - port: 3000 protocol: TCP # nodePort: 31000 selector: app: whoami type: NodePort  因为我们的应用比较简单，只有一个端口。如果 pod 有多个端口，也可以在 spec.ports中继续添加，只有保证多个 port 之间不冲突就行。\n重新创建 rc 和 svc：\n[root@localhost ~]# kubectl create -f ./whoami-svc.yml service \u0026quot;whoami\u0026quot; created [root@localhost ~]# kubectl get rc,pods,svc NAME DESIRED CURRENT READY AGE rc/whoami 3 3 3 10s NAME READY STATUS RESTARTS AGE po/whoami-8zc3d 1/1 Running 0 10s po/whoami-mc2fg 1/1 Running 0 10s po/whoami-z6skj 1/1 Running 0 10s NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes 10.10.10.1 \u0026lt;none\u0026gt; 443/TCP 14h svc/whoami 10.10.10.163 \u0026lt;nodes\u0026gt; 3000:31647/TCP 7s  需要注意的是，因为我们没有指定 nodePort 的值，kubernetes 会自动给我们分配一个，比如这里的 31647（默认的取值范围是 30000-32767）。当然我们也可以删除配置中 # nodePort: 31000 的注释，这样会使用 31000 端口。\nnodePort 类型的服务会在所有的 worker 节点（运行了 kube-proxy）上统一暴露出端口对外提供服务，也就是说外部可以任意选择一个节点进行访问。比如我本地集群有三个节点：172.17.8.100、172.17.8.101 和 172.17.8.102：\n[root@localhost ~]# curl http://172.17.8.100:31647 viola from whoami-mc2fg [root@localhost ~]# curl http://172.17.8.101:31647 viola from whoami-8zc3d [root@localhost ~]# curl http://172.17.8.102:31647 viola from whoami-z6skj  有了 nodePort，用户可以通过外部的 Load Balance 或者路由器把流量转发到任意的节点，对外提供服务的同时，也可以做到负载均衡的效果。\nnodePort 类型的服务并不影响原来虚拟 IP 的访问方式，内部节点依然可以通过 vip:port 的方式进行访问。\nLoadBalancer 类型的服务需要公有云支持，如果你的集群部署在公有云（GCE、AWS等）可以考虑这种方式。\nservice 原理解析 (iptables) 目前 kube-proxy 默认使用 iptables 模式，上述展现的 service 功能都是通过修改 iptables 实现的。\n我们来看一下从主机上访问 service:port 的时候发生了什么（通过 iptables-save 命令打印出来当前机器上的 iptables 规则）。\n所有发送出去的报文会进入 KUBE-SERVICES 进行处理\n*nat -A PREROUTING -m comment --comment \u0026quot;kubernetes service portals\u0026quot; -j KUBE-SERVICES -A OUTPUT -m comment --comment \u0026quot;kubernetes service portals\u0026quot; -j KUBE-SERVICES -A POSTROUTING -m comment --comment \u0026quot;kubernetes postrouting rules\u0026quot; -j KUBE-POSTROUTING -A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000  KUBE-SERVICES 每条规则对应了一个 service，它告诉继续进入到某个具体的 service chain 进行处理，比如这里的 KUBE-SVC-OQCLJJ5GLLNFY3XB\n-A KUBE-SERVICES -d 10.10.10.28/32 -p tcp -m comment --comment \u0026quot;default/whoami: cluster IP\u0026quot; -m tcp --dport 3000 -j KUBE-SVC-OQCLJJ5GLLNFY3XB  更具体的 chain 中定义了怎么转发到对应 endpoint 的规则，比如我们的 rc 有三个 pods，这里也就会生成三个规则。这里利用了 iptables 随机和概率转发的功能\n-A KUBE-SVC-OQCLJJ5GLLNFY3XB -m comment --comment \u0026quot;default/whoami:\u0026quot; -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-VN72UHNM6XOXLRPW -A KUBE-SVC-OQCLJJ5GLLNFY3XB -m comment --comment \u0026quot;default/whoami:\u0026quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-YXCSPWPTUFI5WI5Y -A KUBE-SVC-OQCLJJ5GLLNFY3XB -m comment --comment \u0026quot;default/whoami:\u0026quot; -j KUBE-SEP-FN74S3YUBFMWHBLF  我们来看第一个 chain，这个 chain 有两个规则，第一个表示给报文打上 mark；第二个是进行 DNAT（修改报文的目的地址），转发到某个 pod 地址和端口。\n-A KUBE-SEP-VN72UHNM6XOXLRPW -s 10.11.32.6/32 -m comment --comment \u0026quot;default/whoami:\u0026quot; -j KUBE-MARK-MASQ -A KUBE-SEP-VN72UHNM6XOXLRPW -p tcp -m comment --comment \u0026quot;default/whoami:\u0026quot; -m tcp -j DNAT --to-destination 10.11.32.6:3000  会匹配 chain 的第二条规则，因为地址是发送出去的，报文会根据路由规则进行处理，后续的报文就是通过 flannel 的网络路径发送出去的。\nnodePort 类型的 service 原理也是类似的，在 KUBE-SERVICES chain 的最后，如果目标地址不是 VIP 则会通过 KUBE-NODEPORTS ：\nChain KUBE-SERVICES (2 references) pkts bytes target prot opt in out source destination 0 0 KUBE-NODEPORTS all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL  而 KUBE-NODEPORTS chain 和 KUBE-SERVICES chain 其他规则一样，都是转发到更具体的 service chain，然后转发到某个 pod 上面。\n-A KUBE-NODEPORTS -p tcp -m comment --comment \u0026quot;default/whoami:\u0026quot; -m tcp --dport 31647 -j KUBE-MARK-MASQ -A KUBE-NODEPORTS -p tcp -m comment --comment \u0026quot;default/whoami:\u0026quot; -m tcp --dport 31647 -j KUBE-SVC-OQCLJJ5GLLNFY3XB  注意：在流量转发出去的时候，就已经选择好了目的 Pod:TargetPod，而源IP是 node 的ip。\n不足之处 看起来 service 是个完美的方案，可以解决服务访问的所有问题，但是 service 这个方案（iptables 模式）也有自己的缺点。\n首先，如果转发的 pod 不能正常提供服务，它不会自动尝试另一个 pod，当然这个可以通过 readiness probes 来解决。每个 pod 都有一个健康检查的机制，当有 pod 健康状况有问题时，kube-proxy 会删除对应的转发规则。\n另外，nodePort 类型的服务也无法添加 TLS 或者更复杂的报文路由机制。\nproxy 的 iptables mode因为使用iptable NAT来完成转发，也存在不可忽视的性能损耗。另外，如果集群中存在上万的Service/Endpoint，那么Node上的iptables rules将会非常庞大，性能还会再打折扣。\n这也导致，目前大部分企业用k8s上生产时，都不会直接用kube-proxy作为服务代理，而是通过自己开发或者通过Ingress Controller来集成HAProxy, Nginx来代替kube-proxy。\n代码阅读 重要结构体说明 本文以iptables 代理模式为例,对proxy 的功能实现进行分析。基于iptables的kube-proxy的主要职责包括两大块：一块是侦听service更新事件，并更新service相关的iptables规则，一块是侦听endpoint更新事件，更新endpoint相关的iptables规则。也就是说kube-proxy只是作为controller 负责更新更新规则，实现转发服务的是内核的netfilter，体现在用户态则是iptables。\nProxyServer type ProxyServer struct { // k8s client Client clientset.Interface EventClient v1core.EventsGetter // 接口中定义了更新iptables 的方法集合，如DeleteChian,DeleteRule, EnsureChain,EnsureRule IptInterface utiliptables.Interface IpvsInterface utilipvs.Interface IpsetInterface utilipset.Interface // 定义包装os/exec库中Command, Commandcontext, LookPath方法的接口 execer exec.Interface // 处理同步时的处理器，有三种模式 Proxier proxy.ProxyProvider //接受Event，交于各个处理函数进行处理 Broadcaster record.EventBroadcaster // 代理模式，ipvs iptables userspace kernelspace(windows)四种 ProxyMode string // 配置同步周期 ConfigSyncPeriod time.Duration // service 与 endpoint 事件处理器 ServiceEventHandler config.ServiceHandler EndpointsEventHandler config.EndpointsHandler }  Proxier 在每一种代理模式下，都定义了自己的Proxier 结构体，该结构体及方法实现了该模式下的代理规则的更新方法。在Iptables 模式下，Proxier 结构体定义如下：\ntype Proxier struct { //EndpointChangeTracker中items属性为一个两级map,用来保存所有namespace 下endpoints 的变化信息。 //第一级map以namespece 为key，value 值为该namespace下所有endpoints 更新前（previous)、后(current)的信息。 //前、后信息分别为一个map ,即第二级map: ServiceMap。 //第二级map的key为ServicePortName 结构，标记endpoints 对应的service，value为endpoint信息。 // EndpointChangeTracker 中实现了更新endpoint 的方法 endpointsChanges *proxy.EndpointChangeTracker // 同理，ServiceChangeTracker 中使用一个两级map保存所有namespace 下的service的变化信息，并定义了更新service的方法 serviceChanges *proxy.ServiceChangeTracker mu sync.Mutex // protects the following fields serviceMap proxy.ServiceMap // 同serviceChanges 的第二及map 结构，记录了所有namespace下需要更新iptables规则的service endpointsMap proxy.EndpointsMap //同endpointsChanges 的第二及map 结构，记录了所有namespace 需要更新iptables规则的endpoints portsMap map[utilproxy.LocalPort]utilproxy.Closeable endpointsSynced bool // Proxier 初始化时为False servicesSynced bool // Proxier 初始化时为False initialized int32 syncRunner *async.BoundedFrequencyRunner //async.BoundedFrequencyRunner 具有QPS功能，控制被托管方法的发生速率 // These are effectively const and do not need the mutex to be held. iptables utiliptables.Interface //iptables的执行器，定义了Iptables 的操作方法 masqueradeAll bool masqueradeMark string exec utilexec.Interface // 抽象了 os/exec 中的方法 clusterCIDR string hostname string nodeIP net.IP portMapper utilproxy.PortOpener //以打开的UDP或TCP端口 recorder record.EventRecorder healthChecker healthcheck.Server healthzServer healthcheck.HealthzUpdater precomputedProbabilities []string iptablesData *bytes.Buffer existingFilterChainsData *bytes.Buffer filterChains *bytes.Buffer filterRules *bytes.Buffer natChains *bytes.Buffer natRules *bytes.Buffer endpointChainsNumber int // Values are as a parameter to select the interfaces where nodeport works. nodePortAddresses []string // networkInterfacer defines an interface for several net library functions. // Inject for test purpose. networkInterfacer utilproxy.NetworkInterfacer }  Proxier 自定义的链 在iptables 原有的5个链上，k8s 又增加了以下自定义链，在自定义链上添加规则，以控制iptables 对k8s 数据包的转发。\nconst ( iptablesMinVersion = utiliptables.MinCheckVersion // 支持-C/--flag 参数的iptable 最低版本 //对于Service type=ClusterIP的每个端口都会在KUBE-SERVICES中有一条对应的规则 kubeServicesChain utiliptables.Chain = \u0026quot;KUBE-SERVICES\u0026quot; // kubeExternalServicesChain utiliptables.Chain=\u0026quot;KUBE-EXTERNAL-SERVICES\u0026quot; //对于Service type=NodePort的每个端口都会在KUBE-NODEPORTS中有一条对应的规则 kubeNodePortsChain utiliptables.Chain = \u0026quot;KUBE-NODEPORTS\u0026quot; //在KUBE-POSTROUTING链上，对(0x400)包做SNAT kubePostroutingChain utiliptables.Chain = \u0026quot;KUBE-POSTROUTING\u0026quot; //打标签链，对于进入此链的报文打标签(0x400)，预示被标签包要做NAT KubeMarkMasqChain utiliptables.Chain = \u0026quot;KUBE-MARK-MASQ\u0026quot; //打标签链，对于进入此链的报文打标签(0x800)，预示此包将要被放弃 KubeMarkDropChain utiliptables.Chain = \u0026quot;KUBE-MARK-DROP\u0026quot; //跳转 kubeForwardChain utiliptables.Chain = \u0026quot;KUBE-FORWARD\u0026quot; )  Proxy Server 启动 穿过cobra.Command 包装的一个启动命令，走到跟kube-proxy 服务相关的一个代码入口 Run()。在Run()中，主要就是两件事：\n 生成一个ProxyServer 实例；\n 运行ProxyServer 实例的Run 方法，运行服务。\n  kubernetes/cmd/kube-proxy/app/server.go\nfunc (o *Options) Run() error { if len(o.WriteConfigTo) \u0026gt; 0 { return o.writeConfigFile() } proxyServer, err := NewProxyServer(o) //初始化结构体ProxyServer if err != nil { return err } return proxyServer.Run() // 运行ProxyServer }  ProxyServer 初始化 进入NewProxyServer(o) 方法，开始ProxyServer 的初始化过程初始化过程中，重要的一个环节就是根据不同的代理模式生成不通的Proxier。初始化过程中，主要变量的初始化及作用已在代码中说明。\ncmd/kube-proxy/app/server_others.go func newProxyServer( config *proxyconfigapi.KubeProxyConfiguration, cleanupAndExit bool, cleanupIPVS bool, scheme *runtime.Scheme, master string) (*ProxyServer, error) { ... protocol := utiliptables.ProtocolIpv4 // 获取机器使用的IP协议版本，默认使用IPV4 ... // Create a iptables utils. execer := exec.New() // 包装了os/exec中的command,LookPath,CommandContext 方法，组装一个系统调用的命令和参数 dbus = utildbus.New() //iptInterface 赋值为runner结构体，该结构体实现了接口utiliptables.Interface中定义的方法， //各方法中通过runContext()方法调用execer的命令包装方法返回一个被包装的iptables 命令 iptInterface = utiliptables.New(execer, dbus, protocol) ... //EventBroadcaster会将收到的Event交于各个处理函数进行处理。接收Event的缓冲队列长为1000，不停地取走Event并广播给各个watcher; //watcher通过recordEvent()方法将Event写入对应的EventSink里，最大重试次数为12次，重试间隔随机生成(见staging/src/k8s.io/client-go/tools/record/event.go); // EnventSink 将在ProxyServer.Run() 中调用s.Broadcaster.StartRecordingToSink（） 传进来; // NewBroadcaster() 最后会启动一个goroutine 运行Loop 方法（staging/src/k8s.io/apimachinery/pkg/watch/mux.go), eventBroadcaster := record.NewBroadcaster() //EventRecorder通过generateEvent()实际生成各种Event，并将其添加到监视队列。 recorder := eventBroadcaster.NewRecorder(scheme, v1.EventSource{Component: \u0026quot;kube-proxy\u0026quot;, Host: hostname}) ... if len(config.HealthzBindAddress) \u0026gt; 0 {//服务健康检查的 IP 地址和端口（IPv4默认为0.0.0.0:10256，对于所有 IPv6 接口设置为 ::） healthzServer = healthcheck.NewDefaultHealthzServer(config.HealthzBindAddress, 2*config.IPTables.SyncPeriod.Duration, recorder, nodeRef) healthzUpdater = healthzServer } ... proxyMode := getProxyMode(string(config.Mode), iptInterface, kernelHandler, ipsetInterface, iptables.LinuxKernelCompatTester{}) ... if proxyMode == proxyModeIPTables { klog.V(0).Info(\u0026quot;Using iptables Proxier.\u0026quot;) if config.IPTables.MasqueradeBit == nil { // MasqueradeBit must be specified or defaulted. return nil, fmt.Errorf(\u0026quot;unable to read IPTables MasqueradeBit from config\u0026quot;) } // 返回一个Proxier 结构体实例 proxierIPTables, err := iptables.NewProxier(...) //参数略 if err != nil { return nil, fmt.Errorf(\u0026quot;unable to create proxier: %v\u0026quot;, err) } metrics.RegisterMetrics() proxier = proxierIPTables // Iptables Proxier 实现了 ServiceHandler 和 EndpointsHandler 的接口。 serviceEventHandler = proxierIPTables endpointsEventHandler = proxierIPTables userspace.CleanupLeftovers(iptInterface)// 无条件强制清除之前userspace 模式的规则 // 因为无法区分iptables 规则是否由IPVS 代理生成，因此由用户根据实际情况决定是否调用ipvs.CleanupLeftovers() if canUseIPVS { ipvs.CleanupLeftovers(ipvsInterface, iptInterface, ipsetInterface, cleanupIPVS) } } else if proxyMode == proxyModeIPVS {// 初始化IPVS Proxier } else { // 初始化 userspace Proxier } iptInterface.AddReloadFunc(proxier.Sync) return \u0026amp;ProxyServer{ // 赋值过程略 ... }, nil }  Proxier 初始化 // kubernetes/pkg/proxy/iptables/proxier.go func NewProxier(...) (*Proxier, error) { //参数略 ... //kube-proxy要求NODE节点操作系统中有/sys/module/br_netfilter模块，还要设置bridge-nf-call-iptables=1； //如果不满足要求，kube-proxy在运行过程中设置的某些iptables规则就不会工作。 if val, err := sysctl.GetSysctl(sysctlBridgeCallIPTables); err == nil \u0026amp;\u0026amp; val != 1 { klog.Warning(\u0026quot;missing br-netfilter module or unset sysctl br-nf-call-iptables; proxy may not work as intended\u0026quot;) } // Generate the masquerade mark to use for SNAT rules. masqueradeValue := 1 \u0026lt;\u0026lt; uint(masqueradeBit) //masqueradeBit: Default 14 // 输出一个8位16进制数值 ，默认即0x00004000/0x00004000,用来标记k8s管理的报文。 //标记 0x4000的报文（即POD发出的报文)，在离开Node（物理机）的时候需要进行SNAT转换。 masqueradeMark := fmt.Sprintf(\u0026quot;%#08x/%#08x\u0026quot;, masqueradeValue, masqueradeValue) healthChecker := healthcheck.NewServer(hostname, recorder, nil, nil) // use default implementations of deps isIPv6 := ipt.IsIpv6() proxier := \u0026amp;Proxier{ portsMap: make(map[utilproxy.LocalPort]utilproxy.Closeable), serviceMap: make(proxy.ServiceMap), ... networkInterfacer: utilproxy.RealNetwork{}, } burstSyncs := 2 ... //Default: syncPeriod=30s (--iptables-sync-period duration)，将proxier.syncProxyRules 托管至BoundedFrequencyRunner 结构体， //BoundedFrequencyRunner 中含有一个Limiter ，该Limiter 采用\u0026quot;桶令牌\u0026quot; 限流算法控制proxier.syncProxyRules 方法运行的频率。 //minSyncPeriod=0 时，无速率限制。限流时，桶类初始令牌数量为burstSyncs。 proxier.syncRunner = async.NewBoundedFrequencyRunner(\u0026quot;sync-runner\u0026quot;, proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs) return proxier, nil }  注册ResourceHandler ProxyServer 及Proxier 这两个重要的结构体初始化完成以后，就进入了proxyServer.Run() 方法。在Run() 方法中，大致做了如下工作：\n 准备工作，如设置OOMScoreAdj, 注册service 和endpoints 的处理方法\n 使用list-watch 机制对service，endpoints资源监听。\n 最后进入一个无限循环，对service与endpoints的变化进行iptables规则的同步。\n  在Run方法中，主要关注一下对service 和endpoints资源变化的处理方法的注册过程。\n// cmd/kube-proxy/app/server.go func (s *ProxyServer) Run() error { ... //在用户空间通过写oomScoreAdj参数到/proc/self/oom_score_adj文件来改变进程的 oom_adj 内核参数； //oom_adj的值的大小决定了进程被 OOM killer，取值范围[-1000,1000] 选中杀掉的概率,值越低越不容易被杀死.此处默认值是-999。 if s.OOMScoreAdj != nil { oomAdjuster = oom.NewOOMAdjuster() if err := oomAdjuster.ApplyOOMScoreAdj(0, int(*s.OOMScoreAdj)); err != nil { klog.V(2).Info(err) } } if len(s.ResourceContainer) != 0 { ... // resourcecontainer.RunInResourceContainer(s.ResourceContainer); ... } if s.Broadcaster != nil \u0026amp;\u0026amp; s.EventClient != nil { // EventSinkImpl 包装了处理event 的方法create ,update, patchs //s.Broadcaster 已经在ProxyServer 初始化中作为一个goroutine 在运行。 s.Broadcaster.StartRecordingToSink(\u0026amp;v1core.EventSinkImpl{Interface: s.EventClient.Events(\u0026quot;\u0026quot;)}) } // Start up a healthz server if requested if s.HealthzServer != nil { s.HealthzServer.Run() } // Start up a metrics server if requested if len(s.MetricsBindAddress) \u0026gt; 0 { ... } // Tune conntrack, if requested // Conntracker is always nil for windows if s.Conntracker != nil { max, err := getConntrackMax(s.ConntrackConfiguration) ... } // Default: s.ConfigSyncPeriod =15m (--config-sync-period) //返回一个sharedInformerFactory结构体实例(staing/src/k8s.io/client-go/informers/factory.go) informerFactory := informers.NewSharedInformerFactory(s.Client, s.ConfigSyncPeriod) //ServiceConfig结构体跟踪记录Service配置信息的变化 //informerFactory.Core().V1().Services() 返回一个 serviceInformer 结构体引用(staing/src/k8s.io/client-go/informers/core/v1/service.go serviceConfig := config.NewServiceConfig(informerFactory.Core().V1().Services(), s.ConfigSyncPeriod) //RegisterEventHandler 是将Service的处理方法追加到serviceConfig的eventHandlers 中，eventHandlers为一个列表，元素类型ServiceHandler接口 // ServiceHandler接口定义了每个hanlder 处理service的api方法:OnServiceAdd,OnServiceUpdate,OnServiceDelete,OnServiceSynced // 此处s.ServiceEventHandler 为proxier，proxier实现了ServiceHandler接口定义的方法 //serviceConfig 中的handleAddService,handleUpdateService,handleDeleteService 将会调用每个eventHandler的OnServiceAdd等方法 serviceConfig.RegisterEventHandler(s.ServiceEventHandler) go serviceConfig.Run(wait.NeverStop) //初始化同步service,调用了一次proxier.syncProxyRules() endpointsConfig := config.NewEndpointsConfig(informerFactory.Core().V1().Endpoints(), s.ConfigSyncPeriod) endpointsConfig.RegisterEventHandler(s.EndpointsEventHandler) go endpointsConfig.Run(wait.NeverStop) // This has to start after the calls to NewServiceConfig and NewEndpointsConfig because those // functions must configure their shared informer event handlers first. go informerFactory.Start(wait.NeverStop) // Birth Cry after the birth is successful s.birthCry() // Just loop forever for now... s.Proxier.SyncLoop() return nil }  ServiceConfig 来跟踪 service 配置变化，通过 chan 接收到 set、add、remove的操作，然后调用相应的注册的 ServiceHandler。\ntype ServiceConfig struct { lister listers.ServiceLister listerSynced cache.InformerSynced eventHandlers []ServiceHandler }  上面以注释的方式描述了proxier中service处理方法的被调用流程：通过serviceConfig.RegisterEventHandler()方法实现了在serviceConfig中的handleAddService()等方法中调用proxier中的OnServiceAdd()等对应的方法。那么serviceConfig.handleAddService()等方法是在哪里以及何时被调用的呢？再次回看serviceConfig的实例化方法 NewServiceConfig() 挖掘handleAddService()的被调用处。\n// pkg/proxy/config/config.go func NewServiceConfig(serviceInformer coreinformers.ServiceInformer, resyncPeriod time.Duration) *ServiceConfig { result := \u0026amp;ServiceConfig{ lister: serviceInformer.Lister(), listerSynced: serviceInformer.Informer().HasSynced, } //结构体cache.ResourceEventHandlerFuncs 是一个ResourceEventHandler接口类型(staing/src/k8s.io/client-go/tools/cache/controller.go) //将ServicConfig 结构体的handleAddService 等方法赋予了cache.ResourceEventHandlerFuncs,实现一个ResourceEventHandler实例 //serviceInformer.Informer() 返回一个sharedIndexInformer 实例(staing/src/k8s.io/client-go/tools/cache/shared_informer.go) //通过AddEventHandlerWithResyncPeriod() 方法，将ResourceEventHandler实例赋值给processorListener结构体的handler属性 serviceInformer.Informer().AddEventHandlerWithResyncPeriod( cache.ResourceEventHandlerFuncs{ AddFunc: result.handleAddService, UpdateFunc: result.handleUpdateService, DeleteFunc: result.handleDeleteService, }, resyncPeriod, ) return result }  看完上面的注释，大概就明白了proxier 中的OnServiceAdd() 等法法的调用流程 在上边代码serviceInformer.Informer()返回之前，还将调用InformerFor()方法给informerFactory的informers属性赋值f.informers[informerType] = informer, 此行代码的意义可理解为：从api server 监听到 informerType类型资源变化的处理者记录(映射)为informer。此处的资源类型即为service, informer 便为sharedIndexInformer。\nfunc (c *ServiceConfig) handleAddService(obj interface{}) { service, ok := obj.(*v1.Service) ... for i := range c.eventHandlers { klog.V(4).Info(\u0026quot;Calling handler.OnServiceAdd\u0026quot;) c.eventHandlers[i].OnServiceAdd(service) } }  在调用 handleAddService() 中遍历调用 eventHandlers 的 OnServiceAdd() 方法\n具体的调用时机和最上层方法入口还要从informerFactory这个东西说起，这又是k8s 中另一个比较系统的公共组件的实现原理了，即client-go的SharedInformer。\npkg/proxy/apis/config/register.go const GroupName = \u0026quot;kubeproxy.config.k8s.io\u0026quot;   这个用途是什么？\n 记录资源变化 上面介绍了ResourceHandler 的注册及被调用过程。 Proxier 实现了 services 和 endpoints 事件各种最终的观察者，最终的事件触发都会在 proxier 中进行处理。对于通过监听 API Server 变化的信息，通过调用ResourceHandler将变化的信息保存到 endpointsChanges 和 serviceChanges。那么一个ResourceHandler是如何实现的呢？service 和endpoints 的变化如何记录为servriceChanges 和endpointsChanges？回看上边源码中被注册的对象s.ServiceEventHandler，s.EndpointsEventHandler的具体实现便可明白。\nservice 和endpoints 的处理原则相似，以对servcie 的处理为例，看一下对service 的处理方法。\nservice 和endpoints 的处理原则相似，以对servcie 的处理为例，看一下对service 的处理方法。\n//pkg/proxy/iptables/proxier.go func (proxier *Proxier) OnServiceAdd(service *v1.Service) { proxier.OnServiceUpdate(nil, service) } func (proxier *Proxier) OnServiceUpdate(oldService, service *v1.Service) { if proxier.serviceChanges.Update(oldService, service) \u0026amp;\u0026amp; proxier.isInitialized() { proxier.syncRunner.Run() // 通过channel 发送一个信号，调用tryRun() } } func (proxier *Proxier) OnServiceDelete(service *v1.Service) { proxier.OnServiceUpdate(service, nil) }  从上边代码中，可以看到，对service的处理方法大致分为三种：\n增加一个service 删除一个service 处理一个已存在的service的变化。\n其中，增加、删除service 都是给OnServiceUpdate() 传入参数后，由OnServiceUpdate() 方法处理。因此，重点看一下OnServiceUpdate()调用的update() 方法的实现。\nproxy 用 ServiceChangeTracker 来记录资源的变化情况，ServiceChangeTracker 是在 NewProxier() 函数中创建的，它用了一个双重的 map，外面一层 map 如下：\ntype ServiceChangeTracker struct { items map[types.NamespacedName]*serviceChange }  其中每个服务用 \u0026lt;namespace\u0026gt;/\u0026lt;name\u0026gt; 来标识，服务的变化存储在 serviceChange 中。\ntype serviceChange struct { previous ServiceMap current ServiceMap }  previous 是变化之前的状态，current 是变化之后的状态。\ntype ServiceMap map[ServicePortName]ServicePort // ServicePortName carries a namespace + name + portname. This is the unique // identifier for a load-balanced service. type ServicePortName struct { types.NamespacedName Port string } // ServicePort is an interface which abstracts information about a service. type ServicePort interface { // String returns service string. An example format can be: `IP:Port/Protocol`. String() string // ClusterIPString returns service cluster IP in string format. ClusterIPString() string // GetProtocol returns service protocol. GetProtocol() v1.Protocol // GetHealthCheckNodePort returns service health check node port if present. If return 0, it means not present. GetHealthCheckNodePort() int // GetNodePort returns a service Node port if present. If return 0, it means not present. GetNodePort() int }  通过 ServicePort 来表示一个服务。\n通过 ServiceChangeTracker.Update() 来实现服务的更新。\n//pkg/proxy/service.go func (sct *ServiceChangeTracker) Update(previous, current *v1.Service) bool { svc := current if svc == nil { svc = previous } // previous == nil \u0026amp;\u0026amp; current == nil is unexpected, we should return false directly. if svc == nil { return false } namespacedName := types.NamespacedName{Namespace: svc.Namespace, Name: svc.Name} sct.lock.Lock() defer sct.lock.Unlock() change, exists := sct.items[namespacedName] if !exists { // 在serviceChanges 中不存在一个以namespacedName 为key 的资源 change = \u0026amp;serviceChange{} // 初始化一个serviceChange change.previous = sct.serviceToServiceMap(previous) sct.items[namespacedName] = change } change.current = sct.serviceToServiceMap(current) // if change.previous equal to change.current, it means no change if reflect.DeepEqual(change.previous, change.current) { // 从update传递进来的资源没有变化，则从serviceChanges中删除。 delete(sct.items, namespacedName) } return len(sct.items) \u0026gt; 0 }  update 方法就是根据previous ,current 参数新生成一个change 或者修改一个存在的change。并且把无变化的资源从serviceChanges 中删除。serviceChanges.items 会在将变化信息更新到proxier.serviceMap 后清空。\n限流同步机制 在对proxy server 关心的资源变化进行了监听记录之后，最后从s.Proxier.SyncLoop()进入proxier.syncRunner.Loop()方法，由proxier.syncRunner 对托管syncProxyRules() ，syncProxyRules() 实现了修改iptables规则的具体流程。此处值得留意的是proxier.syncRunner采用“令牌桶”算法实现了限流的同步控制。\nfunc (proxier *Proxier) SyncLoop() { proxier.syncRunner.Loop(wait.NeverStop) }  //pkg/utils/async/bounded_frequency_runner.go func (bfr *BoundedFrequencyRunner) Loop(stop \u0026lt;-chan struct{}) { klog.V(3).Infof(\u0026quot;%s Loop running\u0026quot;, bfr.name) bfr.timer.Reset(bfr.maxInterval) for { select { case \u0026lt;-stop: bfr.stop() klog.V(3).Infof(\u0026quot;%s Loop stopping\u0026quot;, bfr.name) return //先确认是否到了运行时机，如果可以运行，就调用syncProxyRules()，之后重新计时。 //具体参考Timer 的实现机制 case \u0026lt;-bfr.timer.C(): bfr.tryRun() case \u0026lt;-bfr.run: //收到一个channel信号 bfr.tryRun() } } }  修改 Iptables 规则 介绍了资源监听、记录和同步机制，再来看一下kube-proxy是如何将资源的变化反馈到iptables规则中的。在iptables的代理模式中，syncProxyRule()方法实现了修改iptables规则的细节流程。走读分析该方法，能将明白在node节点观察到的新链及规则产生的方式及目的。\nsyncProxyRules()这一单个方法的代码较长（约700+ 行），具体的细节功能也多，本节将对syncProxyRules()里的代码执行流程分开介绍。\n 更新proxier.endpointsMap，proxier.servieMap。\nproxier.serviceMap：把sercvieChanges.current 写入proxier.serviceMap，再把存在于sercvieChanges.previous 但不存在于sercvieChanges.current 的service 从 proxier.serviceMap中删除，并且删除的时候，把使用UDP协议的cluster_ip 记录于UDPStaleClusterIP 。\nproxier.endpointsMap：把endpointsChanges.previous 从proxier.endpointsMap 删除，再把endpointsChanges.current 加入proxier.endpointsMap。把存在于endpointsChanges.previous 但不存在于endpointsChanges.current 的endpoint 组装为ServiceEndpoint 结构，把该结构记录于staleEndpoints。\n​具体相关代码流程如下：\n//kubernetes/pkg/proxy/iptables/proxier.go func (proxier *Proxier) syncProxyRules() { ... serviceUpdateResult := proxy.UpdateServiceMap(proxier.serviceMap, proxier.serviceChanges) endpointUpdateResult := proxy.UpdateEndpointsMap(proxier.endpointsMap, proxier.endpointsChanges) staleServices := serviceUpdateResult.UDPStaleClusterIP // 利用endpointUpdateResult.StaleServiceNames，再次更新 staleServices for _, svcPortName := range endpointUpdateResult.StaleServiceNames { if svcInfo, ok := proxier.serviceMap[svcPortName]; ok \u0026amp;\u0026amp; svcInfo != nil \u0026amp;\u0026amp; svcInfo.GetProtocol() == v1.ProtocolUDP { klog.V(2).Infof(\u0026quot;Stale udp service %v -\u0026gt; %s\u0026quot;, svcPortName, svcInfo.ClusterIPString()) staleServices.Insert(svcInfo.ClusterIPString()) } } ... } //kubernetes/pkg/proxy/servcie.go func UpdateServiceMap(serviceMap ServiceMap, changes *ServiceChangeTracker) (result UpdateServiceMapResult) { result.UDPStaleClusterIP = sets.NewString() // apply 方法中，继续调用了merge，filter, umerge // merge:将change.current的servicemap 信息合入proxier.servicemap中。 // filter:将change.previous和change.current共同存在的servicemap从将change.previous删除 // unmerge: 将change.previous 中使用UDP 的servicemap 从 proxier.serviceMap 中删除，并记录删除的服务IP 到UDPStaleClusterIP //apply中最后重置了proxy.serviceChanges.items serviceMap.apply(changes, result.UDPStaleClusterIP) //HCServiceNodePorts 保存proxier.serviceMap 中所有服务的健康检查端口 result.HCServiceNodePorts = make(map[types.NamespacedName]uint16) for svcPortName, info := range serviceMap { if info.GetHealthCheckNodePort() != 0 { result.HCServiceNodePorts[svcPortName.NamespacedName] = uint16(info.GetHealthCheckNodePort()) } } return result } //kubernetes/pkg/proxy/endpoints.go func UpdateEndpointsMap(endpointsMap EndpointsMap, changes *EndpointChangeTracker) (result UpdateEndpointMapResult) { result.StaleEndpoints = make([]ServiceEndpoint, 0) result.StaleServiceNames = make([]ServicePortName, 0) //从proixer.endpointsMap 中删除和change.previous 相同的elelment. // 将change.current 添加至proixer.endpointsMap // StaleEndpoints 保存了存在于previous 但不存在current的endpoints // StaleServicenames保存了一种ServicePortName,这样的ServicePortName在change.previous不存在对应的endpoints，在change.current存在endpoints。 // 最后重置了了proxy.endpointsChanges.items endpointsMap.apply(changes, \u0026amp;result.StaleEndpoints, \u0026amp;result.StaleServiceNames) // computing this incrementally similarly to endpointsMap. result.HCEndpointsLocalIPSize = make(map[types.NamespacedName]int) localIPs := GetLocalEndpointIPs(endpointsMap) for nsn, ips := range localIPs { result.HCEndpointsLocalIPSize[nsn] = len(ips) } return result }   在准好了更新iptables需要的资源变量后，接下来就是调用iptables 命令建立了自定义链，并在对应的内核链上引用这些自定义链。这些自定义链在k8s 服务中是必须的，不会跟随资源变化而变化，所以在更新规则之前，提前无条件生成这些链，做好准备工作，随后会在这些自定义链上创建相应的规则。  相关代码如下：\nfor _, chain := range iptablesJumpChains { if _, err := proxier.iptables.EnsureChain(chain.table, chain.chain); err != nil { //创建链 klog.Errorf(\u0026quot;Failed to ensure that %s chain %s exists: %v\u0026quot;, chain.table, kubeServicesChain, err) return } args := append(chain.extraArgs, \u0026quot;-m\u0026quot;, \u0026quot;comment\u0026quot;, \u0026quot;--comment\u0026quot;, chain.comment, \u0026quot;-j\u0026quot;, string(chain.chain), ) if _, err := proxier.iptables.EnsureRule(utiliptables.Prepend, chain.table, chain.sourceChain, args...); err != nil { // 引用链 klog.Errorf(\u0026quot;Failed to ensure that %s chain %s jumps to %s: %v\u0026quot;, chain.table, chain.sourceChain, chain.chain, err) return } }  上边代码完成的iptables命令如下：\niptables -w -N KUBE-EXTERNAL-SERVICES -t filter iptables -w -I INPUT -t filter -m conntrack --ctstate NEW -m comment --comment -j KUBE-EXTERNAL-SERVICES kubernetes externally-visible service portals iptables -w -N KUBE-SERVICES -t filter iptables -w -I OUTPUT -t filter -m conntrack --ctstate NEW -m comment --comment -j KUBE-SERVICES kubernetes service portals iptables -w -N KUBE-SERVICES -t nat iptables -w -I OUTPUT -t nat -m conntrack --ctstate NEW -m comment --comment -j KUBE-SERVICES kubernetes service portals iptables -w -N KUBE-SERVICES -t nat iptables -w -I PREROUTING -t nat -m conntrack --ctstate NEW -m comment --comment -j KUBE-SERVICES kubernetes service portals iptables -w -N KUBE-POSTROUTING -t nat iptables -w -I POSTROUTING -t nat -m conntrack --ctstate NEW -m comment --comment -j KUBE-POSTROUTING kubernetes postrouting rules iptables -w -N KUBE-FORWARD -t filter iptables -w -I FORWARD -t filter -m conntrack --ctstate NEW -m comment --comment -j KUBE-FORWARD kubernetes forwarding rules  将当前内核中filter表和nat 表中的全部规则临时导出到数个buffer，具体的：\n 使用 proxier.existingFilterChainsData 保存filter表的信息 使用 existingFilterChains保存 proxier.existingFilterChainsData 的chain 信息 使用 proxier.iptablesData 保存nat 表的信息 使用 existingNATChains 保存 proxier.iptablesData 的chain 信息 重置 proxier.filterChains，proxier.filterRules，proxier.natChains，proxier.natRules 四个buffer , 这四个buffer 用来缓存最新的关于k8s 服务于endpoints 的 iptables 信息。  在上面准备工作做好之后，开始向上述四个buffer中根据条件不断追加内容，缓存内容在同步规则的最后环节刷入内核。\n  问题  kube proxy 读取到的数据结构是怎样的？ （输入）\n读取到的数据是监听 API server 的 Service 和 Endpoint 的数据。\n kube proxy 写出的数据机构是怎样的？ （输出）\n通过 Service 和 Endpoint 组装 iptables rules。\n pod 如何发现和路由到远端的 pod？ （egress）\n参考问题 1 和 2 的答案\n node 接收到流量后怎么路由到相应的 pod 上的？ （ingress）\n收到的报文的目的 IP 是 Pod 的 IP，Node 上有到 Pod 的路由直接转发给 Pod。\n kube-proxy 有多种实现方式：userspace、iptables、ipvs，如何分别实现的？\n可以在命令行中指定模式，用 interface ProxyProvider，每种模式都会有一个 Proxier 来实现这个 interface。\n Proxy 如何解决了同一主宿机相同服务端口冲突的问题？\nPod 的 IP 不一样，即使端口冲突也没关系。\n ServiceInformer 是如何工作的？\nlist-watch 机制如何实现的？\n ServicePortName 为什么是负载均衡的唯一标识？\n// ServicePortName carries a namespace + name + portname. This is the unique // identifier for a load-balanced service. type ServicePortName struct { types.NamespacedName Port string }\n 既然 kube-proxy 能够自动监听 apiserver 的变化，并更新 iptables，为什么这里还要再每隔一段时间强制同步一次呢？\n我的看法是这只是安全防护措施，来规避有些情况（比如代码 bug，或者网络、环境问题等原因）下数据可能没有及时同步。\n 更新 iptable rule 的时候用什么策略？如果直接更新规则会影响正在转发的流量吗？\n只会影响有变化的配置，已经存在的规则不会变，先写入更新后的规则，然后删除旧的规则。\n  总结 node节点上的iptables中有到达所有service的规则，service 的cluster IP并不是一个实际的IP，它的存在只是为了找出实际的endpoint地址，对达到cluster IP的报文都要进行DNAT为Pod IP(+port)，不同node上的报文实际上是通过POD IP传输的，cluster IP只是本node节点的一个概念，用于查找并DNAT，即目的地址为clutter IP的报文只是本node发送的，其他节点不会发送(也没有路由支持)，即默认下cluster ip仅支持本node节点的service访问，如果需要跨node节点访问，可以使用插件实现，如flannel，它将pod ip进行了封装\n最后用两张图总结一下 kube-proxy 更新iptables 的流程\n1) 资源更新信息来源\n2) 链建立及规则导向\n另外：对于数据包的出入口，有这么一句心得：只要你站在内核的角度理解，无论从虚拟网卡还是物理网卡收到一个包，对内核来说都是收包，都是prerouting链开始。无论一个包去往物理网卡还是虚拟网卡，对内核来说都是发出，都是从postrouting结束。本机进程收到就是input链，本机进程发出就是output链。\n参考 我是怎么阅读kubernetes源代码的？\nkube-proxy 源码解析\n理解kubernetes环境的iptables\nkube-proxy 源码分析\n","date":1559692800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559692800,"objectID":"1801322119fd2ed28aed808b26e7e68e","permalink":"/post/cloud/k8s/201901-how-read-kubeproxy/","publishdate":"2019-06-05T00:00:00Z","relpermalink":"/post/cloud/k8s/201901-how-read-kubeproxy/","section":"post","summary":"Kube-proxy 把访问 Service VIP 的请求转发到运行的 Pods 上，并实现负载均衡","tags":["Kubernetes"],"title":"Kube proxy 源码解读","type":"post"},{"authors":null,"categories":null,"content":" Kubernetes 本身不提供容器网络, 但是实现了一套支持多种网络插件的框架代码, 通过调用网络插件来为容器设置网络环境。\n而约束网络插件的是 CNI（Container Network Interface），一种标准的容器网络接口，定义了如何将容器加入网络和将容器从网络中删除。\nCNI 接口由 runtime 在创建容器和删除容器时调用。具体的接口定义如下：\n// vendor/github.com/containernetworking/cni/libcni/api.go type CNI interface { AddNetworkList(net *NetworkConfigList, rt *RuntimeConf) (types.Result, error) DelNetworkList(net *NetworkConfigList, rt *RuntimeConf) error AddNetwork(net *NetworkConfig, rt *RuntimeConf) (types.Result, error) DelNetwork(net *NetworkConfig, rt *RuntimeConf) error }  Kubernetes plugin 接口 kubelet 是通过 NetworkPlugin interface 来调用底层的网络插件为容器设置网络环境.\n// kubelet/dockershim/network/plugins.go // Plugin is an interface to network plugins for the kubelet type NetworkPlugin interface { // Init initializes the plugin. This will be called exactly once // before any other methods are called. Init(host Host, hairpinMode kubeletconfig.HairpinMode, nonMasqueradeCIDR string, mtu int) error // Called on various events like: // NET_PLUGIN_EVENT_POD_CIDR_CHANGE Event(name string, details map[string]interface{}) // Name returns the plugin's name. This will be used when searching // for a plugin by name, e.g. Name() string // Returns a set of NET_PLUGIN_CAPABILITY_* Capabilities() utilsets.Int // SetUpPod is the method called after the infra container of // the pod has been created but before the other containers of the // pod are launched. SetUpPod(namespace string, name string, podSandboxID kubecontainer.ContainerID, annotations, options map[string]string) error // TearDownPod is the method called before a pod's infra container will be deleted TearDownPod(namespace string, name string, podSandboxID kubecontainer.ContainerID) error // GetPodNetworkStatus is the method called to obtain the ipv4 or ipv6 addresses of the container GetPodNetworkStatus(namespace string, name string, podSandboxID kubecontainer.ContainerID) (*PodNetworkStatus, error) // Status returns error if the network plugin is in error state Status() error }  实现了 NetworkPlugin interface 就可以新增一种 Kubernetes 的 Network plugin。这个 interface 也并没有具体容器网络的实现，而是做了一层封装，具体的容器网络由独立的二进制实现，比如官方提供的 bridge、host-local 或者第三方的 calico、flannel 等，也可以是自己定制的实现。\nK8S 支持两种 plugin：\n cniNetworkPlugin kubenetNetworkPlugin  下面讲述 plugin 是如何初始化和工作的\nkubelet 启动 kubelet 启动后会调用 run() 进入处理流程，在进入主处理流程之前的初始化阶段会根据用户配置的网络插件名选择对应的网络插件。\n// cmd/kubelet/app/server.go func run(s *options.KubeletServer, kubeDeps *kubelet.KubeletDeps) (err error) { ... // 创建 kubelete // 根据 kubelet 的运行参数运行 kubelet // 这里会根据用户配置的网络插件名选择网络插件 if err := RunKubelet(\u0026amp;s.KubeletConfiguration, kubeDeps, s.RunOnce, standaloneMode); err != nil { return err } ... }  // cmd/kubelet/app/server.go func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error { ... k, err := CreateAndInitKubelet(\u0026amp;kubeServer.KubeletConfiguration, kubeDeps, ...) ... }  func CreateAndInitKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, ...) { k, err = kubelet.NewMainKubelet(kubeCfg, kubeDeps, crOptions ... ) k.BirthCry() k.StartGarbageCollection() }  // NewMainKubelet instantiates a new Kubelet object along with all the required internal modules. // No initialization of Kubelet and its modules should happen here. func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies ...) { klet := \u0026amp;Kubelet{ hostname: hostname, hostnameOverridden: len(hostnameOverride) \u0026gt; 0, ... } switch containerRuntime { case kubetypes.DockerContainerRuntime: // Create and start the CRI shim running as a grpc server. streamingConfig := getStreamingConfig(kubeCfg, kubeDeps, crOptions) ds, err := dockershim.NewDockerService(kubeDeps.DockerClientConfig, crOptions.PodSandboxImage, streamingConfig, \u0026amp;pluginSettings, runtimeCgroups, kubeCfg.CgroupDriver, crOptions.DockershimRootDirectory, !crOptions.RedirectContainerStreaming) ... server := dockerremote.NewDockerServer(remoteRuntimeEndpoint, ds) if err := server.Start(); err != nil { return nil, err } case kubetypes.RemoteContainerRuntime: // No-op. break default: return nil, fmt.Errorf(\u0026quot;unsupported CRI runtime: %q\u0026quot;, containerRuntime) } // 向 k8s network plugin 通报 NET_PLUGIN_EVENT_POD_CIDR_CHANGE 事件 // 该事件将会被 NetworkPlugin interface 的 Event 方法捕获 if _, err := klet.updatePodCIDR(kubeCfg.PodCIDR); err != nil { klog.Errorf(\u0026quot;Pod CIDR update failed %v\u0026quot;, err) } ... }  目前只支持 CRI 为 docker。\n根据用户配置选择 CNI // pkg/kubelet/dockershim/docker_service.go // NOTE: Anything passed to DockerService should be eventually handled in another way when we switch to running the shim as a different process. func NewDockerService(config *ClientConfig, podSandboxImage string, ...) (DockerService, error) { ds := \u0026amp;dockerService{ client: c, os: kubecontainer.RealOS{}, podSandboxImage: podSandboxImage, streamingRuntime: \u0026amp;streamingRuntime{ client: client, execHandler: \u0026amp;NativeExecHandler{}, }, containerManager: cm.NewContainerManager(cgroupsName, client), checkpointManager: checkpointManager, startLocalStreamingServer: startLocalStreamingServer, networkReady: make(map[string]bool), } // Determine the hairpin mode. if err := effectiveHairpinMode(pluginSettings); err != nil { // This is a non-recoverable error. Returning it up the callstack will just // lead to retries of the same failure, so just fail hard. return nil, err } // 根据配置配置 CNI // dockershim currently only supports CNI plugins. pluginSettings.PluginBinDirs = cni.SplitDirs(pluginSettings.PluginBinDirString) cniPlugins := cni.ProbeNetworkPlugins(pluginSettings.PluginConfDir, pluginSettings.PluginBinDirs) // 加了一个默认的 CNI 插件 kubenet cniPlugins = append(cniPlugins, kubenet.NewPlugin(pluginSettings.PluginBinDirs)) netHost := \u0026amp;dockerNetworkHost{ \u0026amp;namespaceGetter{ds}, \u0026amp;portMappingGetter{ds}, } // 根据用户配置选择对应的网络插件对象，做 init() 初始化 plug, err := network.InitNetworkPlugin(cniPlugins, pluginSettings.PluginName, netHost, pluginSettings.HairpinMode, pluginSettings.NonMasqueradeCIDR, pluginSettings.MTU) ds.network = network.NewPluginManager(plug) klog.Infof(\u0026quot;Docker cri networking managed by %v\u0026quot;, plug.Name()) return ds, nil }  Hairpin 模式\n发夹式转发模式 (Hairpin mode)又称反射式转发模式 (Reflective Relay) ，指交换机可以将报文的接受端口同时作为发送端口, 即报文可以从它的入端口转发出去, 如下图所示:\nNewDockerService() 函数首先通过 effectiveHairpinMode() 计算出有效的 Hairpin 模式, 然后根据 NetworkPluginName 从插件列表中选择对应的网络插件对象.\nProbeNetworkPlugins() 根据配置的 CNI 插件的路径生成 network.NetworkPlugin interface 的实现 cniNetworkPlugin。\nInitNetworkPlugin() 负责从网络插件对象列表中根据用户配置的网络插件名选择对应的网络插件对象，调用插件的 init() 执行初始化。\n// pkg/kubelet/dockershim/network/plugins.go // InitNetworkPlugin inits the plugin that matches networkPluginName. Plugins must have unique names. func InitNetworkPlugin(plugins []NetworkPlugin, networkPluginName string, host Host, hairpinMode kubeletconfig.HairpinMode, nonMasqueradeCIDR string, mtu int) (NetworkPlugin, error) { // 如果用户没有配置网络插件名, 默认就是NoopNetworkPlugin, 不会提供任何容器网络 // NoopNetworkPlugin 是 NetworkPlugin interface 的实现 if networkPluginName == \u0026quot;\u0026quot; { // default to the no_op plugin plug := \u0026amp;NoopNetworkPlugin{} plug.Sysctl = utilsysctl.New() if err := plug.Init(host, hairpinMode, nonMasqueradeCIDR, mtu); err != nil { return nil, err } return plug, nil } ... chosenPlugin := pluginMap[networkPluginName] if chosenPlugin != nil { // 执行插件的初始化操作 err := chosenPlugin.Init(host, hairpinMode, nonMasqueradeCIDR, mtu) } ... }  通告 Pod CIDR 的更新 k8s 对 Pod 的管理是通过 runtime 来操作的，因此对 CIDR 的更新也是通过 runtime 实现。当 Pod 的 CIDR 更新时调用 runtime 的 UpdatePodCIDR()。\n// pkg/kubelet/kubelet_network.go // updatePodCIDR updates the pod CIDR in the runtime state if it is different // from the current CIDR. Return true if pod CIDR is actually changed. func (kl *Kubelet) updatePodCIDR(cidr string) (bool, error) { // 配置与当前状态比较，没有变化直接返回 podCIDR := kl.runtimeState.podCIDR() if podCIDR == cidr { return false, nil } // kubelet -\u0026gt; generic runtime -\u0026gt; runtime shim -\u0026gt; network plugin // docker/non-cri implementations have a passthrough UpdatePodCIDR if err := kl.getRuntime().UpdatePodCIDR(cidr); err != nil { // If updatePodCIDR would fail, theoretically pod CIDR could not change. // But it is better to be on the safe side to still return true here. return true, fmt.Errorf(\u0026quot;failed to update pod CIDR: %v\u0026quot;, err) } // 更新当前状态，以便以后比较 kl.runtimeState.setPodCIDR(cidr) return true, nil }  runtime 要先讲下 k8s runtime 的管理。\nk8s 通过 kubeGenericRuntimeManager 来做统一的 RC 管理，该类会调用对应的 RC shim 来做下发操作。\nkubelet 的 containerRuntime 是在 NewMainKubelet() 函数中如下代码片段配置的。\nruntime, err := kuberuntime.NewKubeGenericRuntimeManager( kubecontainer.FilterEventRecorder(kubeDeps.Recorder), ...) klet.containerRuntime = runtime klet.streamingRuntime = runtime klet.runner = runtime  // kuberuntime/kuberuntime_manager.go // UpdatePodCIDR is just a passthrough method to update the runtimeConfig of the shim // with the podCIDR supplied by the kubelet. func (m *kubeGenericRuntimeManager) UpdatePodCIDR(podCIDR string) error { // TODO(#35531): do we really want to write a method on this manager for each // field of the config? klog.Infof(\u0026quot;updating runtime config through cri with podcidr %v\u0026quot;, podCIDR) return m.runtimeService.UpdateRuntimeConfig( \u0026amp;runtimeapi.RuntimeConfig{ NetworkConfig: \u0026amp;runtimeapi.NetworkConfig{ PodCidr: podCIDR, }, }) }  kubeGenericRuntimeManager 的 runtimeService 在初始化时设置的是 instrumentedRuntimeService，这个结构是对 RuntimeService interface 的一个封装和实现，用来记录操作和错误的 metrics。\n// instrumentedRuntimeService wraps the RuntimeService and records the operations // and errors metrics. type instrumentedRuntimeService struct { service internalapi.RuntimeService }  而真正的 RuntimeService interface 的实现是在 NewMainKubelet() 的如下片段中赋值的。\nruntimeService, imageService, err := getRuntimeAndImageServices(remoteRuntimeEndpoint, remoteImageEndpoint, kubeCfg.RuntimeRequestTimeout) klet.runtimeService = runtimeService  remoteRuntimeEndpoint 是在 kubelet 启动命令中指定的值为 unix:///var/run/dockershim.sock ，kubelet 就是通过这个 socket 与 runtime 进行gRPC 通信的。保存在 KubeletFlags 中，该参数在\ntype KubeletFlags struct { KubeConfig string ... RemoteRuntimeEndpoint string RemoteImageEndpoint string }  getRuntimeAndImageServices() 调用 NewRemoteRuntimeService() 根据 RC 的 endpoint 创建一个 gRPC 的 client 封装到 RemoteRuntimeService 中，这是一个 internalapi.RuntimeService interface 的具体实现。\n// NewRemoteRuntimeService creates a new internalapi.RuntimeService. func NewRemoteRuntimeService(endpoint string, connectionTimeout time.Duration) (internalapi.RuntimeService, error) { addr, dailer, err := util.GetAddressAndDialer(endpoint) ctx, cancel := context.WithTimeout(context.Background(), connectionTimeout) defer cancel() conn, err := grpc.DialContext(ctx, addr, grpc.WithInsecure(), grpc.WithDialer(dailer), grpc.WithDefaultCallOptions(grpc.MaxCallRecvMsgSize(maxMsgSize))) return \u0026amp;RemoteRuntimeService{ timeout: connectionTimeout, runtimeClient: runtimeapi.NewRuntimeServiceClient(conn), lastError: make(map[string]string), errorPrinted: make(map[string]time.Time), }, nil }  runtime 主要提供两种服务：RuntimeService 和 ImageService 用来管理容器的镜像。 k8s 与 runtime 通过 RPC 通信，在配置 podCIDR 时调用的是 RuntimeService 的 UpdateRuntimeConfig rpc：\n// pkg/kubelet/apis/cri/runtime/v1alpha2/api.proto service RuntimeService { ... // UpdateRuntimeConfig updates the runtime configuration based on the given request. rpc UpdateRuntimeConfig(UpdateRuntimeConfigRequest) returns (UpdateRuntimeConfigResponse) {} }  这样网络配置就下发给了 runtime，runtime 调用 CNI 插件来做网络配置变更。\n// pkg/kubelet/dockershim/docker_service.go // UpdateRuntimeConfig updates the runtime config. Currently only handles podCIDR updates. func (ds *dockerService) UpdateRuntimeConfig(_ context.Context, r *runtimeapi.UpdateRuntimeConfigRequest) (*runtimeapi.UpdateRuntimeConfigResponse, error) { runtimeConfig := r.GetRuntimeConfig() if runtimeConfig == nil { return \u0026amp;runtimeapi.UpdateRuntimeConfigResponse{}, nil } klog.Infof(\u0026quot;docker cri received runtime config %+v\u0026quot;, runtimeConfig) if ds.network != nil \u0026amp;\u0026amp; runtimeConfig.NetworkConfig.PodCidr != \u0026quot;\u0026quot; { event := make(map[string]interface{}) event[network.NET_PLUGIN_EVENT_POD_CIDR_CHANGE_DETAIL_CIDR] = runtimeConfig.NetworkConfig.PodCidr ds.network.Event(network.NET_PLUGIN_EVENT_POD_CIDR_CHANGE, event) } return \u0026amp;runtimeapi.UpdateRuntimeConfigResponse{}, nil }  下图是 kubelet runtime UML\nkubenet plugin 实现 前面知道网络插件的接口是 NetworkPlugin interface，k8s kubenet 网络框架用 embed network.NoopNetworkPlugin 的 kubenetNetworkPlugin 实现了接口。\nkubenet 利用的是官方提供的三个 cni 类型插件: bridge, host-local, loopback (参考 cni plugins, cni ipam), 这个插件一般位于每个 Node 的 /opt/cni/bin 目录。\ntype kubenetNetworkPlugin struct { network.NoopNetworkPlugin host network.Host netConfig *libcni.NetworkConfig loConfig *libcni.NetworkConfig cniConfig libcni.CNI bandwidthShaper bandwidth.BandwidthShaper mu sync.Mutex //Mutex for protecting podIPs map, netConfig, and shaper initialization podIPs map[kubecontainer.ContainerID]string mtu int execer utilexec.Interface nsenterPath string hairpinMode kubeletconfig.HairpinMode // kubenet can use either hostportSyncer and hostportManager to implement hostports // Currently, if network host supports legacy features, hostportSyncer will be used, // otherwise, hostportManager will be used. hostportSyncer hostport.HostportSyncer hostportManager hostport.HostPortManager iptables utiliptables.Interface sysctl utilsysctl.Interface ebtables utilebtables.Interface // binDirs is passed by kubelet cni-bin-dir parameter. // kubenet will search for CNI binaries in DefaultCNIDir first, then continue to binDirs. binDirs []string nonMasqueradeCIDR string podCidr string gateway net.IP }  kubenet 直接利用了官方提供的三个 cni plugin:\n// pkg/kubelet/network/kubenet/kubenet_linux.go // CNI plugins required by kubenet in /opt/cni/bin or vendor directory var requiredCNIPlugins = [...]string{\u0026quot;bridge\u0026quot;, \u0026quot;host-local\u0026quot;, \u0026quot;loopback\u0026quot;}  kubenet 网络框架原理非常的简单, 主要利用 \u0026ldquo;bridge\u0026rdquo;, \u0026ldquo;host-local\u0026rdquo;, \u0026ldquo;loopback\u0026rdquo; (位于 /opt/cni/bin 目录下) 这三个 cni plugin主要的功能：\n 在每个 Node 上创建一个 cbr0 网桥 根据 PodCIDR 为每个 Pod 的 interface 分配一个 ip, 将该 interface 连接到 cbr0 网桥上.  当然, 对于 kubernetes 集群来说, 还需要解决两个问题:\n Node 的 PodCIDR 设置\nk8s kubenet 网络框架中，必须给每个 node 配置一个 podCIDR.\n那么, 每个 Node 的 PodCIDR 如何设置呢? 这个需要参考 kubenet 网络的配置文档了:\n The node must be assigned an IP subnet through either the \u0026ndash;pod-cidr kubelet command-line option or the \u0026ndash;allocate-node-cidrs=true \u0026ndash;cluster-cidr= controller-manager command-line options.\n 其实就是两种方式:\n 通过 \u0026ndash;pod-cidr 为每个 Node 上的 kubelet 配置好 PodCIDR 通过 \u0026ndash;allocate-node-cidrs=true \u0026ndash;cluster-cidr= 让 controller-manager 来为每个 Node 分配 PodCIDR.  Node 之间的路由设置\n虽然现在每个 Node 都配置好了 PodCIDR, 比如:\nNode1: 192.168.0.0/24 Node2: 192.168.1.0/24  但是 Node1 和 Node2 上的容器如何通信呢?\n It is typically used together with a cloud provider that sets up routing rules for communication between nodes, or in single-node environments.\n 通常情况下, kubenet 网络插件会跟 cloud provider 一起使用, 从而利用 cloud provider 来设置节点间的路由. kubenet 网络插件也可以用在单节点环境, 这样就不需要考虑 Node 间的路由了. 另外, 我们还可以通过实现一个 network controller 来保证 Node 间的路由.\n  kubenet Init // pkg/kubelet/dockershim/network/kubenet/kubenet_linux.go func NewPlugin(networkPluginDirs []string) network.NetworkPlugin { protocol := utiliptables.ProtocolIpv4 execer := utilexec.New() dbus := utildbus.New() sysctl := utilsysctl.New() iptInterface := utiliptables.New(execer, dbus, protocol) return \u0026amp;kubenetNetworkPlugin{ podIPs: make(map[kubecontainer.ContainerID]string), execer: utilexec.New(), iptables: iptInterface, sysctl: sysctl, binDirs: append([]string{DefaultCNIDir}, networkPluginDirs...), hostportSyncer: hostport.NewHostportSyncer(iptInterface), hostportManager: hostport.NewHostportManager(iptInterface), nonMasqueradeCIDR: \u0026quot;10.0.0.0/8\u0026quot;, } }  在前面的 InitNetworkPlugin() 流程中会调用各个插件的 Init() 来初始化插件。\n// pkg/kubelet/dockershim/network/kubenet/kubenet_linux.go func (plugin *kubenetNetworkPlugin) Init(host network.Host, hairpinMode kubeletconfig.HairpinMode, nonMasqueradeCIDR string, mtu int) error { ... // 确认加载了 br-netfilter，设置 bridge-nf-call-iptables=1 plugin.execer.Command(\u0026quot;modprobe\u0026quot;, \u0026quot;br-netfilter\u0026quot;).CombinedOutput() err := plugin.sysctl.SetSysctl(sysctlBridgeCallIPTables, 1) // 配置 loopback cni 插件 plugin.loConfig, err = libcni.ConfFromBytes([]byte(`{ \u0026quot;cniVersion\u0026quot;: \u0026quot;0.1.0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;kubenet-loopback\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;loopback\u0026quot; }`)) plugin.nsenterPath, err = plugin.execer.LookPath(\u0026quot;nsenter\u0026quot;) // 下发 SNAT 的 ipatable rule // Need to SNAT outbound traffic from cluster if err = plugin.ensureMasqRule(); err != nil { return err } return nil }   在 kubenet 中有个 MTU 的配置选项，network-plugin-mtu 指定 MTU，设置合理的 MTU 能有一个更好的网络性能。仅在 kubenet plugin 中支持。\n kubenet Event kubelet 启动到 NewMainKubelet 时, 根据用户配置通过 klet.updatePodCIDR(kubeCfg.PodCIDR) 向 k8s network plugin 通报 NET_PLUGIN_EVENT_POD_CIDR_CHANGE 事件, 该事件将会被 Event 方法捕获.\n// pkg/kubelet/network/kubenet/kubenet_linux.go const NET_CONFIG_TEMPLATE = `{ \u0026quot;cniVersion\u0026quot;: \u0026quot;0.1.0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;kubenet\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;bridge\u0026quot;, \u0026quot;bridge\u0026quot;: \u0026quot;%s\u0026quot;, \u0026quot;mtu\u0026quot;: %d, \u0026quot;addIf\u0026quot;: \u0026quot;%s\u0026quot;, \u0026quot;isGateway\u0026quot;: true, \u0026quot;ipMasq\u0026quot;: false, \u0026quot;hairpinMode\u0026quot;: %t, \u0026quot;ipam\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;host-local\u0026quot;, \u0026quot;subnet\u0026quot;: \u0026quot;%s\u0026quot;, \u0026quot;gateway\u0026quot;: \u0026quot;%s\u0026quot;, \u0026quot;routes\u0026quot;: [ { \u0026quot;dst\u0026quot;: \u0026quot;0.0.0.0/0\u0026quot; } ] } }` func (plugin *kubenetNetworkPlugin) Event(name string, details map[string]interface{}) { podCIDR, ok := details[network.NET_PLUGIN_EVENT_POD_CIDR_CHANGE_DETAIL_CIDR].(string) _, cidr, err := net.ParseCIDR(podCIDR) if err == nil { setHairpin := plugin.hairpinMode == kubeletconfig.HairpinVeth // Set bridge address to first address in IPNet cidr.IP[len(cidr.IP)-1] += 1 // 更新 cni 网络配置 // 从 NET_CONFIG_TEMPLATE 中看出, host-local ipam 的 subnet 就是 podCIDR // 这其实也就是为什么 k8s kubenet 网络插件需要为每个 node 分配 podCIDR 的原因 json := fmt.Sprintf(NET_CONFIG_TEMPLATE, BridgeName, plugin.mtu, network.DefaultInterfaceName, setHairpin, podCIDR, cidr.IP.String()) // 网络配置都保存在 netConfig 中 plugin.netConfig, err = libcni.ConfFromBytes([]byte(json)) if err == nil { klog.V(5).Infof(\u0026quot;CNI network config:\\n%s\u0026quot;, json) // Ensure cbr0 has no conflicting addresses; CNI's 'bridge' // plugin will bail out if the bridge has an unexpected one plugin.clearBridgeAddressesExcept(cidr) } plugin.podCidr = podCIDR plugin.gateway = cidr.IP } }   todo: Event() 也只是更新了 podCidr 和 netConfig，哪里下发了更新？\n 根据配置的改变设置 kubenetNetworkPlugin 对应的变量。\nkubenet SetUpPod 创建 Pod 的时候会调用该方法，该方法调用 setup() 来完成配置，这个接口最重要的功能是将容器的 eth0 接口加入到了 namespace 中\n// setup sets up networking through CNI using the given ns/name and sandbox ID. func (plugin *kubenetNetworkPlugin) setup(namespace string, name string, id kubecontainer.ContainerID, annotations map[string]string) error { // 添加 loopback interface 到 pod 的 network namespace // Bring up container loopback interface if _, err := plugin.addContainerToNetwork(plugin.loConfig, \u0026quot;lo\u0026quot;, namespace, name, id); err != nil { return err } // 添加 DefaultInterfaceName eth0 到 pod 的 network namespace // Hook container up with our bridge resT, err := plugin.addContainerToNetwork(plugin.netConfig, network.DefaultInterfaceName, namespace, name, id) if err != nil { return err } // Coerce the CNI result version res, err := cnitypes020.GetResult(resT) ip4 := res.IP4.IP.IP.To4() // 为了配置 hairpin 设置网卡混杂模式 ... plugin.podIPs[id] = ip4.String() // TODO: replace with CNI port-forwarding plugin // TODO: portMappings 的用途是什么？ portMappings, err := plugin.host.GetPodPortMappings(id.ID) if err != nil { return err } if portMappings != nil \u0026amp;\u0026amp; len(portMappings) \u0026gt; 0 { if err := plugin.hostportManager.Add(id.ID, \u0026amp;hostport.PodPortMapping{ Namespace: namespace, Name: name, PortMappings: portMappings, IP: ip4, HostNetwork: false, }, BridgeName); err != nil { return err } } return nil }  接着看看 addContainerToNetwork() 方法:\n// pkg/kubelet/dockershim/network/kubenet/kubenet_linux.go func (plugin *kubenetNetworkPlugin) addContainerToNetwork(config *libcni.NetworkConfig, ifName, namespace, name string, id kubecontainer.ContainerID) (cnitypes.Result, error) { rt, err := plugin.buildCNIRuntimeConf(ifName, id, true) if err != nil { return nil, fmt.Errorf(\u0026quot;Error building CNI config: %v\u0026quot;, err) } // The network plugin can take up to 3 seconds to execute, // so yield the lock while it runs. plugin.mu.Unlock() res, err := plugin.cniConfig.AddNetwork(config, rt) plugin.mu.Lock() return res, nil }  由前面 CNI 库接口可知, plugin.cniConfig.AddNetwork() 实际上调用的是 cni plugin 去实现容器网络配置. kubenet plugin 主要通过 loopback 和 bridge cni 插件将容器的 lo 和 eth0 添加到容器网络中. bridge 插件负责 Node 上 cbr0 的创建, 然后创建 veth 接口对, 通过 veth 接口对, 将容器添加到容器网络中. 另外, host-local IPAM plugin 负责为 eth0 分配 ip 地址.\nkubenet TearDownPod 删除 Pod 的时候会被调用。主要是通过函数 teardown() 实现。主要的流程是调用 CNI 删除网络配置。\n// Tears down as much of a pod's network as it can even if errors occur. Returns // an aggregate error composed of all errors encountered during the teardown. func (plugin *kubenetNetworkPlugin) teardown(namespace string, name string, id kubecontainer.ContainerID, podIP string) error { errList := []error{} if err := plugin.delContainerFromNetwork(plugin.netConfig, network.DefaultInterfaceName, namespace, name, id); err != nil { // This is to prevent returning error when TearDownPod is called twice on the same pod. This helps to reduce event pollution. if podIP != \u0026quot;\u0026quot; { klog.Warningf(\u0026quot;Failed to delete container from kubenet: %v\u0026quot;, err) } else { errList = append(errList, err) } } portMappings, err := plugin.host.GetPodPortMappings(id.ID) if err != nil { errList = append(errList, err) } else if portMappings != nil \u0026amp;\u0026amp; len(portMappings) \u0026gt; 0 { if err = plugin.hostportManager.Remove(id.ID, \u0026amp;hostport.PodPortMapping{ Namespace: namespace, Name: name, PortMappings: portMappings, HostNetwork: false, }); err != nil { errList = append(errList, err) } } return utilerrors.NewAggregate(errList) }  由前面 CNI 库接口可知, plugin.cniConfig.DelNetwork() 实际上调用的是 cni plugin 去删除容器网络配置. bridge 插件负责调用 host-local IPAM plugin 释放该容器的 ip, 然后删除容器的网络接口等.\nCNI plugin 实现 CNI plugin 是一种更通用的实现，允许用户自定义插件。cniNetworkPlugin 是 NetworkPlugin interface 的一个实现，具体的代码如下。\n// pkg/kubelet/dockershim/network/cni/cni.go type cniNetworkPlugin struct { network.NoopNetworkPlugin loNetwork *cniNetwork sync.RWMutex defaultNetwork *cniNetwork host network.Host execer utilexec.Interface nsenterPath string confDir string binDirs []string podCidr string }  通过 cniNetwork 类型的 loNetwork 和 defaultNetwork 来调用 CNI 插件，cniNetwork 定义如下。\n// pkg/kubelet/dockershim/network/cni/cni.go type cniNetwork struct { name string NetworkConfig *libcni.NetworkConfigList CNIConfig libcni.CNI }  CNI Init 在 NewDockerService() 函数中调用 ProbeNetworkPlugins() 根据配置的 CNI 插件的路径生成 network.NetworkPlugin interface 的实现 cniNetworkPlugin。\n// pkg/kubelet/dockershim/network/cni/cni.go func ProbeNetworkPlugins(confDir string, binDirs []string) []network.NetworkPlugin { old := binDirs binDirs = make([]string, 0, len(binDirs)) for _, dir := range old { if dir != \u0026quot;\u0026quot; { binDirs = append(binDirs, dir) } } plugin := \u0026amp;cniNetworkPlugin{ defaultNetwork: nil, loNetwork: getLoNetwork(binDirs), execer: utilexec.New(), confDir: confDir, binDirs: binDirs, } // sync NetworkConfig in best effort during probing. plugin.syncNetworkConfig() return []network.NetworkPlugin{plugin} }  主要是对 loNetwork 和 defaultNetwork 变量的配置。\n// pkg/kubelet/dockershim/network/cni/cni_others.go func getLoNetwork(binDirs []string) *cniNetwork { loConfig, err := libcni.ConfListFromBytes([]byte(`{ \u0026quot;cniVersion\u0026quot;: \u0026quot;0.2.0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;cni-loopback\u0026quot;, \u0026quot;plugins\u0026quot;:[{ \u0026quot;type\u0026quot;: \u0026quot;loopback\u0026quot; }] }`)) loNetwork := \u0026amp;cniNetwork{ name: \u0026quot;lo\u0026quot;, NetworkConfig: loConfig, CNIConfig: \u0026amp;libcni.CNIConfig{Path: binDirs}, } return loNetwork }  Init() 做的就是配置 defaultNetwork\nfunc getDefaultCNINetwork(confDir string, binDirs []string) (*cniNetwork, error) { // 从配置文件获取 confList network := \u0026amp;cniNetwork{ name: confList.Name, NetworkConfig: confList, CNIConfig: \u0026amp;libcni.CNIConfig{Path: binDirs}, } return network, nil } }  CNI Event 收到 NET_PLUGIN_EVENT_POD_CIDR_CHANGE 事件时只是更新了 podCidr 的值\nfunc (plugin *cniNetworkPlugin) Event(name string, details map[string]interface{}) { podCIDR, ok := details[network.NET_PLUGIN_EVENT_POD_CIDR_CHANGE_DETAIL_CIDR].(string) plugin.podCidr = podCIDR }  可见 k8s cni 网络方式并没有规定使用 podCidr 来配置 node 上容器的网络 ip 段, 而把 pod 的 ip 分配完全交给 IPAM, 这样使得 IPAM 更加灵活, 多样化和定制化\nCNI SetUpPod // pkg/kubelet/dockershim/network/cni/cni.go func (plugin *cniNetworkPlugin) SetUpPod(namespace string, name string, id kubecontainer.ContainerID, annotations, options map[string]string) error { ... // Windows doesn't have loNetwork. It comes only with Linux if plugin.loNetwork != nil { if _, err = plugin.addToNetwork(plugin.loNetwork, name, namespace, id, netnsPath, annotations, options); err != nil { return err } } _, err = plugin.addToNetwork(plugin.getDefaultNetwork(), name, namespace, id, netnsPath, annotations, options) return err }  func (plugin *cniNetworkPlugin) addToNetwork(network *cniNetwork, podName string, podNamespace string, podSandboxID kubecontainer.ContainerID, podNetnsPath string, annotations, options map[string]string) (cnitypes.Result, error) { netConf, cniNet := network.NetworkConfig, network.CNIConfig res, err := cniNet.AddNetworkList(netConf, rt) }  由前面 CNI 库接口可知, cninet.AddNetwork() 实际上调用的是底层用户配置的 cni plugin 去实现容器网络配置.\nCNI TearDownPod 与前面的流程类似，最终调用底层的 cni plugin 删掉配置。代码略。\nkubernets network plugin 安装 kubelet 有一个默认的 plugin，然后为整个集群提供一个默认的网络。当启动时探测到插件后就可以在 pod 整个生命周期里调换用。有两个启动参数：\n cni-bin-dir：启动时加载这个参数指定的路径里的 plugin network-plugin：插件的名字，要能匹配上面路径中的插件，比如 CNI 配置为 \u0026ldquo;cni\u0026rdquo;  network plugin 需求 plguin 除了要提供 NetworkPlugin interface 添加和删除 pod 的网络之外，还要实现对 kube-proxy 的支持。proxy 依赖 iptables，plugin 需要确保容器流量可以使用 iptables。比如 plugin 将容器添加到 Linux bridge，就需要通过 sysctl 设置 net/bridge/bridge-nf-call-iptables = 1 来确保 iptables proxy 功能正常。\n如果没有指定 network plugin，就使用 noop plugin 设置 net/bridge/bridge-nf-call-iptables=1。\nCNI 在 kubelet 命令行中 --network-plugin=cni 指定了采用 CNI 插件，kubelet 从 --cni-conf-dir（默认 /etc/cni/net.d）读取配置文件来配置 pod 网络。CNI 配置文件要遵循 CNI specification，配置文件中指定的 CNI 插件的执行程序要放在 \u0026ndash;cni-bin-dir (default /opt/cni/bin)。\n如果目录下有多个 CNI 配置文件，按字典序采用第一个配置文件。\n除了配置文件中指定的 CNI 插件外，K8S 还需要标准的 lo 插件。\nhostPort 支持 CNI plugin 支持 hostPort，可以采用官方的 portmap，也可以自己实现。\n需要在 cni-conf-dir 中开启 portMappings capability 来支持 hostPort。\n{ \u0026quot;name\u0026quot;: \u0026quot;k8s-pod-network\u0026quot;, \u0026quot;cniVersion\u0026quot;: \u0026quot;0.3.0\u0026quot;, \u0026quot;plugins\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;calico\u0026quot;, \u0026quot;log_level\u0026quot;: \u0026quot;info\u0026quot;, \u0026quot;datastore_type\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;nodename\u0026quot;: \u0026quot;127.0.0.1\u0026quot;, \u0026quot;ipam\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;host-local\u0026quot;, \u0026quot;subnet\u0026quot;: \u0026quot;usePodCidr\u0026quot; }, \u0026quot;policy\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;k8s\u0026quot; }, \u0026quot;kubernetes\u0026quot;: { \u0026quot;kubeconfig\u0026quot;: \u0026quot;/etc/cni/net.d/calico-kubeconfig\u0026quot; } }, { \u0026quot;type\u0026quot;: \u0026quot;portmap\u0026quot;, \u0026quot;capabilities\u0026quot;: {\u0026quot;portMappings\u0026quot;: true} } ] }  traffic shaping 支持 CNI plugin 支持 pod ingress 和 egress 整形，可以使用官方提供的 bandwidth 或自定义的插件。同样需要在配置文件（默认在 /etc/cni/net.d）中配置。\n{ \u0026quot;name\u0026quot;: \u0026quot;k8s-pod-network\u0026quot;, \u0026quot;cniVersion\u0026quot;: \u0026quot;0.3.0\u0026quot;, \u0026quot;plugins\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;calico\u0026quot;, \u0026quot;log_level\u0026quot;: \u0026quot;info\u0026quot;, \u0026quot;datastore_type\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;nodename\u0026quot;: \u0026quot;127.0.0.1\u0026quot;, \u0026quot;ipam\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;host-local\u0026quot;, \u0026quot;subnet\u0026quot;: \u0026quot;usePodCidr\u0026quot; }, \u0026quot;policy\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;k8s\u0026quot; }, \u0026quot;kubernetes\u0026quot;: { \u0026quot;kubeconfig\u0026quot;: \u0026quot;/etc/cni/net.d/calico-kubeconfig\u0026quot; } }, { \u0026quot;type\u0026quot;: \u0026quot;bandwidth\u0026quot;, \u0026quot;capabilities\u0026quot;: {\u0026quot;bandwidth\u0026quot;: true} } ] }  现在你可以向 pod 中添加 kubernetes.io/ingress-bandwidth 和 kubernetes.io/egress-bandwidth 的 annotations，例如：\napiVersion: v1 kind: Pod metadata: annotations: kubernetes.io/ingress-bandwidth: 1M kubernetes.io/egress-bandwidth: 1M ...  CNI 演进 Multi CNI and Containers with Multi Network Interfaces on Kubernetes with CNI-Genie\nmultus-cni\n参考 k8s network\nNetwork Plugins\n问题  dockerService 用途是什么？\n 在一个集群里不同的 Node 上可以配置不同的 plugin 吗？\n  ","date":1559001600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559001600,"objectID":"631156bfce891192f2b6a9b8eef15b66","permalink":"/post/cloud/k8s/201905-k8s-network-arch/","publishdate":"2019-05-28T00:00:00Z","relpermalink":"/post/cloud/k8s/201905-k8s-network-arch/","section":"post","summary":"Kubernetes本身不提供容器网络，但具有可扩展的网络框架","tags":["Kubernetes","CNI"],"title":"Kubernetes 网络框架","type":"post"},{"authors":null,"categories":null,"content":" 人们对云计算提出了更高的要求，为大量项目构建运营环境的效率问题，缩短新业务的上线部署时间，大规模的计算机房快速迁移需求；提高服务器资源的利用率，同时确保相同的性能和可用性，又有降低成本的需求。相较于传统的虚拟化解决方案，容器云可以较好的实现上述目标。\n容器云的核心功能：\n 快速扩容 智能调度和编排 弹性伸缩  快速扩容  扩容速度：  VM - 扩容20个实例需要4分钟（扩容完成后需要再执行服务发布） docker - 扩容20个实例仅需30s，秒级别扩容（扩容完成即服务启动）  扩容速度提高 8~12倍 节约了用户手动操作申请/发布的成本  智能调度 调度系统是云集群的中央处理器，要解决的核心问题是为容器选择合适的宿主机。有如下的指标：\n 资源利用率，提高整体物理集群的资源利用率 业务可用性保障：业务容器容灾能力、保障运行业务的稳定高可用 并发调度能力：调度系统请求处理能力的体现  资源最大化利用  按CPU/Mem/IO等类型对服务进行调度，最大化资源利用 业务按需使用资源，提升资源利用率  混布与独占  在线服务与离线任务混布 重要业务资源池独占  容器编排  有调用关系的多个服务实例，优先部署到相同/相近的宿主机上 同服务实例打散，分布到不同宿主机上，提高服务可用性 高负载容器，自动迁移到低负载宿主机 自动化容器实例健康检查，异常实例自动迁移  调度计算 通过先过滤filter之后排序打分rank的方式找到最优的部署位置。\n在一批宿主机中先过滤掉超售的，然后考虑到打散、混部、减少碎片和负载均衡之后找到合适的宿主机\n调度SLA（Service Level Agreement）  高可用：99.999 调度成功率：99.99 并发调度：单机并发处理200+，并发调度机器1000+ 低延迟：TCP90 63ms HA：分布式调度，横向扩展，多IDC部署容灾 监控报警：Falcon  弹性收缩 周期收缩\n根据设定时间段伸缩（适合秒杀/直播等业务）\n监控伸缩\n 根据QPS/CPU等触发条件伸缩 线性可扩展的无状态服务  服务画像\n针对数据建模，描绘服务特征：\n 服务画像：仿照用户画像，根据服务数据，抽取服务Tag  QPS特征（高峰时段、QPS max/min等） 资源利用率 CPU密集型 or IO密集型   基于历史数据建模的服务画像可以做服务特征值的预测，比如QPS的预测：\n QPS预测：RNN LSTM 即使监控数据源完全不可用，无数据，也能较准确的扩缩容  异常处理  监控数据异常，怎么办？会不会因监控值偏低而一直缩容？ 监控数据有延迟，怎么办？ 监控数据没了，怎么办？  通过数据无关的缩容退避+熔断机制来保证异常情况下的正常运行：\n 针对监控数据偏低（异常）而触发持续缩容 数据无关，不关心数据是否异常 如果连续缩容，那么缩容速度会越来越慢 —\u0026gt; 退避 如果连续缩容次数超过阈值，一段时间内禁止缩容 —\u0026gt; 熔断  ","date":1557158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557158400,"objectID":"318d50b7d7643d72671620a688ce7837","permalink":"/post/cloud/container/201905-why-container-cloud/why-container/","publishdate":"2019-05-07T00:00:00+08:00","relpermalink":"/post/cloud/container/201905-why-container-cloud/why-container/","section":"post","summary":"容器云解决大量项目构建运营环境的效率问题","tags":["Docker","Cloud"],"title":"Why 容器云","type":"post"},{"authors":null,"categories":null,"content":" 在云计算之前，应用程序直接运行在数据中心服务器的操作系统上，网络流量通过与服务器相连的硬件交换机转发。随着云计算虚拟化技术的发展，在服务器中可以运行大量的虚拟机或者容器，应用程序运行在虚拟环境中对虚拟网络提出了新的要求，需要一张连通性、隔离性、安全性更好，以及灵活的虚拟网络。需要在服务器中运行虚拟交换机，连接虚拟机和宿主机网络，有几种虚拟交换机方案：Linux bridge, VMware virtual switch, Cisco Nexus 1000V 和 Open vSwitch。\nOVS（Open vSwitch） 是运行在虚拟化平台的虚拟交换机，OVS 是由 Nicira Networks 公司用 C 语言开发的高质量、多层虚拟交换机，为连接虚拟主机构建复杂网络应用提供支持，支持多种数据面和管理面协议，如：VxLAN、sFlow、CLI、LACP 等。\nOVS 支持 Openflow 协议，可以使用任何支持 Openflow 协议的控制器集成，比如可以与 OpenStack Neutron 的网络插件对接；在没有 Openflow 控制器的情况下 OVS 可以作为传统的二层交换机通过 MAC 学习来完成二层数据转发。Linux Bridge 也可以通过 MAC 地址学习实现二层转发，但是功能比较简单，而 OVS 支持更多的高级特性，比如 Openflow、QoS、ACL 及多种隧道封装，这也是 OVS 更加适用于 SDN 的原因。\nOVS 主要支持如下的 特性：\n VM 通信的网络可视化，通过 NetFlow, sFlow\u0026reg;, IPFIX, SPAN, RSPAN 和 GRE 隧道镜像的方式 LACP，802.1Q VLAN 组播 BFD 双向转发检测和 802.1ag 链路监控 STP 和 RSTP 精细的 QoS 控制 基于端口的流量策略 网卡绑定，主备切换和四层负载均衡 支持 Openflow 支持 IPv6 多种隧道协议支持，GRE, VXLAN, STT, and Geneve, with IPsec support 支持用户控件和内核转发引擎的选项配置 flow-caching 引擎的多流表转发 转发层的抽象更易于移植到新的软硬件平台。  OVS 架构 先看下 OVS 整体架构，内核空间运行着转发功能的 datapath，用户空间运行着 ovs-vswitchd，第一条报文要上送 ovs-vswitchd 转发，ovsdb-server 保存着数据信息。OVS 控制器可以通过 OVSDB 或者 Openflow 协议与 OVS 通信。\nOVS 包括以下的核心模块：\n ovs-vswitchd 主要模块，实现switch的daemon，包括一个支持流交换的Linux内核模块； ovsdb-server 轻量级数据库服务器，提供ovs-vswitchd获取配置信息； openvswitch.ko Linux 内核模块中的转发实现  提供了一些工具来方便管理和配置：\n ovs-dpctl：用来配置交换机内核模块 ovs-vsctl：用于查询和更新ovs-vswitchd的配置信息，操作对象为ovsdb-server。更新配置时，命令会等待配置在ovs-vswitchd生效后才返回。 ovs-appctl：向ovs-vswitchd发送命令。 ovs-ofctl: 用来配置交换机的flow相关。  组件关系  ovs-vswitchd 和 ovsdb-server 运行在用户空间，ovs-vswitchd 通过 unix domain socket 向 ovsdb-server 获取和更新配置信息，ovsdb 通常是一个文件 /etc/openvswitch/conf.db。 openvswitch.ko 运行在内核空间实现报文的转发，与 ovs-vswitchd 通过 netlink 交互。 数据交换是通过内核空间的 datapath 转发的，datapath 通过 flow table 来指导转发，当收到一个报文时，按照匹配到的 flow 的 action 来转发。  转发路径 当收到新的一条报文时，在 datapath 中的 flow table 中找不到匹配项，发送给用户空间的 ovs-vswitchd，后者查询数据库判断转发 action，然后把匹配的actions返回给datapath并设置一条datapath flows到 datapath 中，这样后续进入的同类型的数据包因为缓存匹配会被 datapath 直接处理，不用再次进入用户空间。\ndatapath 专注于数据交换，它不需要知道 OpenFlow 的存在。与 OpenFlow 打交道的是 ovs-vswitchd，ovs-vswitchd 存储所有 Flow 规则供 datapath 查询或缓存。\n转发路径参考下图\nOVS 常用操作 以下操作都需要root权限运行，在所有命令中br0表示网桥名称，eth0为网卡名称。\novs-vsctl 添加网桥：\n#ovs-vsctl add-br br0  列出open vswitch中的所有网桥：\n#ovs-vsctl list-br  判断网桥是否存在\n#ovs-vsctl br-exists br0  将物理网卡挂接到网桥：\n#ovs-vsctl add-port br0 eth0  列出网桥中的所有端口：\n#ovs-vsctl list-ports br0  列出所有挂接到网卡的网桥：\n#ovs-vsctl port-to-br eth0  查看open vswitch的网络状态：\n#ovs-vsctl show  删除网桥上已经挂接的网口：\n#vs-vsctl del-port br0 eth0  删除网桥：\n#ovs-vsctl del-br br0  OVS DPDK DPDK 介绍 DPDK是X86平台报文快速处理的库和驱动的集合，不是网络协议栈，不提供二层，三层转发功能，不具备防火墙ACL功能，但通过DPDK可以轻松的开发出上述功能。\nDPDK的优势在于，可以将用户态的数据，不经过内核直接转发到网卡，实现加速目的。主要架构如图所示：\nDPDK关键技术点：\n 使用大页缓存支持来提高内存访问效率。 利用UIO支持，提供应用空间下驱动程序的支持，也就是说网卡驱动是运行在用户空间的，减小了报文在用户空间和应用空间的多次拷贝。 利用 Linux 亲和性支持，把控制面线程及各个数据面线程绑定到不同的CPU核，节省了线程在各个CPU核来回调度。 Lockless， 提供无锁环形缓存管理，加快内存访问效率。 收发包批处理 ，多个收发包集中到一个cache line，在内存池中实现，无需反复申请和释放。 PMD驱动，用户态轮询驱动，可以减小上下文切换开销，方便实现虚拟机和主机零拷贝。  OVS 流表转发 OVS 工作在内核态，与 guest virtio 的数据传输需要多次内核态和用户态的数据切换, 带来性能瓶颈。\nOVS当前版本采用Megaflow Cache+Microflow Cache的流Cache组织形式，仍保留了Microflow Cache作为一级Cache，即报文进入后首先查这一级Cache，获取到一个索引值，指向的是最近一次查Megaflow Cache表项。那么报文的首次查表就不需要进行线性地链式搜索，可直接对应到其中一张Megaflow的元组表。这样提高了查表效率和转发性能。\novs-dpdk 结合了DPDK和vhost-user技术的优势，vhost-user是一个用户态的vhost-backend程序，从虚拟机到host上实现了数据的zero copy。DPDK 高性能(user space) 网卡驱动、大页内存、无锁化结构设计，可以实现万兆网卡线速的性能。ovs-dpdk使vm到vm和nic到vm的整个数据传输都工作在用户态，极大的提升了ovs的性能。\n转发性能 ovs-dpdk的小包处理能力是原ovs的约3~4倍，网络处理性能有了显著提高。高带宽用户vm，部署在ovs-dpdk上，能降低公网IP网络访问延迟，提高网络并发量。\nOVN OVN (Open Virtual Network) 是 OVS 的控制平面，为 OVS 提供的原生虚拟化网络方案，旨在解决传统SDN架构的性能问题。其主要功能包括：\n L2/L3虚拟网络以及逻辑交换机(logical switch) L2/L3/L4 ACL IPv4/IPv6分布式L3路由 ARP and IPv6 Neighbor Discovery suppression for known IP-MAC bindings Native support for NAT and load balancing using OVS connection tracking Native fully distributed support for DHCP Works with any OVS datapath (such as the default Linux kernel datapath, DPDK, or Hyper-V) that supports all required features (namely Geneve tunnels and OVS connection tracking) Supports L3 gateways from logical to physical networks Supports software-based L2 gateways Supports TOR (Top of Rack) based L2 gateways that implement the hardware_vtep schema Can provide networking for both VMs and containers running inside of those VMs, without a second layer of overlay networking  OVN架构 OVN由以下组件构成：\n northbound database：存储逻辑交换机、路由器、ACL、端口等的信息，目前基于ovsdb-server，未来可能会支持etcd v3 ovn-northd: 集中式控制器，负责把northbound database数据分发到各个ovn-controller ovn-controller: 运行在每台机器上的本地SDN控制器，将配置转换为流表 southbound database：基于ovsdb-server（未来可能会支持etcd v3），包含三类数据  物理网络数据，比如VM的IP地址和隧道封装格式 逻辑网络数据，比如报文转发方式 物理网络和逻辑网络的绑定关系   OVN Kubernetes ovn-kubernetes 提供了一个 OVN CNI 网络插件，支持underlay和overlay两种模式。\n underlay：容器运行在虚拟机中，而ovs则运行在虚拟机所在的物理机上，OVN将容器网络和虚拟机网络连接在一起 overlay：OVN通过logical overlay network连接所有节点的容器，此时ovs可以直接运行在物理机或虚拟机上  CNI插件原理 ADD操作\n 从ovn annotation获取ip/mac/gateway 在容器netns中配置接口和路由 添加ovs端口\novs-vsctl add-port br-int veth_outside \u0026ndash;set interface veth_outside external_ids:attached_mac=mac_address external_ids:iface-id=namespace_pod external_ids:ip_address=ip_address\n  DEL操作\novs-vsctl del-port br-int port  ","date":1555113600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555113600,"objectID":"3f273c624de016934f6c2831be90c132","permalink":"/post/cloud/network/201904-ovs/","publishdate":"2019-04-13T00:00:00Z","relpermalink":"/post/cloud/network/201904-ovs/","section":"post","summary":"介绍 OVS 在云计算中的应用","tags":["Network"],"title":"OVS 虚拟交换机","type":"post"},{"authors":null,"categories":null,"content":" 组件 从 Docker 1.11 之后，Docker Daemon 被分成了多个模块以适应 OCI 标准。容器是由多个组件共同管理协同后才运行起来的，主要的组件有：\n Docker Docker Daemon Containerd RunC  图，容器组件图\n其中，containerd 独立负责容器运行时和生命周期（如创建、启动、停止、中止、信号处理、删除等），其他一些如镜像构建、卷管理、日志等由 Docker Daemon 的其他模块处理。\nDocker 作为 Client 接受用户的指令并将指令发送给 Docker daemon 处理。是 Docker API 的最上层封装，直接面向操作用户。\nDocker Daemon Docker 容器管理的守护进程，负责与 Docker client 交互，实现对 Docker 镜像和容器的管理。\n图，Docker daemon 的作用\n从Docker 1.11开始，通过如下命令启动\ndockerd  在 Hulk 中的进程如下\n/usr/bin/dockerd-current --exec-opt native.cgroupdriver=systemd --userland-proxy-path=/usr/libexec/docker/docker-proxy-current --init-path=/usr/libexec/docker/docker-init-current --seccomp-profile=/etc/docker/seccomp.json --selinux-enabled --log-driver=json-file --log-opt max-size=100m --signature-verification=false --live-restore  Containerd containerd是容器技术标准化之后的产物，为了能够兼容OCI 标准，将容器运行时及其管理功能从 Docker Daemon 剥离。理论上，即使不运行 dockerd，也能够直接通过 containerd 来管理容器。（当然，containerd 本身也只是一个守护进程，容器的实际运行时由后面介绍的 runC 控制。）\nContainerd 主要职责是镜像管理（镜像、元信息等）、容器执行（调用最终运行时（RunC）组件执行）。\n图，Containerd 架构\n图，Containerd 整体架构\ncontainerd 向上为 Docker Daemon 提供了 gRPC 接口，使得 Docker Daemon 屏蔽下面的结构变化，确保原有接口向下兼容。向下通过 containerd-shim 结合 runC，使得引擎可以独立升级，避免之前 Docker Daemon 升级会导致所有容器不可用的问题。\nDocker、containerd 和 containerd-shim 之间的关系，可以通过启动一个 Docker 容器，观察进程之间的关联。首先启动一个容器，\n# docker run -d busybox sleep 1000 94325941834e353c137113d78158a114df3332e8759fba5176bfbd56049f7221  然后通过pstree命令查看进程之间的父子关系（其中 197979 是dockerd的 PID）：\n# pstree -l -a -A 197979 dockerd-current --exec-opt native.cgroupdriver=systemd --userland-proxy-path=/usr/libexec/docker/docker-proxy-current --init-path=/usr/libexec/docker/docker-init-current --seccomp-profile=/etc/docker/seccomp.json --selinux-enabled --log-driver=json-file --log-opt max-size=100m --signature-verification=false --live-restore |-docker-containe -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-runc --runtime-args --systemd-cgroup=true | |-docker-containe 94325941834e353c137113d78158a114df3332e8759fba5176bfbd56049f7221 /var/run/docker/libcontainerd/94325941834e353c137113d78158a114df3332e8759fba5176bfbd56049f7221 docker-runc | | |-sleep 1000 | | `-9*[{docker-containe}]  由于 pstree 截断了进程名字，实际的进程名字是 docker-containerd-shim。Docker daemon 启动之后，dockerd 和 docker-containerd 进程一直存在。当启动容器之后，docker-containerd 进程（也是这里介绍的 containerd 组件）会创建 docker-containerd-shim 进程，其中的参数 94325941834e353c137113d78158a114df3332e8759fba5176bfbd56049f7221 就是要启动容器的 id。最后 docker-containerd-shim 子进程，已经是实际在容器中运行的进程（既 sleep 1000）。\nHulk 中的 containerd 和 containerd-shim 进程如下\n# ps aux | grep containerd root 134481 0.0 0.0 446412 5980 ? Sl 10:31 0:00 /usr/bin/docker-containerd-shim-current 94325941834e353c137113d78158a114df3332e8759fba5176bfbd56049f7221 /var/run/docker/libcontainerd/94325941834e353c137113d78158a114df3332e8759fba5176bfbd56049f7221 docker-runc root 197990 0.0 0.0 3571764 41852 ? Ssl May17 8:26 /usr/bin/docker-containerd-current -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-runc --runtime-args --systemd-cgroup=true  docker-containerd-shim 另一个参数，是一个和容器相关的目录 /var/run/docker/libcontainerd/94325941834e353c137113d78158a114df3332e8759fba5176bfbd56049f7221，里面的内容有：\n. ├── config.json ├── init-stderr ├── init-stdin └── init-stdout  其中包括了容器配置和标准输入、标准输出、标准错误三个管道文件。\ncontainerd 是dockerd和runc之间的一个中间交流组件。\nContainerd-shim Containerd-shim 是一个真实运行的容器的真实垫片载体，每启动一个容器都会起一个新的 containerd-shim 的一个进程，指定的三个参数：\n 容器id boundle目录（containerd的对应某个容器生成的目录，一般位于：/var/run/docker/libcontainerd/containerID）， 运行时二进制（默认为runc）来调用runc的api创建一个容器（比如创建容器：最后拼装的命令如下：runc create xxx）  RunC OCI 定义了容器运行时标准，runC 是 Docker 按照开放容器格式标准（OCF, Open Container Format）制定的一种具体实现。\nrunC 是从 Docker 的 libcontainer 中迁移而来的，实现了容器启停、资源隔离等功能。Docker 默认提供了 docker-runc 实现，事实上，通过 containerd 的封装，可以在 Docker Daemon 启动的时候指定 runc 的实现。\n我们可以通过启动 Docker Daemon 时增加\u0026ndash;add-runtime参数来选择其他的 runC 现。例如：\ndocker daemon --add-runtime \u0026quot;custom=/usr/local/bin/my-runc-replacement\u0026quot;  下面就让我们看下这几个模块如何工作。\n总结 图，各组件的调用关系\n可以查看 Docker update 流程分析 的代码调用流程。\n参考 Docker、Containerd、RunC\u0026hellip;：你应该知道的所有\n","date":1553385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553385600,"objectID":"3a73e25347e9c73457ff85efca8c4f74","permalink":"/post/cloud/container/201903-container-component/","publishdate":"2019-03-24T00:00:00Z","relpermalink":"/post/cloud/container/201903-container-component/","section":"post","summary":"Docker、Containerd、RunC...：你应该知道的所有","tags":["Docker"],"title":"Docker 容器组件","type":"post"},{"authors":null,"categories":null,"content":" API对象 在Kubernetes中API对象是以树形结构表示的，一个API对象在Etcd里完整资源路径，是由Group（API组）、Version（API版本）和Resource（API资源类型）三个部分组成的。\n如果现在要声明一个CronJob对象，那么YAML的开始部分会这么写，CronJob就是这个API对象的资源类型，Batch就是它们的组，v2alpha1就是它的版本\napiVersion: batch/v2alpha1 kind: CronJob ...  API解析 Kubernetes通过对API解析找到对应的对象，分为如下3步：\n 解析API的组 Kubernetes的对象分两种：  核心API对象（如Pod、Node），是不需要Group的，直接在 /api这个下面进行解析 非核心API对象，在 /apis 下先解析出Group，根据batch这个Group找到 /apis/batch，API Group的分类是以对象功能为依据的。  解析API对象的版本号 匹配API对象的资源类型  创建对象 在前面匹配到正确的版本之后，Kubernetes就知道要创建的是一个/apis/batch/v2alpha1下的CronJob对象，APIServer会继续创建这个Cronjob对象。创建过程如下图\n 当发起创建CronJob的POST请求之后，YAML的信息就被提交给了APIServer，APIServer的第一个功能就是过滤这个请求，并完成一些前置性的工作，比如授权、超时处理、审计等 请求进入MUX和Routes流程，MUX和Routes是APIServer完成URL和Handler绑定的场所。APIServer的Handler要做的事情，就是按照上面介绍的匹配过程，找到对应的CronJob类型定义。 根据这个CronJob类型定义，使用用户提交的YAML文件里的字段，创建一个CronJob对象。这个过程中，APIServer会把用户提交的YAML文件，转换成一个叫做Super Version的对象，它正是该API资源类型所有版本的字段全集，这样用户提交的不同版本的YAML文件，就都可以用这个SuperVersion对象来进行处理了。 APIServer会先后进行Admission（如Admission Controller 和 Initializer）和Validation操作（负责验证这个对象里的各个字段是否何方，被验证过得API对象都保存在APIServer里一个叫做Registry的数据结构中）。 APIServer会把验证过得API对象转换成用户最初提交的版本，进行系列化操作，并调用Etcd的API把它保存起来。\n  CRD API插件CRD（Custom Resource Definition） 允许用户在Kubernetes中添加一个跟Pod、Node类似的、新的API资源类型，即：自定义API资源\n举个栗子，添加一个叫Network的API资源类型，它的作用是一旦用户创建一个Network对象，那么Kubernetes就可以使用这个对象定义的网络参数，调用真实的网络插件，为用户创建一个真正的网络，这个过程分为两步\n 首先定义CRD  定义一个group为samplecrd.k8s.io， version为v1的API信息，指定了这个CR的资源类型叫做Network，定义的这个Network是属于一个Namespace的对象，类似于Pod。\napiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: networks.samplecrd.k8s.io spec: group: samplecrd.k8s.io version: v1 names: kind: Network plural: networks scope: Namespaced   对象实例化  实例化名为example-network的Network对象，API组是samplecrd.k8s.io，版本是v1。\napiVersion: samplecrd.k8s.io/v1 kind: Network metadata: name: example-network spec: cidr: \u0026quot;192.168.0.0/16\u0026quot; gateway: \u0026quot;192.168.0.1\u0026quot;  Network对象YAML文件，名叫example-network.yaml,API资源类型是Network，API组是samplecrd.k8s.io，版本是v1\nKubernetes的声明式API能够对API对象进行增量的更新操作：\n 定义好期望的API对象后，Kubernetes来尽力让对象的状态符合预期 允许多个YAML表达，以PATCH的方式对API对象进行修改，而不用关心原始YAML的内容  基于上面两种特性，Kubernetes可以实现基于API对象的更删改查，完成预期和定义的协调过程。\n因此Kubernetes项目编排能力的核心是声明式API。\nKubernetes编程范式即：如何使用控制器模式，同Kubernetes里的API对象的“增、删、改、查”进行协作，进而完成用户业务逻辑的编写过程。\nkubectl apply kubectl apply是声明式的请求，下面一个Deployment的YAML的例子\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80  然后用kubectl apply创建这个Deployment\n$ kubectl apply -f nginx.yaml  修改一下nginx里定义的镜像\napiVersion: apps/v1 kind: Deployment ... spec: containers: - name: nginx image: nginx:1.7.9  执行kubectl apply命令，触发滚动更新\n$ kubectl apply -f nginx.yaml  后面一次的 kubectl apply命令执行了一个对原有API对象的PATCH操作，这是声明式命令同时可以进行多个写操作，具有Merge的能力；而像 kubectl replace命令是用新的YAML替换旧的，这种响应式命令每次只能处理一次写操作。\n声明式API的应用 Istio通过声明式API实现对应用容器所在POD注入Sidecar，然后通过iptables劫持POD的进站和出站流量到Sidecar，Istio通过对Sidecar下发策略来实现对应用流量的管控，继而实现微服务治理。\n在微服务治理中，对Envoy容器的部署和对Envoy代理的配置，应用容器都是不感知的。Istio是使用Kubernetes的Dynamic Admission Control来实现的。\n在APIServer收到API对象的提交请求后，在正常处理这些操作之前会做一些初始化的操作，比如为某些pod或容器加上一些label。这些初始化操作是通过Kubernetes的Admission Controller实现的，在APIServer对象创建之后调用，但这种方式的缺陷是需要将Admission Controller的代码编译到APIServer中，这不是很方便。Kubernetes 1.7引入了热插拔的Admission机制，它就是Dynamic Admission Control，也叫做Initializer。\n如下定义的应用的Pod，包含一个myapp-container的容器。\napiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026amp;\u0026amp; sleep 3600']  Istio要做的就是在这个Pod YAML被提交给Kubernetes之后，在它对应的API对象里自动加上Envoy容器的配置，使对象变成如下的样子：\napiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026amp;\u0026amp; sleep 3600'] - name: envoy image: lyft/envoy:845747b88f102c0fd262ab234308e9e22f693a1 command: [\u0026quot;/usr/local/bin/envoy\u0026quot;] ...  这个pod多了一个envoy的容器，Istio具体的做法是\n 定义Envoy容器的Initializer，并以ConfigMap的方式保存到Kubernetes中 Istio将编写好的Initializer作为一个Pod部署在Kubernetes中  Envoy容器的ConfigMap定义，\napiVersion: v1 kind: ConfigMap metadata: name: envoy-initializer data: config: | containers: - name: envoy image: lyft/envoy:845747db88f102c0fd262ab234308e9e22f693a1 command: [\u0026quot;/usr/local/bin/envoy\u0026quot;] args: - \u0026quot;--concurrency 4\u0026quot; - \u0026quot;--config-path /etc/envoy/envoy.json\u0026quot; - \u0026quot;--mode serve\u0026quot; ports: - containerPort: 80 protocol: TCP resources: limits: cpu: \u0026quot;1000m\u0026quot; memory: \u0026quot;512Mi\u0026quot; requests: cpu: \u0026quot;100m\u0026quot; memory: \u0026quot;64Mi\u0026quot; volumeMounts: - name: envoy-conf mountPath: /etc/envoy volumes: - name: envoy-conf configMap: name: envoy  这个ConfigMap的data部分，正是一个Pod对象的一部分定义，其中可以看到Envoy容器对应的Container字段，以及一个用来声明Envoy配置文件的volumes字段。Initializer要做的就是把这部分Envoy相关的字段，自动添加到用户提交的Pod的API对象里。但是用户提交的Pod里本来就有containers和volumes字段，所以Kubernetes在处理这样的更新请求时，就必须使用类似于git merge这样的操作，才能将这两部分内容合并在一起。即Initializer更新用户的Pod对象时，必须使用PATCH API来完成。\nEnvoy Initializer的pod定义\napiVersion: v1 kind: Pod metadata: labels: app: envoy-initializer name: envoy-initializer spec: containers: - name: envoy-initializer image: envoy-initializer:0.0.1 imagePullPolicy: Always ```\t`envoy-initializer:0.0.1` 镜像是一个自定义控制器（Custom Controller）。Kubernetes的控制器实际上是一个死循环：它不断地获取实际状态，然后与期望状态作对比，并以此为依据决定下一步的操作。 对Initializer控制器，不断获取的实际状态，就是用户新创建的Pod，它期望的状态就是这个Pod里被添加了Envoy容器的定义。它的控制逻辑如下： ```go for { // 获取新创建的 Pod pod := client.GetLatestPod() // Diff 一下，检查是否已经初始化过 if !isInitialized(pod) { // 没有？那就来初始化一下 //istio要往这个Pod里合并的字段，就是ConfigMap里data字段的值 doSomething(pod) } } func doSomething(pod) { //调用APIServer拿到ConfigMap cm := client.Get(ConfigMap, \u0026quot;envoy-initializer\u0026quot;) //把ConfigMap里存在的containers和volumes字段，直接添加进一个空的Pod对象 newPod := Pod{} newPod.Spec.Containers = cm.Containers newPod.Spec.Volumes = cm.Volumes // Kubernetes的API库，提供一个方法使我们可以直接使用新旧两个Pod对象，生成 patch 数据 patchBytes := strategicpatch.CreateTwoWayMergePatch(pod, newPod) // 发起 PATCH 请求，修改这个 pod 对象 client.Patch(pod.Name, patchBytes) }  Envoy机制正是利用了Kubernetes能够对API对象做增量更新，这是Kubernetes声明式API的独特之处。\n参考 Dynamic Admission Control\n【Kubernetes】深入解析声明式API\n","date":1545753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545753600,"objectID":"a932d713bc6cbd242ccbb247be2576d4","permalink":"/post/cloud/k8s/201904-k8s-declarative-api/","publishdate":"2018-12-26T00:00:00+08:00","relpermalink":"/post/cloud/k8s/201904-k8s-declarative-api/","section":"post","summary":"声明式API是Kubernetes成为容器编排事实标准的利器","tags":["Kubernetes"],"title":"Kubernetes声明式API","type":"post"},{"authors":null,"categories":null,"content":" calico calico是一个比较有趣的虚拟网络解决方案，适用于容器的解决方案，完全利用路由规则实现动态组网，通过BGP协议通告路由。\ncalico的好处是endpoints组成的网络是单纯的三层网络，报文的流向完全通过路由规则控制，没有overlay等额外开销。\ncalico的endpoint可以漂移，并且实现了acl。\ncalico的缺点是路由的数目与容器数目相同，非常容易超过路由器、三层交换、甚至node的处理能力，从而限制了整个网络的扩张。\ncalico的每个node上会设置大量（海量)的iptables规则、路由，运维、排障难度大。\ncalico的原理决定了它不可能支持VPC，容器只能从calico设置的网段中获取ip。\ncalico目前的实现没有流量控制的功能，会出现少数容器抢占node多数带宽的情况。\ncalico的网络规模受到BGP网络规模的限制。\n名词解释  endpoint: 接入到calico网络中的网卡称为endpoint AS: 网络自治系统，通过BGP协议与其它AS网络交换路由信息 ibgp: AS内部的BGP Speaker，与同一个AS内部的ibgp、ebgp交换路由信息。 ebgp: AS边界的BGP Speaker，与同一个AS内部的ibgp、其它AS的ebgp交换路由信息。 workloadEndpoint: 虚拟机、容器使用的endpoint hostEndpoints: 物理机(node)的地址  calico在ip fabric中的部署方式 如果底层的网络是ip fabric的方式，三层网络是可靠的，只需要部署一套calico。\n剩下的关键点就是怎样设计BGP网络，calico over ip fabrics中给出两种设计方式:\n AS per rack: 每个rack(机架)组成一个AS，每个rack的TOR交换机与核心交换机组成一个AS AS per server: 每个node做为一个AS，TOR交换机组成一个transit AS  这两种方式采用的是Use of BGP for routing in large-scale data centers中的建议。\nAS per rack  一个机架作为一个AS，分配一个AS号，node是ibgp，TOR交换机是ebgp node只与TOR交换机建立BGP连接，TOR交换机与机架上的所有node建立BGP连接 所有TOR交换机之间以node-to-node mesh方式建立BGP连接  TOR三层联通：\n每个机架上node的数目是有限的，BGP压力转移到了TOR交换机。当机架数很多，TOR交换机组成BGP mesh压力会过大。\nendpoints之间的通信过程:\nEndpointA发出报文 --\u0026gt; nodeA找到了下一跳地址nodeB --\u0026gt; 报文送到TOR交换机A --\u0026gt; 报文送到核心交换机 | v EndpointB收到了报文 \u0026lt;-- nodeB收到了报文 \u0026lt;-- TOR交换机B收到了报文 \u0026lt;-- 核心交换机将报文送达TOR交换机B  优化：“Downward Default model”减少需要记录的路由 Downward Default Model在上面的几种组网方式的基础上，优化了路由的管理。\n在上面的方式中，每个node、每个TOR交换机、每个核心交换机都需要记录全网路由。\n“Downward Default model”模式中:\n 每个node向上(TOR)通告所有路由信息，而TOR向下(node)只通告一条默认路由 每个TOR向上(核心交换机)通告所有路由，核心交换机向下(TOR)只通告一条默认路由 node只知晓本地的路由 TOR只知道接入到自己的所有node上的路由 核心交换机知晓所有的路由  这种模式减少了TOR交换机和node上的路由数量，但缺点是，发送到无效IP的流量必须到达核心交换机以后，才能被确定为无效。\nendpoints之间的通信过程:\nEndpointA发出报文 --\u0026gt; nodeA默认路由到TOR交换机A --\u0026gt; TOR交换机A默认路由到核心交换机 --+ | v EndpointB收到了报文 \u0026lt;-- nodeB收到了报文 \u0026lt;-- TOR交换机B收到了报文 \u0026lt;-- 核心交换机找到了下一跳地址nodeB  calico验证 etcd 在节点上执行\ndocker run -d -p 2379:2379 -p 2380:2380 --name calico_etcd elcolio/etcd \\ -name etcd1 \\ -advertise-client-urls http://10.48.35.14:2379 \\ -listen-client-urls http://0.0.0.0:2379 \\ -initial-advertise-peer-urls http://10.48.35.14:2380 \\ -listen-peer-urls http://0.0.0.0:2380 \\ -initial-cluster-token etcd-cluster \\ -initial-cluster \u0026quot;etcd1=http://10.48.35.14:2380\u0026quot; \\ -initial-cluster-state new  查看集群状态\n# curl 10.48.35.14:2379/version etcd 2.0.10  设置多个节点共用相同的store\nvim /etc/docker/daemon.json { \u0026quot;cluster-store\u0026quot;:\u0026quot;etcd://10.48.35.14:2379\u0026quot;, }  重启docker服务\nsystemctl restart docker sudo service docker restart  etcd保存的calico信息 $ sudo etcdctl ls /calico /calico/v1 /calico/bgp /calico/ipam $ curl http://127.0.0.1:2379/v2/keys/ {\u0026quot;action\u0026quot;:\u0026quot;get\u0026quot;,\u0026quot;node\u0026quot;:{\u0026quot;dir\u0026quot;:true,\u0026quot;nodes\u0026quot;:[{\u0026quot;key\u0026quot;:\u0026quot;/docker\u0026quot;,\u0026quot;dir\u0026quot;:true,\u0026quot;modifiedIndex\u0026quot;:3,\u0026quot;createdIndex\u0026quot;:3},{\u0026quot;key\u0026quot;:\u0026quot;/calico\u0026quot;,\u0026quot;dir\u0026quot;:true,\u0026quot;modifiedIndex\u0026quot;:7,\u0026quot;createdIndex\u0026quot;:7}]}} $ curl 10.48.35.14:2379/v2/keys/calico/bgp/v1/host/k8s-/ip_addr_v4 | python -m json.tool % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 156 100 156 0 0 119k 0 --:--:-- --:--:-- --:--:-- 152k { \u0026quot;action\u0026quot;: \u0026quot;get\u0026quot;, \u0026quot;node\u0026quot;: { \u0026quot;createdIndex\u0026quot;: 74, \u0026quot;key\u0026quot;: \u0026quot;/calico/bgp/v1/host/k8s-/ip_addr_v4\u0026quot;, \u0026quot;modifiedIndex\u0026quot;: 74, \u0026quot;value\u0026quot;: \u0026quot;10.48.35.14\u0026quot; } } $ sudo etcdctl get /calico/bgp/v1/host/k8s-/ip_addr_v4 10.48.35.14  calico安装 直接下载预编译好的可执行文件就可以直接使用了\nsudo wget -O /bin/calicoctl https://github.com/projectcalico/calicoctl/releases/download/v1.6.1/calicoctl sudo chmod +x /bin/calicoctl curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.1.5/calicoctl chmod +x calicoctl mv calicoctl /user/local/bin/  下载 calico node 镜像\ndocker pull calico/node  启动calico服务: 在每台主机上均运行命令：\nsudo calicoctl node run --ip=10.48.35.14 --name node01 --node-image quay.io/calico/node:v2.6.0  其中 calicoctl 命令里的 name 和 ip，每台主机均可以根据自身情况来填写。 命令实际使用 calico/node 镜像启动了一个容器，执行输出内容如下：\nsudo calicoctl node run --name node01 --ip=10.48.32.5 Running command to load modules: modprobe -a xt_set ip6_tables Enabling IPv4 forwarding Enabling IPv6 forwarding Increasing conntrack limit Removing old calico-node container (if running). Running the following command to start calico-node: docker run --net=host --privileged --name=calico-node -d --restart=always -e IP=10.48.32.5 -e ETCD_ENDPOINTS=http://127.0.0.1:2379 -e NODENAME=node01 -e CALICO_NETWORKING_BACKEND=bird -e CALICO_LIBNETWORK_ENABLED=true -v /var/log/calico:/var/log/calico -v /var/run/calico:/var/run/calico -v /lib/modules:/lib/modules -v /run:/run -v /run/docker/plugins:/run/docker/plugins -v /var/run/docker.sock:/var/run/docker.sock quay.io/calico/node:latest Image may take a short time to download if it is not available locally. Container started, checking progress logs. 2019-03-03 05:40:10.424 [INFO][9] startup.go 256: Early log level set to info 2019-03-03 05:40:10.424 [INFO][9] startup.go 272: Using NODENAME environment for node name 2019-03-03 05:40:10.424 [INFO][9] startup.go 284: Determined node name: node01 2019-03-03 05:40:10.425 [INFO][9] startup.go 97: Skipping datastore connection test 2019-03-03 05:40:10.473 [INFO][9] startup.go 367: Building new node resource Name=\u0026quot;node01\u0026quot; 2019-03-03 05:40:10.473 [INFO][9] startup.go 382: Initialize BGP data 2019-03-03 05:40:10.473 [INFO][9] startup.go 476: Using IPv4 address from environment: IP=10.48.32.5 2019-03-03 05:40:10.476 [INFO][9] startup.go 509: IPv4 address 10.48.32.5 discovered on interface br0 2019-03-03 05:40:10.476 [INFO][9] startup.go 452: Node IPv4 changed, will check for conflicts 2019-03-03 05:40:10.477 [INFO][9] startup.go 647: No AS number configured on node resource, using global value 2019-03-03 05:40:10.477 [WARNING][9] startup_linux.go 47: Expected /var/lib/calico to be mounted into the container but it wasn't present. Node name may not be detected properly 2019-03-03 05:40:10.493 [INFO][9] startup.go 536: CALICO_IPV4POOL_NAT_OUTGOING is true (defaulted) through environment variable 2019-03-03 05:40:10.493 [INFO][9] startup.go 781: Ensure default IPv4 pool is created. IPIP mode: 2019-03-03 05:40:10.494 [INFO][9] startup.go 791: Created default IPv4 pool (192.168.0.0/16) with NAT outgoing true. IPIP mode: 2019-03-03 05:40:10.495 [INFO][9] startup.go 536: FELIX_IPV6SUPPORT is true (defaulted) through environment variable 2019-03-03 05:40:10.495 [INFO][9] startup_linux.go 79: IPv6 supported on this platform: true 2019-03-03 05:40:10.495 [INFO][9] startup.go 536: CALICO_IPV6POOL_NAT_OUTGOING is false (defaulted) through environment variable 2019-03-03 05:40:10.495 [INFO][9] startup.go 781: Ensure default IPv6 pool is created. IPIP mode: Never 2019-03-03 05:40:10.496 [INFO][9] startup.go 791: Created default IPv6 pool (fdb3:cdc5:a7b8::/48) with NAT outgoing false. IPIP mode: Never 2019-03-03 05:40:10.498 [INFO][9] startup.go 181: Using node name: node01 Starting libnetwork service Calico node started successfully  calico/node 中的进程\n$ sudo docker exec -it calico-node ps aux PID USER TIME COMMAND 1 root 0:00 /sbin/runsvdir -P /etc/service/enabled 198 root 0:00 runsv bird6 199 root 0:00 runsv confd 200 root 0:00 runsv bird 201 root 0:00 runsv felix 202 root 0:00 runsv libnetwork 203 root 0:00 svlogd /var/log/calico/confd 204 root 0:01 svlogd /var/log/calico/felix 205 root 0:00 svlogd -tt /var/log/calico/bird6 206 root 0:00 svlogd /var/log/calico/libnetwork 207 root 0:00 svlogd -tt /var/log/calico/bird 208 root 0:13 bird6 -R -s /var/run/calico/bird6.ctl -d -c /etc/calico/c 209 root 0:07 confd -confdir=/etc/calico/confd -interval=5 -watch --log 210 root 22:38 calico-felix 211 root 0:13 bird -R -s /var/run/calico/bird.ctl -d -c /etc/calico/con 212 root 0:05 libnetwork-plugin  创建calico网络 先创建两个网络：\n# sudo docker network create --driver calico --ipam-driver calico-ipam net1 # sudo docker network create --driver calico --ipam-driver calico-ipam net2  docker 创建网络的时候，会调用 calico 的网络驱动，由驱动完成具体的工作。注意这个网络是跨主机的，因此无论在哪台机器创建，在其他机器上都能看到：\n$ sudo docker network ls NETWORK ID NAME DRIVER SCOPE 0887c53db680 bridge bridge local 2e0533ba7fad host host local e8a0a439868f net1 calico global d8af5a88e634 net2 calico global 285e1438e0e7 none null local  创建calico网络的容器 然后分别在网络中运行容器：\nnode01:\nsudo docker run -tid --name=test1 --net=net1 busybox sudo docker run --net net2 --name test2 -tid busybox  node02:\nsudo docker run -tid --name=test3 --net=net2 busybox sudo docker run --net net1 --name test4 -tid busybox  calico网络连通性 组网的拓扑如图\ntest1和test4都在net1中，可以连通。通过如下配置test1可以ping通test4。\n$ sudo docker exec -it test1 ping 192.168.105.195 -c 3 PING 192.168.105.195 (192.168.105.195): 56 data bytes 64 bytes from 192.168.105.195: seq=0 ttl=62 time=0.229 ms 64 bytes from 192.168.105.195: seq=1 ttl=62 time=0.274 ms 64 bytes from 192.168.105.195: seq=2 ttl=62 time=0.256 ms --- 192.168.105.195 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max = 0.229/0.253/0.274 ms  同一个网络 docker 会保存各自的名字和 IP 的对应关系，而不同网络的容器无法解析，而且不能相互通信。 test1与test2在不同的网络中，无法ping通。\n$ sudo docker exec -it test1 ping 192.168.246.196 -c 3 PING 192.168.246.196 (192.168.246.196): 56 data bytes --- 192.168.246.196 ping statistics --- 3 packets transmitted, 0 packets received, 100% packet loss  路由 node1 主机网卡\n$ ip addr show 16: cali6f588e66d6f@if15: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP link/ether 52:f7:01:67:89:90 brd ff:ff:ff:ff:ff:ff link-netnsid 2 18: cali3ea014c6cc8@if17: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP link/ether 02:2a:51:4f:26:c0 brd ff:ff:ff:ff:ff:ff link-netnsid 1  容器test1 网卡\n$ sudo docker exec test1 ip addr show 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 15: cali0@if16: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff inet 192.168.246.195/32 scope global cali0 valid_lft forever preferred_lft forever inet6 fe80::ecee:eeff:feee:eeee/64 scope link valid_lft forever preferred_lft forever   容器网卡cali0即16号网卡，对接host节点15号网卡； 也再次印证容器获取的是/32位主机地址； 注意容器网卡的mac地址”ee:ee:ee:ee:ee:ee”, 这是1个固定的特殊地址（所有calico生成的容器mac地址均一样），因为calico只关心三层的ip地址，而不关心二层mac地址  主机路由\n$ ip route default via 10.48.35.1 dev br0 10.48.35.0/24 dev br0 proto kernel scope link src 10.48.35.14 169.254.0.0/16 dev br0 scope link metric 1005 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 192.168.105.192/26 via 10.48.35.35 dev br0 proto bird blackhole 192.168.246.192/26 proto bird 192.168.246.195 dev cali6f588e66d6f scope link 192.168.246.196 dev cali3ea014c6cc8 scope link  calico默认在每个节点上创建一个 26 位掩码的子网，该子网可以有64个IP地址。在本机可以通过calico学习到到其他节点虚拟的路由，在本例中，如图第5条路由。\n最后两条路由是到本机容器的路由，发送到对应的网卡。\n第二条路由是到hulk容器的路由。\n容器test1 路由\n$ sudo docker exec test1 ip route default via 169.254.1.1 dev cali0 169.254.1.1 dev cali0 scope link   容器网关“169.254.1.1”是1个预留本地ip地址，通过cali0端口发送到网关； calico为简化网络配置,将容器的网关设置为1个固定的本地保留地址，容器内路由规则都是一样的，不需要动态更新； 确定下一跳后，容器会发送查询下一跳”169.254.1.1”的mac地址的ARP  这个 ARP 请求发到哪里了呢？要回答这个问题，就要知道 cali0 是 veth pair 的一端，其对端是主机上 caliXXXX 命名的 interface，可以通过 ethtool -S cali0 列出对端的 interface idnex。\n$ sudo docker exec test1 ip neigh show 169.254.1.1 dev cali0 lladdr 52:f7:01:67:89:90 used 0/0/0 probes 1 STALE  默认网卡 169.254.1.1 的mac地址是 cali0 对接网卡的mac地址，也就是host节点的16号网卡的mac地址。\n$ sudo tcpdump -i cali6f588e66d6f -e -nn 11:11:15.375778 ee:ee:ee:ee:ee:ee \u0026gt; 52:f7:01:67:89:90, ethertype ARP (0x0806), length 42: Request who-has 169.254.1.1 tell 192.168.246.195, length 28 11:11:15.375795 52:f7:01:67:89:90 \u0026gt; ee:ee:ee:ee:ee:ee, ethertype ARP (0x0806), length 42: Reply 169.254.1.1 is-at 52:f7:01:67:89:90, length 28  通过在host节点”cali6f588e66d6f”网卡上抓二层包发现，容器的”cali0”网卡（mac地址”ee:ee:ee:ee:ee:ee”）发出的request包，host节点的”cali6f588e66d6f”网卡直接以本地mac地址”52:f7:01:67:89:90”回复； 如果清除容器的arp表，可以更清晰的看到容器的arp请求报文被host节点对应的网卡响应\n换句话说，它把自己的 MAC 地址作为应答返回给容器。容器的后续报文 IP 地址还是目的容器，但是 MAC 地址就变成了主机上该 interface 的地址，也就是说所有的报文都会发给主机，然后主机根据 IP 地址进行转发。\n主机这个 interface 不管 ARP 请求的内容，直接用自己的 MAC 地址作为应答的行为被成为 ARP proxy，是 calico 开启的，可以通过下面的命令确认：\n$ cat /proc/sys/net/ipv4/conf/cali6f588e66d6f/proxy_arp 1  总的来说，可以认为 calico 把主机作为容器的默认网关来使用，所有的报文发到主机，然后主机根据路由表进行转发。和经典的网络架构不同的是，calico 并没有给默认网络配置一个 IP 地址（这样每个网络都会额外消耗一个 IP 资源，而且主机上也会增加对应的 IP 地址和路由信息），而是通过 arp proxy 和修改容器路由表来实现。\n在calico中，IP被称为Endpoint，宿主机上的容器IP称为workloadEndpoint，物理机IP称为hostEndpoint。ipPool等一同被作为资源管理。\n查看默认的地址段:\n$ sudo calicoctl get ippool -o wide CIDR NAT IPIP 192.168.0.0/16 true false fd80:24e2:f998:72d6::/64 false false  会创建默认一个IP地址池为容器使用，这里用的是 192.168.0.0/16。\ncalico为每个宿主机的容器分配了一个网段 宿主机上每个容器都有一条对应的路由表项，下一跳是veth pair，\n组件和架构 calico 做的事情： - 分配和管理 IP - 配置上容器的 veth pair 和容器内默认路由 - 根据集群网络情况实时更新节点上路由表\n从部署过程可以知道，除了 etcd 保存了数据之外，节点上也就只运行了一个 calico-node 的容器，所以推测是这个容器实现了上面所有的功能。calico/node 这个容器运行如下的进程\n[root@node00 ~]# docker exec -it calico-node sh / # ps aux PID USER TIME COMMAND 1 root 0:01 /sbin/runsvdir -P /etc/service/enabled 75 root 0:00 runsv felix 76 root 0:00 runsv bird 77 root 0:00 runsv bird6 78 root 0:00 runsv confd 79 root 0:00 runsv libnetwork 80 root 0:02 svlogd /var/log/calico/felix 81 root 30:49 calico-felix 82 root 0:00 svlogd /var/log/calico/confd 83 root 0:05 confd -confdir=/etc/calico/confd -interval=5 -watch --log-level=debug -node=http://172.17.8.100:2379 -client-key= -client-cert= -client-ca-keys= 84 root 0:00 svlogd -tt /var/log/calico/bird 85 root 0:20 bird -R -s /var/run/calico/bird.ctl -d -c /etc/calico/confd/config/bird.cfg 86 root 0:00 svlogd -tt /var/log/calico/bird6 87 root 0:18 bird6 -R -s /var/run/calico/bird6.ctl -d -c /etc/calico/confd/config/bird6.cfg 94 root 0:00 svlogd /var/log/calico/libnetwork 95 root 0:04 libnetwork-plugin  runsv 是一个 minimal 的 init 系统提供的命令，用来管理多个进程，可以看到它运行的进程包括：felix、bird、bird6、confd 和 libnetwork，这部分就介绍各个进程的功能。\nlibnetwork plugin libnetwork-plugin 是 calico 提供的 docker 网络插件，主要提供的是 IP 管理和网络管理的功能。\n默认情况下，当网络中出现第一个容器时，calico 会为容器所在的节点分配一段子网（子网掩码为 /26，比如192.168.196.128/26），后续出现在该节点上的容器都从这个子网中分配 IP 地址。这样做的好处是能够缩减节点上的路由表的规模，按照这种方式节点上 2^6 = 64 个 IP 地址只需要一个路由表项就行，而不是为每个 IP 单独创建一个路由表项。节点上创建的子网段可以在etcd 中 /calico/ipam/v2/host//ipv4/block/ 看到。\ncalico 还允许创建容器的时候指定 IP 地址，如果用户指定的 IP 地址不在节点分配的子网段中，calico 会专门为该地址添加一个 /32 的网段。\nBIRD BIRD（BIRD Internet Routing Daemon） 是一个常用的网络路由软件，支持很多路由协议（BGP、RIP、OSPF等），calico 用它在节点之间共享路由信息。\n关于 BIRD 如何配置 BGP 协议，可以参考官方文档，对应的配置文件在 /etc/calico/confd/config/ 目录。\nNOTE：至于为什么选择 BGP 协议而不是其他的路由协议，官网上也有介绍: Why BGP?\n默认所有的节点使用相同的 AS number 64512，因为 AS number 是一个32 比特的字段，所以有效取值范围是 [0-4294967295]，可以通过 calicoctl config get asNumber 命令查看当前节点使用的 AS number。\n默认情况下，每个 calico 节点会和集群中其他所有节点建立 BGP peer 连接，也就是说这是一个 O(n^2) 的增长趋势。在集群规模比较小的情况下，这种模式是可以接受的，但是当集群规模扩展到百个节点、甚至更多的时候，这样的连接数无疑会带来很大的负担。为了解决集群规模较大情况下 BGP client 连接数膨胀的问题，calico 引入了 RR（Router Reflector） 的功能。\nRR 的基本思想是选择一部分节点（一个或者多个）作为 Global BGP Peer，它们和所有的其他节点互联来交换路由信息，其他的节点只需要和 Global BGP Peer 相连就行，不需要之间再两两连接。更多的组网模式也是支持的，不管怎么组网，最核心的思想就是所有的节点能获取到整个集群的路由信息。\ncalico 对 BGP 的使用还是相对简单的，BGP 协议的原理不是一两句话能解释清楚的，以后有机会单独写篇文章来说吧。\nbird\nbird -R -s /var/run/calico/bird.ctl -d -c /etc/calico/confd/config/bird.cfg\n-R 选项指定启动后恢复 -s 指定通信的socket -d 指定 debug信息 -c 指定配置文件\nbird配置\n/ # cat /etc/calico/confd/config/bird.cfg # Generated by confd include \u0026quot;bird_aggr.cfg\u0026quot;; include \u0026quot;custom_filters.cfg\u0026quot;; include \u0026quot;bird_ipam.cfg\u0026quot;; router id 10.48.35.14; # Configure synchronization between routing tables and kernel. protocol kernel { learn; # Learn all alien routes from the kernel persist; # Don't remove routes on bird shutdown scan time 2; # Scan kernel routing table every 2 seconds import all; export filter calico_ipip; # Default is export none graceful restart; # Turn on graceful restart to reduce potential flaps in # routes when reloading BIRD configuration. With a full # automatic mesh, there is no way to prevent BGP from # flapping since multiple nodes update their BGP # configuration at the same time, GR is not guaranteed to # work correctly in this scenario. } # Watch interface up/down events. protocol device { debug { states }; scan time 2; # Scan interfaces every 2 seconds } protocol direct { debug { states }; interface -\u0026quot;cali*\u0026quot;, \u0026quot;*\u0026quot;; # Exclude cali* but include everything else. } # Template for all BGP clients template bgp bgp_template { debug { states }; description \u0026quot;Connection to BGP peer\u0026quot;; local as 64512; multihop; gateway recursive; # This should be the default, but just in case. import all; # Import all routes, since we don't know what the upstream # topology is and therefore have to trust the ToR/RR. export filter calico_pools; # Only want to export routes for workloads. next hop self; # Disable next hop processing and always advertise our # local address as nexthop source address 10.48.35.14; # The local address we use for the TCP connection add paths on; graceful restart; # See comment in kernel section about graceful restart. } # For peer /host/k8s-ep133./ip_addr_v4 protocol bgp Mesh_10_48_32_5 from bgp_template { neighbor 10.48.32.5 as 64512; } # For peer /host/k8s-ep98./ip_addr_v4 protocol bgp Mesh_10_48_35_35 from bgp_template { neighbor 10.48.35.35 as 64512; } # For peer /host/k8s-/ip_addr_v4 # Skipping ourselves (10.48.35.14)  confd 因为 bird 的配置文件会根据用户设置的变化而变化，因此需要一种动态的机制来实时维护配置文件并通知 bird 使用最新的配置，这就是 confd 的工作。confd 监听 etcd 的数据，用来更新 bird 的配置文件，并重新启动 bird 进程让它加载最新的配置文件。confd 的工作目录是 /etc/calico/confd，里面有三个目录：\nconf.d：confd 需要读取的配置文件，每个配置文件告诉 confd 模板文件在什么，最终生成的文件应该放在什么地方，更新时要执行哪些操作等 config：生成的配置文件最终放的目录 templates：模板文件，里面包括了很多变量占位符，最终会替换成 etcd 中具体的数据 具体的配置文件很多，我们只看一个例子：\n它会监听 etcd 的 /calico/bgp/v1 路径，一旦发现更新，就用其中的内容更新模板文件 bird.cfg.mesh.template，把新生成的文件放在 /etc/calico/confd/config/bird.cfg，文件改变之后还会运行 reload_cmd 指定的命令重启 bird 程序。\nNOTE：关于 confd 的使用和工作原理请参考它的官方 repo。\nfelix felix 负责最终网络相关的配置，也就是容器网络在 linux 上的配置工作，比如：\n更新节点上的路由表项 更新节点上的 iptables 表项 它的主要工作是从 etcd 中读取网络的配置，然后根据配置更新节点的路由和 iptables，felix 的代码在 github projectcalico/felix。\netcd etcd 已经在前面多次提到过，它是一个分布式的键值存储数据库，保存了 calico 网络元数据，用来协调 calico 网络多个节点。可以使用 etcdctl 命令行来读取 calico 在 etcd 中保存的数据：\netcdctl -C 172.17.8.100:2379 ls /calico /calico/ipam /calico/v1 /calico/bgp  每个目录保存的数据大致功能如下： - /calico/ipam：IP 地址分配管理，保存了节点上分配的各个子网段以及网段中 IP 地址的分配情况 - /calico/v1：profile 和 policy 的配置信息，节点上运行的容器 endpoint 信息（IP 地址、veth pair interface 的名字等）， - /calico/bgp：和 BGP 相关的信息，包括 mesh 是否开启，每个节点作为 gateway 通信的 IP 地址，AS number 等\n强大的防火墙功能 从前面的实验我们不仅知道了 calico 容器网络的报文流程是怎样的，还发现了一个事实：默认情况下，同一个网络的容器能通信（不管容器是不是在同一个主机上），不同网络的容器是无法通信的。\n这个行为是 calico 强大的防火墙实现的，默认情况下 calico 为每个网络创建一个 profile：\n$ sudo calicoctl get profile net1 -o yaml - apiVersion: v1 kind: profile metadata: name: net1 tags: - net1 spec: egress: - action: allow destination: {} source: {} ingress: - action: allow destination: {} source: tag: net1   profile 是和网络对应的，比如上面 metadata.name 的值是 net1，代表它匹配 net1网络，并应用到所有的 net1 网络容器中 calico 使用 label 来增加防火墙规则的灵活性，源地址和目的地址都可以通过 label 匹配 profile 中 metadata.tags 会应用到网络中所有的容器上 如果有定义，profile中的 metadata.labels 也会应用到网络中所有的容器上 spec 指定 profile 默认的网络规则，egress 没有限制，ingress 表示只运行 tag 为 net1 容器（也就是同一个网络的容器）的访问 每一个加入到网络的容器都会加上这个 profile，以此来实现网络之间的隔离。可以通过查看 endpoints 的详情得到它上面绑定的 profiles：\n$ sudo calicoctl get workloadEndpoint 6f588e66d6fef025fe7edb404ffb1684465b20f9454869089bcda08fbf5bc33e -o yaml - apiVersion: v1 kind: workloadEndpoint metadata: name: 6f588e66d6fef025fe7edb404ffb1684465b20f9454869089bcda08fbf5bc33e node: k8s- orchestrator: libnetwork workload: libnetwork spec: interfaceName: cali6f588e66d6f ipNetworks: - 192.168.246.195/32 mac: ee:ee:ee:ee:ee:ee profiles: - net1   iptables $ sudo iptables -L Chain INPUT (policy ACCEPT) target prot opt source destination cali-INPUT all -- anywhere anywhere /* cali:Cz_u1IQiXIMmKD4c */ KUBE-FIREWALL all -- anywhere anywhere Chain FORWARD (policy ACCEPT) target prot opt source destination cali-FORWARD all -- anywhere anywhere /* cali:wUHhoiAYhphO9Mso */ Chain OUTPUT (policy ACCEPT) target prot opt source destination cali-OUTPUT all -- anywhere anywhere /* cali:tVnHkvAo15HuiPy0 */ KUBE-FIREWALL all -- anywhere anywhere  用户也可以根据需求修改 profile 和 policy，可以参考官方教程。\n不过上面的防火墙都是针对网络的（网络中的容器的规则都是相同的），不能精细化到容器，也就是说只能做到网络之间的隔离和连通。不过 calico 也提供了对容器级别防火墙的支持，它主要是借助 docker 容器上的 label，通过匹配这些键值对来精细化控制防火墙。启动 docker label 支持需要在 calicoctl node run 命令运行时加上 \u0026ndash;use-docker-networking-container-labels 参数，而且一旦启用后原来的 profile 就被废弃不能用了（可以用纯 policy 实现原来的 profile 功能）。容器启动的时候需要添加上 label 用来作为 policy 的标识，比如 \u0026ndash;label org.projectcalico.label.role=frontend，具体的使用案例请参考这个教程。\n如果只要提供网络之间的隔离，可以使用 profile 和 policy；如果要实现精细化的容器之间的隔离，就需要启用容器的 label 功能了。在底层，calico 的 flelix 组件会实时跟踪 profile 和 policy 的内容，并更新各个节点的 iptables。\n总结 calico 的核心是通过维护路由规则实现容器的通信，路由信息的传播是 BIRD 软件通过 BGP 协议完成的，而节点上路由和防火墙规则是 felix 维护的。\n从 calico 本身的特性来说，它没有办法实现 VPC 网络，并且需要维护大量的路由表项和 iptables 表项，如果要部署在规模很大的生产环境中，需要预先规划系统的 iptables 和路由表项的上限。\n在我看来，calico 最大的优点有两个：直接三层互联的网络，不需要报文封装，因此性能更好而且能和原来的网络设施直接融合；强大的防火墙规则，利用 label 机制灵活地匹配容器，几乎可以设置任何需求的防火墙。\n但 calico 并非没有缺点，首先是它引入了 BGP 协议，虽然 bird 的配置很简单，但是运维这个系统需要熟悉 BGP 协议，这无疑会增加了人力、时间和金钱方面的投入；其次，calico 能支持的网络规模也有上限，虽然可以通过 Router Reflector 来缓解，但这么做又大大增加了网络规划、使用和排查的复杂度；最后 calico 无法用来实现 VPC 网络，IP 地址空间是所有租户共享的，租户之间是通过防火墙隔离的。\n针对flannel与calico网络方案做简单的对比 命令 创建/查看/更新/删除资源 分别使用creat/get/replace/delete来创建/查看/更新/删除资源。\n创建资源:\ncalicoctl create \u0026ndash;filename= [\u0026ndash;skip-exists] [\u0026ndash;config=] 资源使用yaml文件描述，可以创建以下资源:\nnode //物理机 bgpPeer //与本机建立了bgp连接的node hostEndpoint workloadEndpoint ipPool policy profile  查看资源:\ncalicoctl get ([--scope=\u0026lt;SCOPE\u0026gt;] [--node=\u0026lt;NODE\u0026gt;] [--orchestrator=\u0026lt;ORCH\u0026gt;] [--workload=\u0026lt;WORKLOAD\u0026gt;] (\u0026lt;KIND\u0026gt; [\u0026lt;NAME\u0026gt;]) | --filename=\u0026lt;FILENAME\u0026gt;) [--output=\u0026lt;OUTPUT\u0026gt;] [--config=\u0026lt;CONFIG\u0026gt;]  可以通过下面命令查看所有资源:\ncalicoctl get [资源类型］  例如:\ncalicoctl get node calicoctl delete node node01  IP地址管理 calicoctl ipam \u0026lt;command\u0026gt; [\u0026lt;args\u0026gt;...] release Release a calico assigned IP address. show Show details of a calico assigned IP address.  运行时设置 calicoctl config 获取和更改calico的配置项.\n日志 /var/log/calico/libnetwork/current\n参考资料 Calico网络的原理、组网方式与使用\nKubernetes Networking: Part 2 - Calico\ncalico: Frequently Asked Questions\n","date":1544140800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544140800,"objectID":"c492e2a2155a5aa6d3ca9ded5e9d4172","permalink":"/post/cloud/network/201812-calico/","publishdate":"2018-12-07T00:00:00Z","relpermalink":"/post/cloud/network/201812-calico/","section":"post","summary":"一种便捷强大的容器网络方案","tags":["Network"],"title":"Calico 架构与实践","type":"post"},{"authors":null,"categories":null,"content":" Kubernetes Kubernetes 是 Google 开源的容器集群管理系统，其提供应用部署、维护、 扩展机制等功能，利用 Kubernetes 能方便地管理跨机器运行容器化的应用，其主要功能如下：\n1) 使用 Docker 对应用程序包装 (package)、实例化 (instantiate)、运行 (run)。\n2) 以集群的方式运行、管理跨机器的容器。\n3) 解决 Docker 跨机器容器之间的通讯问题。\n4) Kubernetes 的自我修复机制使得容器集群总是运行在用户期望的状态。\nk8s 要做的不是dockerize，也不是containerize，而是作为一个集群操作系统，为此重新定义了可执行文件、进程、存储、网络的形态。\n整体结构 Master 集群控制节点，master节点上运行着一组关键的进程\n etcd，各个组件通信都并不是互相调用 API 来完成的，而是把状态写入 ETCD（相当于写入一个消息），其他组件通过监听 ETCD 的状态的的变化（相当于订阅消息），然后做后续的处理，然后再一次把更新的数据写入 ETCD。 api server，各个组件并不是直接访问 ETCD，而是访问一个代理，这个代理是通过标准的RESTFul API，重新封装了对 ETCD 接口调用，除此之外，这个代理还实现了一些附加功能，比如身份的认证、缓存等 Controller Manager 是实现任务调度的 Scheduler 是用来做资源调度的  Master 定义了 Kubernetes 集群 Master/API Server 的主要声明，包括 Pod Registry、Controller Registry、Service Registry、Endpoint Registry、Minion Registry、Binding Registry、RESTStorage 以及 Client, 是 client(Kubecfg) 调用 Kubernetes API，管理 Kubernetes 主要构件 Pods、Services、Minions、容器的入口。Master 由 API Server、Scheduler 以及 Registry 等组成。从下图可知 Master 的工作流主要分以下步骤：\n图片 - Master 主要构件及工作流\n1) Kubecfg 将特定的请求，比如创建 Pod，发送给 Kubernetes Client。\n2) Kubernetes Client 将请求发送给 API server。\n3) API Server 根据请求的类型，比如创建 Pod 时 storage 类型是 pods，然后依此选择何种 REST Storage API 对请求作出处理。\n4) REST Storage API 对的请求作相应的处理。\n5) 将处理的结果存入高可用键值存储系统 Etcd 中。\n6) 在 API Server 响应 Kubecfg 的请求后，Scheduler 会根据 Kubernetes Client 获取集群中运行 Pod 及 Minion 信息。\n7) 依据从 Kubernetes Client 获取的信息，Scheduler 将未分发的 Pod 分发到可用的 Minion 节点上。\nMinion Registry Minion Registry 负责跟踪 Kubernetes 集群中有多少 Minion(Host)。Kubernetes 封装 Minion Registry 成实现 Kubernetes API Server 的 RESTful API 接口 REST，通过这些 API，我们可以对 Minion Registry 做Create、Get、List、Delete 操作，由于 Minon 只能被创建或删除，所以不支持 Update 操作，并把 Minion 的相关配置信息存储到 etcd。除此之外，Scheduler 算法根据 Minion 的资源容量来确定是否将新建 Pod 分发到该 Minion 节点。\nPod Registry Pod Registry 负责跟踪 Kubernetes 集群中有多少 Pod 在运行，以及这些 Pod 跟 Minion 是如何的映射关系。将 Pod Registry 和 Cloud Provider 信息及其他相关信息封装成实现 Kubernetes API Server 的 RESTful API 接口 REST。通过这些 API，我们可以对 Pod 进行 Create、Get、List、Update、Delete 操作，并将 Pod 的信息存储到 etcd 中，而且可以通过 Watch 接口监视 Pod 的变化情况，比如一个 Pod 被新建、删除或者更新。\nService Registry Service Registry 负责跟踪 Kubernetes 集群中运行的所有服务。根据提供的 Cloud Provider 及 Minion Registry 信息把 Service Registry 封装成实现 Kubernetes API Server 需要的 RESTful API 接口 REST。利用这些接口，我们可以对 Service 进行 Create、Get、List、Update、Delete 操作，以及监视 Service 变化情况的 watch 操作，并把 Service 信息存储到 etcd。\nController Registry Controller Registry 负责跟踪 Kubernetes 集群中所有的 Replication Controller，Replication Controller 维护着指定数量的 pod 副本 (replicas) 拷贝，如果其中的一个容器死掉，Replication Controller 会自动启动一个新的容器，如果死掉的容器恢复，其会杀死多出的容器以保证指定的拷贝不变。通过封装 Controller Registry 为实现 Kubernetes API Server 的 RESTful API 接口 REST， 利用这些接口，我们可以对 Replication Controller 进行 Create、Get、List、Update、Delete 操作，以及监视 Replication Controller 变化情况的 watch 操作，并把 Replication Controller 信息存储到 etcd。\nEndpoints Registry Endpoints Registry 负责收集 Service 的 endpoint，比如 Name：\u0026rdquo;mysql\u0026rdquo;，Endpoints: [\u0026ldquo;10.10.1.1:1909\u0026rdquo;，\u0026rdquo;10.10.2.2:8834\u0026rdquo;]，同 Pod Registry，Controller Registry 也实现了 Kubernetes API Server 的 RESTful API 接口，可以做 Create、Get、List、Update、Delete 以及 watch 操作。\nBinding Registry Binding 包括一个需要绑定 Pod 的 ID 和 Pod 被绑定的 Host，Scheduler 写 Binding Registry 后，需绑定的 Pod 被绑定到一个 host。Binding Registry 也实现了 Kubernetes API Server 的 RESTful API 接口，但 Binding Registry 是一个 write-only 对象，所有只有 Create 操作可以使用， 否则会引起错误。\nScheduler Scheduler 收集和分析当前 Kubernetes 集群中所有 Minion 节点的资源 (内存、CPU) 负载情况，然后依此分发新建的 Pod 到 Kubernetes 集群中可用的节点。由于一旦 Minion 节点的资源被分配给 Pod，那这些资源就不能再分配给其他 Pod， 除非这些 Pod 被删除或者退出， 因此，Kubernetes 需要分析集群中所有 Minion 的资源使用情况，保证分发的工作负载不会超出当前该 Minion 节点的可用资源范围。具体来说，Scheduler 做以下工作：\n1) 实时监测 Kubernetes 集群中未分发的 Pod。\n2) 实时监测 Kubernetes 集群中所有运行的 Pod，Scheduler 需要根据这些 Pod 的资源状况安全地将未分发的 Pod 分发到指定的 Minion 节点上。\n3) Scheduler 也监测 Minion 节点信息，由于会频繁查找 Minion 节点，Scheduler 会缓存一份最新的信息在本地。\n4) 最后，Scheduler 在分发 Pod 到指定的 Minion 节点后，会把 Pod 相关的信息 Binding 写回 API Server。\nNode Node是工作主机，可以使物理主机、VM等。\n kubelet：负责管控docker容器，如启动/停止、监控运行状态等。 kube-proxy： 负责为pod提供代理。它会定期从etcd获取所有的service，并根据service信息创建代理。  kubelet 根据上图可知 Kubelet 是 Kubernetes 集群中每个 Minion 和 Master API Server 的连接点，Kubelet 运行在每个 Minion 上，是 Master API Server 和 Minion 之间的桥梁，接收 Master API Server 分配给它的 commands 和 work，与持久性键值存储 etcd、file、server 和 http 进行交互，读取配置信息。Kubelet 的主要工作是管理 Pod 和容器的生命周期，其包括 Docker Client、Root Directory、Pod Workers、Etcd Client、Cadvisor Client 以及 Health Checker 组件，具体工作如下：\n1) 通过 Worker 给 Pod 异步运行特定的 Action。\n2) 设置容器的环境变量。\n3) 给容器绑定 Volume。\n4) 给容器绑定 Port。\n5) 根据指定的 Pod 运行一个单一容器。\n6) 杀死容器。\n7) 给指定的 Pod 创建 network 容器。\n8) 删除 Pod 的所有容器。\n9) 同步 Pod 的状态。\n10) 从 Cadvisor 获取 container info、 pod info、root info、machine info。\n11) 检测 Pod 的容器健康状态信息。\n12) 在容器中运行命令。\nContainer Runtime（容器运行时） 每一个Node都会运行一个Container Runtime，其负责下载镜像和运行容器。Kubernetes本身并不停容器运行时环境，但提供了接口，可以插入所选择的容器运行时环境。kubelet使用Unix socket之上的gRPC框架与容器运行时进行通信，kubelet作为客户端，而CRI shim作为服务器。\nprotocol buffers API提供两个gRPC服务，ImageService和RuntimeService。ImageService提供拉取、查看、和移除镜像的RPC。RuntimeSerivce则提供管理Pods和容器生命周期管理的RPC，以及与容器进行交互(exec/attach/port-forward)。容器运行时能够同时管理镜像和容器（例如：Docker和Rkt），并且可以通过同一个套接字提供这两种服务。在Kubelet中，这个套接字通过–container-runtime-endpoint和–image-service-endpoint字段进行设置。Kubernetes CRI支持的容器运行时包括docker、rkt、cri-o、frankti、kata-containers和clear-containers等。\nProxy Proxy 是为了解决外部网络能够访问跨机器集群中容器提供的应用服务而设计的，从上图可知 Proxy 服务也运行在每个 Minion 上。Proxy 提供 TCP/UDP sockets 的 proxy，每创建一种 Service，Proxy 主要从 etcd 获取 Services 和 Endpoints 的配置信息，或者也可以从 file 获取，然后根据配置信息在 Minion 上启动一个 Proxy 的进程并监听相应的服务端口，当外部请求发生时，Proxy 会根据 Load Balancer 将请求分发到后端正确的容器处理。\n服务发现主要通过DNS实现。\n在Kubernetes中，kube proxy负责为Pod创建代理服务；引到访问至服务；并实现服务到Pod的路由和转发，以及通过应用的负载均衡。\nProxy不但解决了同一主宿机相同服务端口冲突的问题，还提供了Service转发服务端口对外提供服务的能力，Proxy后端使用了随机、轮循负载均衡算法。\n分层架构 Kubernetes设计理念和功能其实就是一个类似Linux的分层架构，如下图所示\n 核心层：Kubernetes最核心的功能，对外提供API构建高层的应用，对内提供插件式应用执行环境 应用层：部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS解析等） 管理层：系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态Provision等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy等） 接口层：kubectl命令行工具、客户端SDK以及集群联邦 生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴 Kubernetes外部：日志、监控、配置管理、CI、CD、Workflow、FaaS、OTS应用、ChatOps等 Kubernetes内部：CRI、CNI、CVI、镜像仓库、Cloud Provider、集群自身的配置和管理等  系统流程 一位大牛整理的 K8S 调用流程\n设计理念  声明式 VS 命令式, 声明式优点很多，一个很重要的点是：在分布式系统中，任何组件都可能随时出现故障。当组件恢复时，需要弄清楚要做什么，使用命令式 API 时，处理起来就很棘手。但是使用声明式 API ，组件只需查看 API 服务器的当前状态，即可确定它需要执行的操作。\n 显式的 API, Kubernetes 是透明的，它没有隐藏的内部 API。换句话说 Kubernetes 系统内部用来交互的 API 和我们用来与 Kubernetes 交互的 API 相同。这样做的好处是，当 Kubernetes 默认的组件无法满足我们的需求时，我们可以利用已有的 API 实现我们自定义的特性。\n 无侵入性, 感谢 Docker 容器技术的流行，使得 Kubernetes 为大家提供了无缝的使用方式。我们的应用达到镜像后, 不需要改动就可以遨游在 Kubernetes 集群中。Kubernetes 还提供存储 Secret、Configuration 等包含但不局限于密码、证书、容器镜像信息、应用启动参数能力。如此，Kubernetes 以一种友好的方式将这些东西注入 Pod，减少了大家的工作量，而无需重写或者很大幅度改变原有的应用代码。\n 为了实现这一目标，Kubernetes 引入了 PersistentVolumeClaim（PVC）和 PersistentVolume（PV）API 对象。这些对象将存储实现与存储使用分离。 PersistentVolumeClaim 对象用作用户以与实现无关的方式请求存储的方法，通过它来抹除对底层 PersistentVolume 的差异性。这样就使 Kubernetes 拥有了跨集群的移植能力。\n  容器编排系统的比较 在 Google 的一篇关于内部 Omega 调度系统的论文中，将调度系统分成三类：单体、二层调度和共享状态三种，按照它的分类方法，通常Google的 Borg被分到单体这一类，Mesos被当做二层调度，而Google自己的Omega被当做第三类“共享状态”。\n因为Kubernetes的大部分设计是延续 Borg的，而且Kubernetes的核心组件（Controller Manager和Scheduler）缺省也都是绑定部署在一起，状态也都是存储在ETCD里面的，所以通常大家会把Kubernetes也当做“单体”调度系统，我觉得 Kubernetes 的调度模型也完全是二层调度的，和 Mesos 一样，任务调度和资源的调度是完全分离的，Controller Manager承担任务调度的职责，而Scheduler则承担资源调度的职责。\nMesos 实际上Kubernetes和Mesos调度的最大区别在于资源调度请求的方式：\n主动 Push 方式。是 Mesos 采用的方式，就是 Mesos 的资源调度组件（Mesos Master）主动推送资源 Offer 给 Framework，Framework 不能主动请求资源，只能根据 Offer 的信息来决定接受或者拒绝。\n被动 Pull 方式。是 Kubernetes 的方式，资源调度组件 Scheduler 被动的响应 Controller Manager的资源请求。\n这两种方式带来的不同，我主要从一下 5 个方面来分析。另外注意，我所比较两者的优劣，都是从理论上做的分析，工程实现上会有差异，一些指标我也并没有实际测试过。\n1.资源利用率：Kubernetes 胜出\n理论上，Kubernetes 应该能实现更加高效的集群资源利用率，原因资源调度的职责完全是由Scheduler一个组件来完成的，它有充足的信息能够从全局来调配资源，然后而Mesos 却做不到，因为资源调度的职责被切分到Framework和Mesos Master两个组件上，Framework 在挑选 Offer 的时候，完全没有其他 Framework 工作负载的信息，所以也不可能做出最优的决策。\n举个例子，比如我们希望把对耗费 CPU的工作负载和耗费内存的工作负载尽可能调度到同一台主机上，在Mesos里面不太容易做到，因为他们分属不同的 Framework。\n2.扩展性：Mesos胜出\n从理论上讲，Mesos 的扩展性要更好一点。原因是Mesos的资源调度方式更容易让已经存在的任务调度迁移上来。举个例子，假设已经有了一个任务调度系统，比如 Spark，现在要迁移到集群调度平台上，理论上它迁移到 Mesos 比 Kubernetes 上更加容易。\n如果迁移到 Mesos ，没有改变原来的工作流程和逻辑，原来的逻辑是：来了一个作业请求，调度系统把任务拆分成小的任务，然后从资源池里面挑选一个节点来运行任务，并且记录挑选的节点 IP 和端口号，用来跟踪任务的状态。迁移到 Mesos 之后，还是一样的逻辑，唯一需要变化的是那个资源池，原来是自己管理的资源池，现在变成 Mesos 提供的Offer 列表。\n如果迁移到 Kubernetes，则需要修改原来的基本逻辑来适配 Kubernetes，资源的调度完全需要调用外部的组件来完成，并且这个变成异步的。\n3.灵活的任务调度策略：Mesos 胜出\nMesos 对各种任务的调度策略也支持的更好。举个例子，如果某一个作业，需要 All or Nothing 的策略，Mesos 是能够实现的，但是 Kubernetes 完全无法支持。所以All or Nothing 的意思是，价格整个作业如果需要运行 10 个任务，这 10个任务需要能够全部有资源开始执行，否则就一个都不执行。\n4.性能：Mesos 胜出\nMesos 的性能应该更好，因为资源调度组件，也就是 Mesos Master 把一部分资源调度的工作甩给 Framework了，承担的调度工作更加简单，从数据来看也是这样，在多年之前 Twitter 自己的 Mesos 集群就能够管理超过 8万个节点，而 Kubernetes 1.3 只能支持 5千个节点。\n5.调度延迟：Kubernetes 胜出\nKubernetes调度延迟会更好。因为Mesos的轮流给Framework提供Offer机制，导致会浪费很多时间在给不需要资源的 Framework 提供Offer。\nSwarm Kubernetes的优势 Kubernetes 作为容器编排的事实标准，主要强在以下方面：\n 理念的先进性 Kubernetes 并未将 Docker 作为架构的核心，而仅仅将它作为一个container runtime 实现，k8s的核心是cni、csi、cri、oci等。相对的，mesos是docker的使用者，也必然是docker特性的迁就者。K8S 架构有很强的扩展性，而Mesos则需要考虑 Docker 的支持程度。\n 声明式 API k8s系统的梳理了任务的形态以及任务之间的关系，并为未来留有余地，提供了声明式的 API。\n  云计算平台上的开发者们所关心的，并不是调度，也不是资源管管理，更不是网络或者存储，他们关心的只有一件事，那就是 Kubernetes 的 API，也就是声明式 API 和控制器模式。这个 API 独有的编程范式，即 Controller 和 Operator。作为一个云计算平台的用户，能够用一个 YAML 文件表达我开发的应用的最终运行状态，并且自动地对我的应用进行运维和管理。这种信赖关系，就是连接Kubernetes 项目和开发者们最重要的纽带。\n不同于一个只能生产资源的集群管理工具，Kubernetes 项目最大的价值，乃在于它从一开始就提倡的声明式 API 和以此为基础“控制器”模式。Kubernetes 项目为使用者提供了宝贵的 API 可扩展能力和良好的 API 编程范式，催生出了一个完全基于 Kubernetes API 构建出来的上层应用服务生态。可以说，正是这个生态的逐步完善与日趋成熟，才确立了 Kubernetes 项目如今在云平台领域牢不可破的领导地位，也间接宣告了竞品方案的边缘化。\n未来：应用交付的革命不会停止，Kubernetes 项目一直在做的，其实是在进一步清晰和明确“应用交付”这个亘古不变的话题。只不过，相比于交付一个容器和容器镜像， Kubernetes 项目正在尝试明确的定义云时代“应用”的概念。在这里，应用是一组容器的有机组合，同时也包括了应用运行所需的网络、存储的需求的描述。而像这样一个“描述”应用的 YAML 文件，放在 etcd 里存起来，然后通过控制器模型驱动整个基础设施的状态不断地向用户声明的状态逼近，就是 Kubernetes 的核心工作原理了。PS: 以后你给公有云一个yaml 文件就可以发布自己的应用了。\n横向扩展 几乎所有的集群调度系统都无法横向扩展（Scale Out），Mesos通过优化，一个集群能够管理 8 万个节点，Kubernetes 最新的1.15版本，集群管理节点的上限是 5000 个节点。\n所有的集群调度系统的架构都是无法横向扩展的，如果需要管理更多的服务器，唯一的办法就是创建多个集群。集群调度系统的架构看起来都是这个样子的：\n中间的 Scheduler（资源调度器）是最核心的组件，虽然通常是由多个（通常是3个）实例组成，但是都是单活的，也就是说只有一个节点工作，其他节点都处于 Standby 的状态。\n这是因为集群调度系统的“独立资源池”数量是 1，每一台服务器节点都是一个资源，每当资源消费者请求资源的时候，调度系统用来做调度算法的“独立资源池”是多大呢？答案应该是整个集群的资源组成的资源池，没有办法在切分了，因为:\n 调度系统的职责就是要在全局内找到最优的资源匹配。 另外，哪怕不需要找到最优的资源匹配，资源调度器对每一次资源请求，也没办法判断应该从哪一部分资源池中挑选资源。  正是因为这个原因，“独立资源池”数量是 1，所以集群调度系统无法做到横向扩展。\n优化 主要是 Scheduler 调度器的优化，主要体现在两个地方：\n 预选失败中断机制  一次调度过程在判断一个Node是否可作为目标机器主要分为三个阶段：\n预选阶段：硬性条件，过滤掉不满足条件的节点，这个过程称为Predicates。这是固定先后顺序的一系列过滤条件，任何一个predicate不符合则放弃该Node。\n优选阶段：软性条件，对通过的节点按照优先级排序，称之为Priorities。每一个priority都是一个影响因素，都有一定的权重。\n选定阶段：从优选列表中选择优先级最高的节点，称为Select。选择的Node即为最终部署Pod的机器。\n通过深入分析调度过程我们发现，调度器在预选阶段即使已经知道当前Node不符合某个过滤条件仍然会继续判断后续的过滤条件是否符合。试想如果有上万台Node节点，这些判断逻辑会浪费很多计算时间，也是调度器性能低下的一个重要因素。\n改进为“预选失败中断机制”，即一旦某个预选条件不满足，那么该Node即被立即放弃，后面的预选条件不再做判断计算，从而大大减少了计算量，调度性能也大大提升。如下图：\n 局部最优解  对于优化问题，尤其是最优化问题，总是希望找到全局最优的解或策略，但是当问题的复杂度过于高，要考虑的因素和处理的信息量过多的时候，我们往往会倾向于接受局部最优解，因为局部最优解的质量不一定都是差的。尤其是当我们有确定的评判标准标明得出的解是可以接受的话，通常会接收局部最优的结果。这样，从成本、效率等多方面考虑，才是实际工程中会采取的策略。\n当前调度策略中，每次调度调度器都会遍历集群中所有的Node，以便找出最优的节点，这在调度领域我们称之为BestFit算法，但是在生产环境中，我们是选取最优Node还是次优的Node其实并没有特别大的区别和影响，有时候我们还是避免每次选取最优的Node(例如我们集群为了解决新上线机器后狂在该机器上创建应用的问题就将最优解随机化)。换句话说，我们找出局部最优解就能满足我们的需求。\n假设集群一共1000个Node，一次调度过程PodA，这其中有700个Node都能通过Predicates(预选阶段)，那么就会把所有的Node遍历并找出这700个node，然后经过得分排序找出最优的Node节点NodeX；但是采用局部最优算法，即我们认为只要能找出N个Node，并在这N个Node中选择得分最高的Node即能满足我们的需求，比如我们默认找出100个可以通过Predicates(预选阶段)的Node即可，我们的最优解就在这100个Node中选择，当然全局最优解NodeX可能在也可能不在这100个Node中，但是我们在这100个Node中选择最优的NodeY也能满足我们的要求。最好的情况下我们在遍历100个Node就找出了这100个Node，也可能遍历了200个或者300个Node等等，这样我们可以大大减少计算时间，同时也不会对我们的调度结果产生太大的影响。\n参考 K8S 设计理念\nKubernetes整体结构\n","date":1542672000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542672000,"objectID":"04020d602faf16fa8cc35d08ca7b357c","permalink":"/post/cloud/k8s/201811-k8s-arch/","publishdate":"2018-11-20T00:00:00Z","relpermalink":"/post/cloud/k8s/201811-k8s-arch/","section":"post","summary":"Kubernetes 云计算操作系统","tags":["Kubernetes"],"title":"Kubernetes 框架","type":"post"},{"authors":null,"categories":null,"content":"支持一下功能：\n 照镜子 语音交互 天气预报 室内温湿度 新闻资讯 股票信息 支持定制更多功能  ","date":1535328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535328000,"objectID":"4271ecf2381076959da698bfcf06917c","permalink":"/project/mirror-project/","publishdate":"2018-08-27T00:00:00Z","relpermalink":"/project/mirror-project/","section":"project","summary":"这是一面高颜值、高智商、会说话的镜子","tags":["MagicMirror"],"title":"魔镜","type":"project"},{"authors":null,"categories":null,"content":"专业的区块链、数字币空投搜集网址，通过做一些简单的任务即可获取空投币，可以体验一些区块链小游戏，还可以实时预览多种主流数字币的价格。\n点击前往 airdropfans.com  体验\n","date":1534204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534204800,"objectID":"24363b7477f2eb15cc4f4ff9ca055889","permalink":"/project/blockchain-airdrop/","publishdate":"2018-08-14T00:00:00Z","relpermalink":"/project/blockchain-airdrop/","section":"project","summary":"获取多种空投 Token，还可以参与区块链小游戏。","tags":["BlockChain"],"title":"空投粉","type":"project"},{"authors":null,"categories":null,"content":"周期性获取火币网的多种交易对行情，可以方便的新增交易对、自定义行情周期，获取的结果存储到 MySQL 中。\n","date":1532649600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532649600,"objectID":"15169f7bb87fa24153ed41a267828478","permalink":"/project/blockchain-project/","publishdate":"2018-07-27T00:00:00Z","relpermalink":"/project/blockchain-project/","section":"project","summary":"获取火币网交易对行情并存储到数据库中。","tags":["BlockChain"],"title":"数字币交易所行情","type":"project"},{"authors":null,"categories":null,"content":" 最近在找对象，突发奇想做个基于区块链的相亲合约。\n听说有个程序员在婚恋网站上挂了1个多月都没人搭理他，然后给自己加了个 区块链工程师 的标签，几天后收到大量私信😝\n传统相亲平台的痛点  中介所收费贵，介绍的还不靠谱 百合网、珍爱网之流坑爹，看个消息要钱，还都是机器人发的 虚假信息多，各种酒拖 效率低  区块链相亲的优势 LoveChain可以解决三大婚恋社交问题，分别是安全性、激励性、产业生态。\n安全性 安全性问题又分为三类：用户身份的真实性、网站登记信息的私密性、以及线下实地见面的安全性。\n针对真实用户，运用 AI、大数据、生物识别认证等技术对用户的合法身份进行有效认证，结合百合佳缘积累的社交黑名单库，对可疑的用户进行拦截和清除。同时，反垃圾监测系统可以将可疑用户进行账号隔离，并对其发出的垃圾信息进行单边隐藏，保护合法的用户利益。\n针对用户敏感信息私密性的保护，其中提及一旦成为平台认证的合法用户，会有一个虚拟用户钱包，钱包由用户自己设置账号和密码， 只要把密钥安全存放，任何人都无法获取用户账户内的虚拟资产。用 户可以用自己的公私钥进行对自己的虚拟资产进行消费交易。\n针对线下活动，采用智能合约，质押数字资产存放于平台，只有利益相关方的密钥才可以转移价值，其它人无权转移。当有人违约时，按照智能合约的约定，质押数字资产会自动转移到守约者账户。当各方都遵循合约约定时，合约 履行完毕，质押数字资产自动退到质押方各自账户。\n线下智能约会场景\n激励性 由于婚恋社交的用户粘性比较低，因此可以通过代币奖励来提高积极性，比如通过注册账户、上传照片、主动发消息、身份认证、朋友圈发帖等方式获得激励点。这些代币平台可以应用于多种业务，包括情感咨询、婚礼婚庆甚至婚房等。\n产业生态 可以预见，未来的婚恋问题将向低结婚欲望、大龄未婚等发现发展，这方面的产业生态将发生改变，需要提前去布局。\n实现 基于以太坊，做了一个简单的demo\n请前往airdropfans区块恋体验。\n","date":1530576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530576000,"objectID":"00f7958e45e47773384e7dacb2299df3","permalink":"/post/blockchain/201807-contract-love/","publishdate":"2018-07-03T00:00:00Z","relpermalink":"/post/blockchain/201807-contract-love/","section":"post","summary":"免费发布和获取对方信息的靠谱婚恋平台","tags":["区块链","智能合约"],"title":"基于区块链智能合约的相亲","type":"post"},{"authors":null,"categories":null,"content":" 如果要问智能合约最适合做什么应用，还有比彩票（赌博）更好的答案吗？ 恐怕没有\n传统彩票的痛点  不透明，存在信任危机 效率低，为了小概率中奖而花费很多精力关注开奖日期，错失兑奖时间  区块链彩票的优势 通过区块链技术将传统的纸质彩票进行电子化，利用区块链技术的不可篡改性解决电子化彩票号码可以被篡改的问题。\n每家彩票站作为区块链的一个节点，将卖出的彩票写入区块链中，不可篡改。\n通过智能合约打造一个智能电子彩票交易系统\n当购买彩票的人确定好号码，下注的数量后。该智能交易系统会自动生成智能合约将购买的彩票号码，下注的数量，彩票的价格和中奖的赔率等条件写入智能合约。再者就是彩票公司的数字货币钱包和购彩者的数字货币钱包一并写入智能合约中，交由计算机自动执行。\n当购彩者确认购买后，智能合约自动从购彩者的数字货币钱包将彩票的购买费用转到彩票公司。当该期彩票开奖时，智能合约能够自动根据开奖的号码和每一位购彩者所购的号码来进行判断中奖的情况。如果有中奖，智能合约就能够根据中奖的金额，自动将奖金从彩票公司的数字货币钱包转到中奖者的数字货币钱包，整个过程不需要人为的参与，全程由计算机自动执行。\n这样一来，对于购买彩票的人来说，他们买完后就不需要花较大的精力去关注自己是否中奖，也不需要当心自己的中奖彩票不能及时兑奖而过期，因为智能合约能够自动进行兑奖。对于彩票发行公司来说，有了这个系统后，不仅不需要大量的人来进行兑奖工作，在减少人工成本的同时，很大程度的提高了工作的效率。甚至不需要实体的店面来经营彩票，大大降低了彩票经营管理的成本，提高利润和市场竞争力。\n实现 基于以太坊，做了一个彩票游戏。\n请前往airdropfans押宝游戏体验。\n","date":1528761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528761600,"objectID":"e4e8feaf3be61c36fe4abfeca66f735f","permalink":"/post/blockchain/201806-contract-casino/","publishdate":"2018-06-12T00:00:00Z","relpermalink":"/post/blockchain/201806-contract-casino/","section":"post","summary":"去中心化的、公平的彩票系统","tags":["区块链","智能合约"],"title":"基于区块链智能合约的彩票","type":"post"},{"authors":null,"categories":null,"content":" 目前各种电商、实体店都有很多优惠券发放，但是人们并没有充分利用起来，如果可以把优惠券在智能合约上流通起来，那商家实现了广告的目的，也得到了客流量，而顾客也能得到优惠。这个去中心化的业务场景可以用区块链智能合约实现。\n重新定义  重新定义优惠券的流通方式 重新定义公正透明的拍卖方式  智能合约具有如下特点： - 一种传播、验证或执行合同的协议 - 不需要第三方的可信交易 - 可追踪 - 不可逆转\n智能合约保证了拍卖的公正透明，无需人工参与\n架构设计 发起拍卖的流程\n参与竞拍\n","date":1525478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525478400,"objectID":"febf6d7b2661510ded1b03f3e35dc619","permalink":"/post/blockchain/201805-contract-coupon/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/post/blockchain/201805-contract-coupon/","section":"post","summary":"去中心化的优惠券交易系统","tags":["区块链","智能合约"],"title":"基于区块链智能合约的优惠券","type":"post"}]